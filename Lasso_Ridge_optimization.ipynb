{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## finish this off on the side!!! \n",
    "## next steps build function off the best result\n",
    "\n",
    "## and test it on the test batch\n",
    "\n",
    "## give a graph that shows y_actual vs y_predict\n",
    "\n",
    "## save a model for every parameter you care about --- like flight id or route or \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_r2</th>\n",
       "      <th>mean_MAPE</th>\n",
       "      <th>mean_MSE</th>\n",
       "      <th>std_r2</th>\n",
       "      <th>std_MAPE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>score</th>\n",
       "      <th>alpha</th>\n",
       "      <th>intercept</th>\n",
       "      <th>tol</th>\n",
       "      <th>...</th>\n",
       "      <th>combined_score</th>\n",
       "      <th>opti_run</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>random_state</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>depth</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.078277</td>\n",
       "      <td>4.941517e+16</td>\n",
       "      <td>70.050639</td>\n",
       "      <td>0.019678</td>\n",
       "      <td>6.309893e+14</td>\n",
       "      <td>13.722034</td>\n",
       "      <td>5</td>\n",
       "      <td>99.934116</td>\n",
       "      <td>True</td>\n",
       "      <td>0.112348</td>\n",
       "      <td>...</td>\n",
       "      <td>64.567257</td>\n",
       "      <td>ridge_all_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.078210</td>\n",
       "      <td>4.991278e+16</td>\n",
       "      <td>70.055631</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>7.072129e+14</td>\n",
       "      <td>13.732237</td>\n",
       "      <td>5</td>\n",
       "      <td>99.992802</td>\n",
       "      <td>True</td>\n",
       "      <td>0.059643</td>\n",
       "      <td>...</td>\n",
       "      <td>64.576558</td>\n",
       "      <td>ridge_lasso_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.104810</td>\n",
       "      <td>2.864678e+16</td>\n",
       "      <td>68.859243</td>\n",
       "      <td>0.016020</td>\n",
       "      <td>2.842307e+15</td>\n",
       "      <td>12.656255</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>61.642082</td>\n",
       "      <td>ran_forest_all_features</td>\n",
       "      <td>125.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.097609</td>\n",
       "      <td>2.903251e+16</td>\n",
       "      <td>69.153410</td>\n",
       "      <td>0.012340</td>\n",
       "      <td>3.035956e+15</td>\n",
       "      <td>12.781126</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62.403390</td>\n",
       "      <td>ran_forest_lasso_features</td>\n",
       "      <td>131.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.061302</td>\n",
       "      <td>3.210326e+16</td>\n",
       "      <td>70.430978</td>\n",
       "      <td>0.019263</td>\n",
       "      <td>2.817322e+15</td>\n",
       "      <td>12.424414</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>66.113415</td>\n",
       "      <td>Dec_Tree_all_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.052449</td>\n",
       "      <td>3.238770e+16</td>\n",
       "      <td>70.802836</td>\n",
       "      <td>0.014550</td>\n",
       "      <td>2.616372e+15</td>\n",
       "      <td>12.685494</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>67.089328</td>\n",
       "      <td>Dec_Tree_lasso_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.052449</td>\n",
       "      <td>3.238770e+16</td>\n",
       "      <td>70.802836</td>\n",
       "      <td>0.014550</td>\n",
       "      <td>2.616372e+15</td>\n",
       "      <td>12.685494</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>67.089328</td>\n",
       "      <td>Dec_Tree_lasso_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.720789</td>\n",
       "      <td>2.364996e+16</td>\n",
       "      <td>35.464243</td>\n",
       "      <td>0.104638</td>\n",
       "      <td>8.979420e+14</td>\n",
       "      <td>3.465619</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.901991</td>\n",
       "      <td>catBoost_all_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.084313</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.249099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.775143</td>\n",
       "      <td>2.340971e+16</td>\n",
       "      <td>31.835973</td>\n",
       "      <td>0.085024</td>\n",
       "      <td>8.270877e+14</td>\n",
       "      <td>3.282729</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.158535</td>\n",
       "      <td>catBoost_lasso_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.099768</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.906476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_r2     mean_MAPE   mean_MSE    std_r2      std_MAPE    std_MSE   \n",
       "21  0.078277  4.941517e+16  70.050639  0.019678  6.309893e+14  13.722034  \\\n",
       "26  0.078210  4.991278e+16  70.055631  0.020734  7.072129e+14  13.732237   \n",
       "4   0.104810  2.864678e+16  68.859243  0.016020  2.842307e+15  12.656255   \n",
       "1   0.097609  2.903251e+16  69.153410  0.012340  3.035956e+15  12.781126   \n",
       "26  0.061302  3.210326e+16  70.430978  0.019263  2.817322e+15  12.424414   \n",
       "21  0.052449  3.238770e+16  70.802836  0.014550  2.616372e+15  12.685494   \n",
       "25  0.052449  3.238770e+16  70.802836  0.014550  2.616372e+15  12.685494   \n",
       "23  0.720789  2.364996e+16  35.464243  0.104638  8.979420e+14   3.465619   \n",
       "27  0.775143  2.340971e+16  31.835973  0.085024  8.270877e+14   3.282729   \n",
       "\n",
       "    score      alpha intercept       tol  ... combined_score   \n",
       "21      5  99.934116      True  0.112348  ...      64.567257  \\\n",
       "26      5  99.992802      True  0.059643  ...      64.576558   \n",
       "4       5        NaN       NaN       NaN  ...      61.642082   \n",
       "1       5        NaN       NaN       NaN  ...      62.403390   \n",
       "26      5        NaN       NaN       NaN  ...      66.113415   \n",
       "21      5        NaN       NaN       NaN  ...      67.089328   \n",
       "25      5        NaN       NaN       NaN  ...      67.089328   \n",
       "23      3        NaN       NaN       NaN  ...       9.901991   \n",
       "27      3        NaN       NaN       NaN  ...       7.158535   \n",
       "\n",
       "                     opti_run  n_estimators max_depth  min_samples_split   \n",
       "21         ridge_all_features           NaN       NaN                NaN  \\\n",
       "26       ridge_lasso_features           NaN       NaN                NaN   \n",
       "4     ran_forest_all_features         125.0      19.0                6.0   \n",
       "1   ran_forest_lasso_features         131.0      11.0                7.0   \n",
       "26      Dec_Tree_all_features           NaN       4.0                7.0   \n",
       "21    Dec_Tree_lasso_features           NaN       4.0                4.0   \n",
       "25    Dec_Tree_lasso_features           NaN       4.0                4.0   \n",
       "23      catBoost_all_features           NaN       NaN                NaN   \n",
       "27    catBoost_lasso_features           NaN       NaN                NaN   \n",
       "\n",
       "    min_samples_leaf  random_state  learning_rate  depth  l2_leaf_reg  \n",
       "21               NaN           NaN            NaN    NaN          NaN  \n",
       "26               NaN           NaN            NaN    NaN          NaN  \n",
       "4                9.0           NaN            NaN    NaN          NaN  \n",
       "1                8.0           NaN            NaN    NaN          NaN  \n",
       "26              10.0          50.0            NaN    NaN          NaN  \n",
       "21              10.0          10.0            NaN    NaN          NaN  \n",
       "25              10.0          22.0            NaN    NaN          NaN  \n",
       "23               NaN           NaN       0.084313    9.0     0.249099  \n",
       "27               NaN           NaN       0.099768   10.0     0.906476  \n",
       "\n",
       "[9 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.118e+04, tolerance: 4.157e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2023-09-06 10:20:01,811] A new study created in memory with name: RidgeRegression Optimization\n",
      "[I 2023-09-06 10:20:01,895] Trial 0 finished with value: 69.16957849589886 and parameters: {'alpha': 20.160126965802917, 'fit_intercept': False, 'tol': 0.002515536674290861, 'solver': 'svd'}. Best is trial 0 with value: 69.16957849589886.\n",
      "[I 2023-09-06 10:20:02,469] Trial 1 finished with value: 64.6297222551626 and parameters: {'alpha': 12.482762901443367, 'fit_intercept': True, 'tol': 0.002268324154354905, 'solver': 'sag'}. Best is trial 1 with value: 64.6297222551626.\n",
      "[I 2023-09-06 10:20:03,247] Trial 2 finished with value: 64.5764298438224 and parameters: {'alpha': 76.00107779824371, 'fit_intercept': True, 'tol': 0.0034570440680621894, 'solver': 'saga'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:03,294] Trial 3 finished with value: 64.7132188571416 and parameters: {'alpha': 71.00553783359005, 'fit_intercept': True, 'tol': 0.006503333322647771, 'solver': 'lsqr'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:03,356] Trial 4 finished with value: 69.4154353539828 and parameters: {'alpha': 96.0313404021359, 'fit_intercept': False, 'tol': 0.06016728534083815, 'solver': 'lsqr'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:03,412] Trial 5 finished with value: 64.63481927958453 and parameters: {'alpha': 46.947673783689645, 'fit_intercept': True, 'tol': 0.0159022175888093, 'solver': 'lsqr'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:03,776] Trial 6 finished with value: 69.12915201740316 and parameters: {'alpha': 78.84131711993479, 'fit_intercept': False, 'tol': 0.009081977815954686, 'solver': 'saga'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:03,859] Trial 7 finished with value: 69.17528293071639 and parameters: {'alpha': 15.11981058519386, 'fit_intercept': False, 'tol': 0.004988557554869765, 'solver': 'svd'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:03,943] Trial 8 finished with value: 69.1589578594183 and parameters: {'alpha': 30.804460273869584, 'fit_intercept': False, 'tol': 0.3520221224723784, 'solver': 'svd'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:03,983] Trial 9 finished with value: 69.18870936817504 and parameters: {'alpha': 5.162582950382843, 'fit_intercept': False, 'tol': 0.07640791107469531, 'solver': 'cholesky'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:05,217] Trial 10 finished with value: 64.58806055314373 and parameters: {'alpha': 59.064325143303066, 'fit_intercept': True, 'tol': 0.0010166831447318513, 'solver': 'saga'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:06,213] Trial 11 finished with value: 64.58642414868103 and parameters: {'alpha': 59.92584589744996, 'fit_intercept': True, 'tol': 0.0015904101004725374, 'solver': 'saga'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:07,420] Trial 12 finished with value: 64.59372055372944 and parameters: {'alpha': 51.04595509115805, 'fit_intercept': True, 'tol': 0.0010611923971607174, 'solver': 'saga'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:07,479] Trial 13 finished with value: 64.57788295033562 and parameters: {'alpha': 79.06728919739464, 'fit_intercept': True, 'tol': 0.019793192298273716, 'solver': 'auto'}. Best is trial 2 with value: 64.5764298438224.\n",
      "[I 2023-09-06 10:20:07,545] Trial 14 finished with value: 64.57204859288683 and parameters: {'alpha': 89.96607919652293, 'fit_intercept': True, 'tol': 0.021378823549867393, 'solver': 'auto'}. Best is trial 14 with value: 64.57204859288683.\n",
      "[I 2023-09-06 10:20:07,601] Trial 15 finished with value: 64.56860862628929 and parameters: {'alpha': 97.02053360567967, 'fit_intercept': True, 'tol': 0.04130035120018312, 'solver': 'auto'}. Best is trial 15 with value: 64.56860862628929.\n",
      "[I 2023-09-06 10:20:07,661] Trial 16 finished with value: 64.56784113458212 and parameters: {'alpha': 98.66519337994258, 'fit_intercept': True, 'tol': 0.044973630830117135, 'solver': 'auto'}. Best is trial 16 with value: 64.56784113458212.\n",
      "[I 2023-09-06 10:20:07,719] Trial 17 finished with value: 64.56731694623242 and parameters: {'alpha': 99.80409866502514, 'fit_intercept': True, 'tol': 0.058102457589359266, 'solver': 'auto'}. Best is trial 17 with value: 64.56731694623242.\n",
      "[I 2023-09-06 10:20:07,771] Trial 18 finished with value: 64.56758859690585 and parameters: {'alpha': 99.21228179723606, 'fit_intercept': True, 'tol': 0.11724858761716764, 'solver': 'auto'}. Best is trial 17 with value: 64.56731694623242.\n",
      "[I 2023-09-06 10:20:07,840] Trial 19 finished with value: 64.57408552355595 and parameters: {'alpha': 86.01734063657959, 'fit_intercept': True, 'tol': 0.16477410731868608, 'solver': 'auto'}. Best is trial 17 with value: 64.56731694623242.\n",
      "[I 2023-09-06 10:20:07,934] Trial 20 finished with value: 65.3270823320209 and parameters: {'alpha': 87.4994139442422, 'fit_intercept': True, 'tol': 0.6448123994206165, 'solver': 'sag'}. Best is trial 17 with value: 64.56731694623242.\n",
      "[I 2023-09-06 10:20:07,988] Trial 21 finished with value: 64.56725747888089 and parameters: {'alpha': 99.9341164466404, 'fit_intercept': True, 'tol': 0.11234816808170511, 'solver': 'auto'}. Best is trial 21 with value: 64.56725747888089.\n",
      "[I 2023-09-06 10:20:08,044] Trial 22 finished with value: 64.56758012782045 and parameters: {'alpha': 99.23068034375281, 'fit_intercept': True, 'tol': 0.12915397526491276, 'solver': 'auto'}. Best is trial 21 with value: 64.56725747888089.\n",
      "[I 2023-09-06 10:20:08,098] Trial 23 finished with value: 64.57262752429145 and parameters: {'alpha': 88.82722825139396, 'fit_intercept': True, 'tol': 0.1968341141630425, 'solver': 'auto'}. Best is trial 21 with value: 64.56725747888089.\n",
      "[I 2023-09-06 10:20:08,156] Trial 24 finished with value: 64.57073498048618 and parameters: {'alpha': 92.60067504700586, 'fit_intercept': True, 'tol': 0.08660925508279546, 'solver': 'cholesky'}. Best is trial 21 with value: 64.56725747888089.\n",
      "[I 2023-09-06 10:20:08,206] Trial 25 finished with value: 64.56742462919995 and parameters: {'alpha': 99.56908712847289, 'fit_intercept': True, 'tol': 0.2347638551373764, 'solver': 'auto'}. Best is trial 21 with value: 64.56725747888089.\n",
      "[I 2023-09-06 10:20:08,295] Trial 26 finished with value: 64.57497733886267 and parameters: {'alpha': 84.33846183941719, 'fit_intercept': True, 'tol': 0.25606942928866117, 'solver': 'auto'}. Best is trial 21 with value: 64.56725747888089.\n",
      "[I 2023-09-06 10:20:08,351] Trial 27 finished with value: 64.58293245354072 and parameters: {'alpha': 70.56944683377412, 'fit_intercept': True, 'tol': 0.36934792843420605, 'solver': 'auto'}. Best is trial 21 with value: 64.56725747888089.\n",
      "[I 2023-09-06 10:20:08,409] Trial 28 finished with value: 64.57091441133744 and parameters: {'alpha': 92.23659545207882, 'fit_intercept': True, 'tol': 0.09988686224231083, 'solver': 'auto'}. Best is trial 21 with value: 64.56725747888089.\n",
      "[I 2023-09-06 10:20:08,481] Trial 29 finished with value: 69.84348884769206 and parameters: {'alpha': 84.53035420309278, 'fit_intercept': False, 'tol': 0.9292908154413171, 'solver': 'sag'}. Best is trial 21 with value: 64.56725747888089.\n",
      "[I 2023-09-06 10:20:08,481] A new study created in memory with name: RidgeRegression Optimization\n",
      "[I 2023-09-06 10:20:08,548] Trial 0 finished with value: 64.65341317431835 and parameters: {'alpha': 11.754596012668417, 'fit_intercept': False, 'tol': 0.03018288557349952, 'solver': 'svd'}. Best is trial 0 with value: 64.65341317431835.\n",
      "[I 2023-09-06 10:20:08,578] Trial 1 finished with value: 64.64939600794284 and parameters: {'alpha': 37.938301539245266, 'fit_intercept': False, 'tol': 0.0915013349567025, 'solver': 'cholesky'}. Best is trial 1 with value: 64.64939600794284.\n",
      "[I 2023-09-06 10:20:10,986] Trial 2 finished with value: 72.08060015245059 and parameters: {'alpha': 2.5142118772583433, 'fit_intercept': False, 'tol': 0.0026720111148197142, 'solver': 'sag'}. Best is trial 1 with value: 64.64939600794284.\n",
      "[I 2023-09-06 10:20:11,054] Trial 3 finished with value: 64.58256851083894 and parameters: {'alpha': 57.34502149281626, 'fit_intercept': True, 'tol': 0.0019295548858040503, 'solver': 'cholesky'}. Best is trial 3 with value: 64.58256851083894.\n",
      "[I 2023-09-06 10:20:14,782] Trial 4 finished with value: 71.72823655319274 and parameters: {'alpha': 95.94534038602295, 'fit_intercept': False, 'tol': 0.0013797334205006599, 'solver': 'sag'}. Best is trial 3 with value: 64.58256851083894.\n",
      "[I 2023-09-06 10:20:14,858] Trial 5 finished with value: 64.59115587905244 and parameters: {'alpha': 3.9757240397924387, 'fit_intercept': True, 'tol': 0.4996771368959891, 'solver': 'svd'}. Best is trial 3 with value: 64.58256851083894.\n",
      "[I 2023-09-06 10:20:14,921] Trial 6 finished with value: 72.78167735463909 and parameters: {'alpha': 42.58384925931421, 'fit_intercept': True, 'tol': 0.7068319647820448, 'solver': 'saga'}. Best is trial 3 with value: 64.58256851083894.\n",
      "[I 2023-09-06 10:20:14,986] Trial 7 finished with value: 64.65465935321576 and parameters: {'alpha': 4.017309489405796, 'fit_intercept': False, 'tol': 0.7059537947809449, 'solver': 'svd'}. Best is trial 3 with value: 64.58256851083894.\n",
      "[I 2023-09-06 10:20:16,262] Trial 8 finished with value: 73.45473122154847 and parameters: {'alpha': 97.8746302299699, 'fit_intercept': False, 'tol': 0.009838036937266651, 'solver': 'saga'}. Best is trial 3 with value: 64.58256851083894.\n",
      "[I 2023-09-06 10:20:16,341] Trial 9 finished with value: 64.58684582903325 and parameters: {'alpha': 29.770146935039843, 'fit_intercept': True, 'tol': 0.09553025071725484, 'solver': 'svd'}. Best is trial 3 with value: 64.58256851083894.\n",
      "[I 2023-09-06 10:20:16,395] Trial 10 finished with value: 64.5813123643151 and parameters: {'alpha': 65.85804131754638, 'fit_intercept': True, 'tol': 0.0011706289179944428, 'solver': 'cholesky'}. Best is trial 10 with value: 64.5813123643151.\n",
      "[I 2023-09-06 10:20:16,453] Trial 11 finished with value: 64.58138729192818 and parameters: {'alpha': 65.34464225762503, 'fit_intercept': True, 'tol': 0.0010355403277680735, 'solver': 'cholesky'}. Best is trial 10 with value: 64.5813123643151.\n",
      "[I 2023-09-06 10:20:16,504] Trial 12 finished with value: 64.58149428786068 and parameters: {'alpha': 64.61276148957828, 'fit_intercept': True, 'tol': 0.0010352966643490315, 'solver': 'cholesky'}. Best is trial 10 with value: 64.5813123643151.\n",
      "[I 2023-09-06 10:20:16,570] Trial 13 finished with value: 72.82879255205324 and parameters: {'alpha': 69.72705956548587, 'fit_intercept': True, 'tol': 0.004745315088241456, 'solver': 'lsqr'}. Best is trial 10 with value: 64.5813123643151.\n",
      "[I 2023-09-06 10:20:16,675] Trial 14 finished with value: 64.57963352180815 and parameters: {'alpha': 77.55425427738105, 'fit_intercept': True, 'tol': 0.004702431482206997, 'solver': 'auto'}. Best is trial 14 with value: 64.57963352180815.\n",
      "[I 2023-09-06 10:20:16,748] Trial 15 finished with value: 64.57926736074332 and parameters: {'alpha': 80.15550951972877, 'fit_intercept': True, 'tol': 0.005767415340678074, 'solver': 'auto'}. Best is trial 15 with value: 64.57926736074332.\n",
      "[I 2023-09-06 10:20:16,812] Trial 16 finished with value: 64.5788756799518 and parameters: {'alpha': 82.95856096247368, 'fit_intercept': True, 'tol': 0.007400088903423329, 'solver': 'auto'}. Best is trial 16 with value: 64.5788756799518.\n",
      "[I 2023-09-06 10:20:16,870] Trial 17 finished with value: 64.57890683342222 and parameters: {'alpha': 82.73482973716594, 'fit_intercept': True, 'tol': 0.009091289636904762, 'solver': 'auto'}. Best is trial 16 with value: 64.5788756799518.\n",
      "[I 2023-09-06 10:20:16,920] Trial 18 finished with value: 64.57847613925347 and parameters: {'alpha': 85.83999521123528, 'fit_intercept': True, 'tol': 0.017702250468684753, 'solver': 'auto'}. Best is trial 18 with value: 64.57847613925347.\n",
      "[I 2023-09-06 10:20:16,975] Trial 19 finished with value: 64.57803955237638 and parameters: {'alpha': 89.01453596262957, 'fit_intercept': True, 'tol': 0.018847534125687015, 'solver': 'auto'}. Best is trial 19 with value: 64.57803955237638.\n",
      "[I 2023-09-06 10:20:17,031] Trial 20 finished with value: 64.57789904278114 and parameters: {'alpha': 90.04204755851023, 'fit_intercept': True, 'tol': 0.018139239632787664, 'solver': 'auto'}. Best is trial 20 with value: 64.57789904278114.\n",
      "[I 2023-09-06 10:20:17,088] Trial 21 finished with value: 64.5779264881567 and parameters: {'alpha': 89.84112185570432, 'fit_intercept': True, 'tol': 0.0201823716271206, 'solver': 'auto'}. Best is trial 20 with value: 64.57789904278114.\n",
      "[I 2023-09-06 10:20:17,144] Trial 22 finished with value: 64.57759046089444 and parameters: {'alpha': 92.3086976100948, 'fit_intercept': True, 'tol': 0.018637963547043206, 'solver': 'auto'}. Best is trial 22 with value: 64.57759046089444.\n",
      "[I 2023-09-06 10:20:17,194] Trial 23 finished with value: 64.57662266297868 and parameters: {'alpha': 99.50883818893315, 'fit_intercept': True, 'tol': 0.044914589432507854, 'solver': 'auto'}. Best is trial 23 with value: 64.57662266297868.\n",
      "[I 2023-09-06 10:20:17,246] Trial 24 finished with value: 72.82879255205324 and parameters: {'alpha': 99.75772001365932, 'fit_intercept': True, 'tol': 0.04752952530128366, 'solver': 'lsqr'}. Best is trial 23 with value: 64.57662266297868.\n",
      "[I 2023-09-06 10:20:17,300] Trial 25 finished with value: 64.57767347944119 and parameters: {'alpha': 91.69752871870195, 'fit_intercept': True, 'tol': 0.054924661587349974, 'solver': 'auto'}. Best is trial 23 with value: 64.57662266297868.\n",
      "[I 2023-09-06 10:20:17,348] Trial 26 finished with value: 64.5765582826559 and parameters: {'alpha': 99.99280200008904, 'fit_intercept': True, 'tol': 0.05964264884819345, 'solver': 'auto'}. Best is trial 26 with value: 64.5765582826559.\n",
      "[I 2023-09-06 10:20:17,397] Trial 27 finished with value: 64.57659035477616 and parameters: {'alpha': 99.75162863150508, 'fit_intercept': True, 'tol': 0.18279751745597325, 'solver': 'auto'}. Best is trial 26 with value: 64.5765582826559.\n",
      "[I 2023-09-06 10:20:17,446] Trial 28 finished with value: 64.57667030355785 and parameters: {'alpha': 99.15111692541953, 'fit_intercept': True, 'tol': 0.2023796323438101, 'solver': 'auto'}. Best is trial 26 with value: 64.5765582826559.\n",
      "[I 2023-09-06 10:20:17,553] Trial 29 finished with value: 74.14588361807601 and parameters: {'alpha': 73.81840792094025, 'fit_intercept': False, 'tol': 0.20321330377375782, 'solver': 'sag'}. Best is trial 26 with value: 64.5765582826559.\n",
      "[I 2023-09-06 10:20:17,561] A new study created in memory with name: RandomForestRegression Optimization\n",
      "[I 2023-09-06 10:20:23,377] Trial 0 finished with value: 62.122422137116295 and parameters: {'n_estimators': 55, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 8}. Best is trial 0 with value: 62.122422137116295.\n",
      "[I 2023-09-06 10:20:32,637] Trial 1 finished with value: 68.52975274637802 and parameters: {'n_estimators': 55, 'max_depth': 26, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 0 with value: 62.122422137116295.\n",
      "[I 2023-09-06 10:20:35,913] Trial 2 finished with value: 84.69662925915303 and parameters: {'n_estimators': 18, 'max_depth': 29, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 0 with value: 62.122422137116295.\n",
      "[I 2023-09-06 10:20:37,734] Trial 3 finished with value: 62.87273679197377 and parameters: {'n_estimators': 24, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 0 with value: 62.122422137116295.\n",
      "[I 2023-09-06 10:20:53,045] Trial 4 finished with value: 61.64208238433055 and parameters: {'n_estimators': 125, 'max_depth': 19, 'min_samples_split': 6, 'min_samples_leaf': 9}. Best is trial 4 with value: 61.64208238433055.\n",
      "[I 2023-09-06 10:20:55,663] Trial 5 finished with value: 66.09896164890945 and parameters: {'n_estimators': 18, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 4 with value: 61.64208238433055.\n",
      "[I 2023-09-06 10:20:59,512] Trial 6 finished with value: 63.156807756753764 and parameters: {'n_estimators': 26, 'max_depth': 31, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 4 with value: 61.64208238433055.\n",
      "[I 2023-09-06 10:21:08,947] Trial 7 finished with value: 62.33234537153258 and parameters: {'n_estimators': 67, 'max_depth': 27, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 4 with value: 61.64208238433055.\n",
      "[I 2023-09-06 10:21:10,173] Trial 8 finished with value: 63.065405046817396 and parameters: {'n_estimators': 15, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 10}. Best is trial 4 with value: 61.64208238433055.\n",
      "[I 2023-09-06 10:21:19,394] Trial 9 finished with value: 64.4702149400623 and parameters: {'n_estimators': 72, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 4 with value: 61.64208238433055.\n",
      "[I 2023-09-06 10:21:19,397] A new study created in memory with name: RandomForestRegression Optimization\n",
      "[I 2023-09-06 10:21:20,539] Trial 0 finished with value: 63.11938631521713 and parameters: {'n_estimators': 10, 'max_depth': 22, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 0 with value: 63.11938631521713.\n",
      "[I 2023-09-06 10:21:32,858] Trial 1 finished with value: 62.4033896651386 and parameters: {'n_estimators': 131, 'max_depth': 11, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 1 with value: 62.4033896651386.\n",
      "[I 2023-09-06 10:21:35,518] Trial 2 finished with value: 63.98065145625751 and parameters: {'n_estimators': 25, 'max_depth': 31, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 1 with value: 62.4033896651386.\n",
      "[I 2023-09-06 10:21:42,724] Trial 3 finished with value: 62.71959420905725 and parameters: {'n_estimators': 66, 'max_depth': 29, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 1 with value: 62.4033896651386.\n",
      "[I 2023-09-06 10:21:43,695] Trial 4 finished with value: 64.282288095221 and parameters: {'n_estimators': 23, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 9}. Best is trial 1 with value: 62.4033896651386.\n",
      "[I 2023-09-06 10:21:51,860] Trial 5 finished with value: 63.83689191468405 and parameters: {'n_estimators': 197, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 1 with value: 62.4033896651386.\n",
      "[I 2023-09-06 10:21:58,955] Trial 6 finished with value: 63.688057299992614 and parameters: {'n_estimators': 173, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 1 with value: 62.4033896651386.\n",
      "[I 2023-09-06 10:21:59,992] Trial 7 finished with value: 63.11938631521713 and parameters: {'n_estimators': 10, 'max_depth': 28, 'min_samples_split': 9, 'min_samples_leaf': 7}. Best is trial 1 with value: 62.4033896651386.\n",
      "[I 2023-09-06 10:22:07,106] Trial 8 finished with value: 64.95428185264483 and parameters: {'n_estimators': 59, 'max_depth': 19, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 1 with value: 62.4033896651386.\n",
      "[I 2023-09-06 10:22:13,611] Trial 9 finished with value: 86.3796185519668 and parameters: {'n_estimators': 58, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 1 with value: 62.4033896651386.\n",
      "[I 2023-09-06 10:22:13,620] A new study created in memory with name: DecisionTreeRegressor Optimization\n",
      "[I 2023-09-06 10:22:13,829] Trial 0 finished with value: 125.20997142660737 and parameters: {'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 2, 'random_state': 25}. Best is trial 0 with value: 125.20997142660737.\n",
      "[I 2023-09-06 10:22:13,985] Trial 1 finished with value: 118.84413310150755 and parameters: {'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 2, 'random_state': 10}. Best is trial 1 with value: 118.84413310150755.\n",
      "[I 2023-09-06 10:22:14,075] Trial 2 finished with value: 114.44521958516259 and parameters: {'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 1, 'random_state': 2}. Best is trial 2 with value: 114.44521958516259.\n",
      "[I 2023-09-06 10:22:14,223] Trial 3 finished with value: 82.99570115338638 and parameters: {'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 7, 'random_state': 33}. Best is trial 3 with value: 82.99570115338638.\n",
      "[I 2023-09-06 10:22:14,420] Trial 4 finished with value: 83.26980202582091 and parameters: {'max_depth': 32, 'min_samples_split': 10, 'min_samples_leaf': 8, 'random_state': 3}. Best is trial 3 with value: 82.99570115338638.\n",
      "[I 2023-09-06 10:22:14,701] Trial 5 finished with value: 132.1535268851682 and parameters: {'max_depth': 21, 'min_samples_split': 7, 'min_samples_leaf': 2, 'random_state': 46}. Best is trial 3 with value: 82.99570115338638.\n",
      "[I 2023-09-06 10:22:14,895] Trial 6 finished with value: 95.61669874883364 and parameters: {'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 6, 'random_state': 7}. Best is trial 3 with value: 82.99570115338638.\n",
      "[I 2023-09-06 10:22:15,157] Trial 7 finished with value: 246.19465651863584 and parameters: {'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'random_state': 15}. Best is trial 3 with value: 82.99570115338638.\n",
      "[I 2023-09-06 10:22:15,313] Trial 8 finished with value: 82.99565731474146 and parameters: {'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 7, 'random_state': 13}. Best is trial 8 with value: 82.99565731474146.\n",
      "[I 2023-09-06 10:22:15,573] Trial 9 finished with value: 151.476089748725 and parameters: {'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 2, 'random_state': 47}. Best is trial 8 with value: 82.99565731474146.\n",
      "[I 2023-09-06 10:22:15,728] Trial 10 finished with value: 68.61108761907923 and parameters: {'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 21}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:15,857] Trial 11 finished with value: 68.61108761907923 and parameters: {'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 9, 'random_state': 21}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:15,978] Trial 12 finished with value: 67.68042817542546 and parameters: {'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 24}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:16,118] Trial 13 finished with value: 68.25321700429899 and parameters: {'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 10, 'random_state': 34}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:16,323] Trial 14 finished with value: 78.00046131742559 and parameters: {'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 10, 'random_state': 35}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:16,577] Trial 15 finished with value: 106.55067334614107 and parameters: {'max_depth': 22, 'min_samples_split': 4, 'min_samples_leaf': 4, 'random_state': 33}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:16,742] Trial 16 finished with value: 75.98820915432636 and parameters: {'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 9, 'random_state': 37}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:16,977] Trial 17 finished with value: 84.27405127861877 and parameters: {'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 5, 'random_state': 41}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:17,124] Trial 18 finished with value: 73.51097928468418 and parameters: {'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 9, 'random_state': 28}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:17,359] Trial 19 finished with value: 83.27975966103999 and parameters: {'max_depth': 21, 'min_samples_split': 5, 'min_samples_leaf': 8, 'random_state': 28}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:17,580] Trial 20 finished with value: 77.82110325839854 and parameters: {'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 10, 'random_state': 19}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:17,705] Trial 21 finished with value: 68.61108761907923 and parameters: {'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 22}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:17,848] Trial 22 finished with value: 71.60185026061025 and parameters: {'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 8, 'random_state': 29}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:17,994] Trial 23 finished with value: 68.25321700429899 and parameters: {'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 10, 'random_state': 40}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:18,158] Trial 24 finished with value: 75.98820915432636 and parameters: {'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 9, 'random_state': 41}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:18,353] Trial 25 finished with value: 85.0278020181347 and parameters: {'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 7, 'random_state': 40}. Best is trial 12 with value: 67.68042817542546.\n",
      "[I 2023-09-06 10:22:18,582] Trial 26 finished with value: 66.11341454266656 and parameters: {'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 10, 'random_state': 50}. Best is trial 26 with value: 66.11341454266656.\n",
      "[I 2023-09-06 10:22:18,803] Trial 27 finished with value: 101.32209443375812 and parameters: {'max_depth': 13, 'min_samples_split': 8, 'min_samples_leaf': 4, 'random_state': 50}. Best is trial 26 with value: 66.11341454266656.\n",
      "[I 2023-09-06 10:22:19,021] Trial 28 finished with value: 85.36861894667754 and parameters: {'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 9, 'random_state': 45}. Best is trial 26 with value: 66.11341454266656.\n",
      "[I 2023-09-06 10:22:19,166] Trial 29 finished with value: 70.21388582476332 and parameters: {'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 8, 'random_state': 17}. Best is trial 26 with value: 66.11341454266656.\n",
      "[I 2023-09-06 10:22:19,171] A new study created in memory with name: DecisionTreeRegressor Optimization\n",
      "[I 2023-09-06 10:22:19,330] Trial 0 finished with value: 107.48315951523035 and parameters: {'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 3, 'random_state': 32}. Best is trial 0 with value: 107.48315951523035.\n",
      "[I 2023-09-06 10:22:19,474] Trial 1 finished with value: 79.44184901115291 and parameters: {'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 10, 'random_state': 2}. Best is trial 1 with value: 79.44184901115291.\n",
      "[I 2023-09-06 10:22:19,661] Trial 2 finished with value: 134.34208838482508 and parameters: {'max_depth': 22, 'min_samples_split': 6, 'min_samples_leaf': 2, 'random_state': 43}. Best is trial 1 with value: 79.44184901115291.\n",
      "[I 2023-09-06 10:22:19,840] Trial 3 finished with value: 110.56333347654191 and parameters: {'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 3, 'random_state': 26}. Best is trial 1 with value: 79.44184901115291.\n",
      "[I 2023-09-06 10:22:20,011] Trial 4 finished with value: 104.05238179549254 and parameters: {'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 4, 'random_state': 0}. Best is trial 1 with value: 79.44184901115291.\n",
      "[I 2023-09-06 10:22:20,169] Trial 5 finished with value: 87.18333258163503 and parameters: {'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 8, 'random_state': 39}. Best is trial 1 with value: 79.44184901115291.\n",
      "[I 2023-09-06 10:22:20,321] Trial 6 finished with value: 101.43077920992773 and parameters: {'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 4, 'random_state': 31}. Best is trial 1 with value: 79.44184901115291.\n",
      "[I 2023-09-06 10:22:20,489] Trial 7 finished with value: 88.26352818352431 and parameters: {'max_depth': 30, 'min_samples_split': 7, 'min_samples_leaf': 5, 'random_state': 36}. Best is trial 1 with value: 79.44184901115291.\n",
      "[I 2023-09-06 10:22:20,654] Trial 8 finished with value: 88.6731345248614 and parameters: {'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 9, 'random_state': 30}. Best is trial 1 with value: 79.44184901115291.\n",
      "[I 2023-09-06 10:22:20,808] Trial 9 finished with value: 88.48116723733045 and parameters: {'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 7, 'random_state': 29}. Best is trial 1 with value: 79.44184901115291.\n",
      "[I 2023-09-06 10:22:20,905] Trial 10 finished with value: 68.61108761907923 and parameters: {'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 8}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:20,995] Trial 11 finished with value: 68.61108761907923 and parameters: {'max_depth': 2, 'min_samples_split': 4, 'min_samples_leaf': 10, 'random_state': 4}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:21,094] Trial 12 finished with value: 68.61108761907923 and parameters: {'max_depth': 2, 'min_samples_split': 4, 'min_samples_leaf': 10, 'random_state': 12}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:21,186] Trial 13 finished with value: 68.61108761907923 and parameters: {'max_depth': 2, 'min_samples_split': 4, 'min_samples_leaf': 8, 'random_state': 15}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:21,386] Trial 14 finished with value: 83.86266451192995 and parameters: {'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 7, 'random_state': 12}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:21,524] Trial 15 finished with value: 73.95952496236386 and parameters: {'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 17}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:21,655] Trial 16 finished with value: 87.61427724622216 and parameters: {'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 6, 'random_state': 6}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:21,777] Trial 17 finished with value: 74.71814765285613 and parameters: {'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 9, 'random_state': 50}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:21,942] Trial 18 finished with value: 81.3531933002875 and parameters: {'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 8, 'random_state': 19}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:22,039] Trial 19 finished with value: 69.30381950972965 and parameters: {'max_depth': 2, 'min_samples_split': 3, 'min_samples_leaf': 1, 'random_state': 6}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:22,238] Trial 20 finished with value: 88.7258702241837 and parameters: {'max_depth': 25, 'min_samples_split': 8, 'min_samples_leaf': 9, 'random_state': 23}. Best is trial 10 with value: 68.61108761907923.\n",
      "[I 2023-09-06 10:22:22,358] Trial 21 finished with value: 67.08932809589209 and parameters: {'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 10, 'random_state': 10}. Best is trial 21 with value: 67.08932809589209.\n",
      "[I 2023-09-06 10:22:22,490] Trial 22 finished with value: 69.4306482074766 and parameters: {'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 10, 'random_state': 9}. Best is trial 21 with value: 67.08932809589209.\n",
      "[I 2023-09-06 10:22:22,613] Trial 23 finished with value: 72.34421111540814 and parameters: {'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 9, 'random_state': 4}. Best is trial 21 with value: 67.08932809589209.\n",
      "[I 2023-09-06 10:22:22,787] Trial 24 finished with value: 86.29616934383564 and parameters: {'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 7, 'random_state': 9}. Best is trial 21 with value: 67.08932809589209.\n",
      "[I 2023-09-06 10:22:22,907] Trial 25 finished with value: 67.08932809589209 and parameters: {'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 10, 'random_state': 22}. Best is trial 21 with value: 67.08932809589209.\n",
      "[I 2023-09-06 10:22:23,071] Trial 26 finished with value: 81.36979909850507 and parameters: {'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 8, 'random_state': 21}. Best is trial 21 with value: 67.08932809589209.\n",
      "[I 2023-09-06 10:22:23,225] Trial 27 finished with value: 74.71814765285613 and parameters: {'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 9, 'random_state': 15}. Best is trial 21 with value: 67.08932809589209.\n",
      "[I 2023-09-06 10:22:23,411] Trial 28 finished with value: 96.09901913593146 and parameters: {'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 6, 'random_state': 25}. Best is trial 21 with value: 67.08932809589209.\n",
      "[I 2023-09-06 10:22:23,584] Trial 29 finished with value: 77.22967330098456 and parameters: {'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 11}. Best is trial 21 with value: 67.08932809589209.\n",
      "[I 2023-09-06 10:22:23,587] A new study created in memory with name: catboosterregressor Optimization\n",
      "[I 2023-09-06 10:22:30,478] Trial 15 finished with value: 57.04712498140329 and parameters: {'learning_rate': 0.03342589645070349, 'depth': 2, 'l2_leaf_reg': 1.3040276597456244}. Best is trial 15 with value: 57.04712498140329.\n",
      "[I 2023-09-06 10:22:31,435] Trial 12 finished with value: 53.09830350276416 and parameters: {'learning_rate': 0.09521825795047058, 'depth': 2, 'l2_leaf_reg': 1.962303640847587}. Best is trial 12 with value: 53.09830350276416.\n",
      "[I 2023-09-06 10:22:31,443] Trial 10 finished with value: 53.731986754282985 and parameters: {'learning_rate': 0.08543230746955233, 'depth': 2, 'l2_leaf_reg': 2.801451346329026}. Best is trial 12 with value: 53.09830350276416.\n",
      "[I 2023-09-06 10:22:32,332] Trial 13 finished with value: 47.020483229872916 and parameters: {'learning_rate': 0.06963552691822165, 'depth': 3, 'l2_leaf_reg': 1.4605473264230158}. Best is trial 13 with value: 47.020483229872916.\n",
      "[I 2023-09-06 10:22:34,957] Trial 4 finished with value: 48.17207284041206 and parameters: {'learning_rate': 0.03048932902725293, 'depth': 4, 'l2_leaf_reg': 1.7238188891963195}. Best is trial 13 with value: 47.020483229872916.\n",
      "[I 2023-09-06 10:22:35,854] Trial 9 finished with value: 42.438146108567835 and parameters: {'learning_rate': 0.07561314758997206, 'depth': 4, 'l2_leaf_reg': 2.5754413757538632}. Best is trial 9 with value: 42.438146108567835.\n",
      "[I 2023-09-06 10:22:35,867] Trial 0 finished with value: 36.36809446690497 and parameters: {'learning_rate': 0.08005024494400508, 'depth': 4, 'l2_leaf_reg': 0.41061677491460397}. Best is trial 0 with value: 36.36809446690497.\n",
      "[I 2023-09-06 10:22:35,885] Trial 1 finished with value: 44.26529667044179 and parameters: {'learning_rate': 0.04920339461693525, 'depth': 4, 'l2_leaf_reg': 1.1679551230937646}. Best is trial 0 with value: 36.36809446690497.\n",
      "[I 2023-09-06 10:22:35,964] Trial 6 finished with value: 45.429571703115926 and parameters: {'learning_rate': 0.03898410775540266, 'depth': 4, 'l2_leaf_reg': 1.0939736283164962}. Best is trial 0 with value: 36.36809446690497.\n",
      "[I 2023-09-06 10:22:40,688] Trial 18 finished with value: 42.355342942351896 and parameters: {'learning_rate': 0.08822364308369916, 'depth': 3, 'l2_leaf_reg': 0.43195366209701147}. Best is trial 0 with value: 36.36809446690497.\n",
      "[I 2023-09-06 10:22:42,689] Trial 11 finished with value: 32.83135294543038 and parameters: {'learning_rate': 0.09550298118977886, 'depth': 6, 'l2_leaf_reg': 2.5588088307962646}. Best is trial 11 with value: 32.83135294543038.\n",
      "[I 2023-09-06 10:22:42,745] Trial 2 finished with value: 34.2612825369685 and parameters: {'learning_rate': 0.07466178525974598, 'depth': 6, 'l2_leaf_reg': 2.201491354607609}. Best is trial 11 with value: 32.83135294543038.\n",
      "[I 2023-09-06 10:22:43,876] Trial 16 finished with value: 32.934690099330254 and parameters: {'learning_rate': 0.09876333133875821, 'depth': 5, 'l2_leaf_reg': 2.1167407326701957}. Best is trial 11 with value: 32.83135294543038.\n",
      "[I 2023-09-06 10:22:47,600] Trial 14 finished with value: 25.938615555584636 and parameters: {'learning_rate': 0.08334024402719456, 'depth': 7, 'l2_leaf_reg': 1.539938330049436}. Best is trial 14 with value: 25.938615555584636.\n",
      "[I 2023-09-06 10:22:48,383] Trial 21 finished with value: 36.512296083236265 and parameters: {'learning_rate': 0.08973052104963887, 'depth': 4, 'l2_leaf_reg': 0.9907039907070578}. Best is trial 14 with value: 25.938615555584636.\n",
      "[I 2023-09-06 10:22:50,433] Trial 17 finished with value: 43.56624245799145 and parameters: {'learning_rate': 0.03062430916249179, 'depth': 6, 'l2_leaf_reg': 2.257788755227235}. Best is trial 14 with value: 25.938615555584636.\n",
      "[I 2023-09-06 10:22:51,199] Trial 3 finished with value: 23.693802709980496 and parameters: {'learning_rate': 0.07963146200779682, 'depth': 7, 'l2_leaf_reg': 0.9917808573339573}. Best is trial 3 with value: 23.693802709980496.\n",
      "[I 2023-09-06 10:23:01,008] Trial 7 finished with value: 31.21414935324067 and parameters: {'learning_rate': 0.06454149062493854, 'depth': 8, 'l2_leaf_reg': 2.844083447509256}. Best is trial 3 with value: 23.693802709980496.\n",
      "[I 2023-09-06 10:23:12,999] Trial 26 finished with value: 16.691021850103276 and parameters: {'learning_rate': 0.0978331326734531, 'depth': 7, 'l2_leaf_reg': 0.33374715476666056}. Best is trial 26 with value: 16.691021850103276.\n",
      "[I 2023-09-06 10:23:14,791] Trial 19 finished with value: 32.0178255406489 and parameters: {'learning_rate': 0.059179693115994816, 'depth': 8, 'l2_leaf_reg': 2.7192942060182195}. Best is trial 26 with value: 16.691021850103276.\n",
      "[I 2023-09-06 10:23:27,954] Trial 27 finished with value: 21.700752701942363 and parameters: {'learning_rate': 0.09922338462445489, 'depth': 8, 'l2_leaf_reg': 2.182569066182127}. Best is trial 26 with value: 16.691021850103276.\n",
      "[I 2023-09-06 10:23:30,463] Trial 5 finished with value: 16.87227976931217 and parameters: {'learning_rate': 0.09023211320267482, 'depth': 9, 'l2_leaf_reg': 1.8302997693408292}. Best is trial 26 with value: 16.691021850103276.\n",
      "[I 2023-09-06 10:23:31,034] Trial 31 finished with value: 24.783266784107283 and parameters: {'learning_rate': 0.09893676873355797, 'depth': 8, 'l2_leaf_reg': 2.9588336178469463}. Best is trial 26 with value: 16.691021850103276.\n",
      "[I 2023-09-06 10:23:38,194] Trial 20 finished with value: 9.901990717769834 and parameters: {'learning_rate': 0.08431293609093718, 'depth': 9, 'l2_leaf_reg': 0.24909876517215812}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:23:41,847] Trial 24 finished with value: 25.343037367835038 and parameters: {'learning_rate': 0.06399224769228913, 'depth': 9, 'l2_leaf_reg': 2.3115113055064516}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:23:42,129] Trial 23 finished with value: 21.921191766536005 and parameters: {'learning_rate': 0.04472904865584734, 'depth': 9, 'l2_leaf_reg': 0.5645487133315354}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:23:43,067] Trial 28 finished with value: 19.908265064688862 and parameters: {'learning_rate': 0.09963627371419312, 'depth': 9, 'l2_leaf_reg': 2.9490262929536946}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:23:45,439] Trial 29 finished with value: 21.01258021632644 and parameters: {'learning_rate': 0.09061646144034889, 'depth': 9, 'l2_leaf_reg': 2.4918705227153444}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:23:45,503] Trial 25 finished with value: 17.115239812269895 and parameters: {'learning_rate': 0.05481627355993967, 'depth': 9, 'l2_leaf_reg': 0.32851032633982125}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:23:45,607] Trial 22 finished with value: 30.55654679439025 and parameters: {'learning_rate': 0.04284938987129075, 'depth': 9, 'l2_leaf_reg': 2.0084115931366657}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:23:47,607] Trial 32 finished with value: 18.514777876002597 and parameters: {'learning_rate': 0.06030730918748197, 'depth': 9, 'l2_leaf_reg': 0.8638688361327174}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:23:49,934] Trial 30 finished with value: 19.617903498621917 and parameters: {'learning_rate': 0.09961209723845793, 'depth': 9, 'l2_leaf_reg': 2.9417302177553895}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:23:50,367] Trial 33 finished with value: 17.75501699002172 and parameters: {'learning_rate': 0.06485326193582261, 'depth': 9, 'l2_leaf_reg': 0.7933876250567299}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:23:53,848] Trial 34 finished with value: 10.115389487013216 and parameters: {'learning_rate': 0.09910835466422063, 'depth': 9, 'l2_leaf_reg': 0.6502007786380898}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:24:10,609] Trial 8 finished with value: 17.91416740307038 and parameters: {'learning_rate': 0.0885493888726912, 'depth': 10, 'l2_leaf_reg': 2.719689233350054}. Best is trial 20 with value: 9.901990717769834.\n",
      "[I 2023-09-06 10:24:10,619] A new study created in memory with name: catboosterregressor Optimization\n",
      "[I 2023-09-06 10:24:16,906] Trial 3 finished with value: 54.34994176657173 and parameters: {'learning_rate': 0.06738117539008957, 'depth': 2, 'l2_leaf_reg': 1.1978592039500526}. Best is trial 3 with value: 54.34994176657173.\n",
      "[I 2023-09-06 10:24:16,921] Trial 12 finished with value: 52.843517852980845 and parameters: {'learning_rate': 0.08388506669314869, 'depth': 2, 'l2_leaf_reg': 0.5002126792233985}. Best is trial 12 with value: 52.843517852980845.\n",
      "[I 2023-09-06 10:24:17,795] Trial 6 finished with value: 54.90751853257839 and parameters: {'learning_rate': 0.06707246169083009, 'depth': 2, 'l2_leaf_reg': 2.3335175149120047}. Best is trial 12 with value: 52.843517852980845.\n",
      "[I 2023-09-06 10:24:19,841] Trial 14 finished with value: 48.47770106691373 and parameters: {'learning_rate': 0.05027782643658013, 'depth': 3, 'l2_leaf_reg': 1.0364463432020836}. Best is trial 14 with value: 48.47770106691373.\n",
      "[I 2023-09-06 10:24:23,551] Trial 10 finished with value: 44.14964239328873 and parameters: {'learning_rate': 0.04414983580884811, 'depth': 5, 'l2_leaf_reg': 2.927139555785145}. Best is trial 10 with value: 44.14964239328873.\n",
      "[I 2023-09-06 10:24:26,024] Trial 18 finished with value: 43.87413762308338 and parameters: {'learning_rate': 0.08719769009244073, 'depth': 3, 'l2_leaf_reg': 1.3512870278804463}. Best is trial 18 with value: 43.87413762308338.\n",
      "[I 2023-09-06 10:24:29,298] Trial 7 finished with value: 30.128174915128373 and parameters: {'learning_rate': 0.07565041888892998, 'depth': 5, 'l2_leaf_reg': 0.22296345959144143}. Best is trial 7 with value: 30.128174915128373.\n",
      "[I 2023-09-06 10:24:33,256] Trial 19 finished with value: 31.60065501535644 and parameters: {'learning_rate': 0.09312827829075183, 'depth': 5, 'l2_leaf_reg': 1.75046461898763}. Best is trial 7 with value: 30.128174915128373.\n",
      "[I 2023-09-06 10:24:35,049] Trial 4 finished with value: 26.990139154222998 and parameters: {'learning_rate': 0.08400318621383927, 'depth': 7, 'l2_leaf_reg': 2.077479243097812}. Best is trial 4 with value: 26.990139154222998.\n",
      "[I 2023-09-06 10:24:35,843] Trial 0 finished with value: 38.874134372428465 and parameters: {'learning_rate': 0.03118251712843944, 'depth': 6, 'l2_leaf_reg': 0.6827441708505153}. Best is trial 4 with value: 26.990139154222998.\n",
      "[I 2023-09-06 10:24:35,894] Trial 1 finished with value: 34.8961745048034 and parameters: {'learning_rate': 0.06683983677368911, 'depth': 7, 'l2_leaf_reg': 2.962663841641901}. Best is trial 4 with value: 26.990139154222998.\n",
      "[I 2023-09-06 10:24:35,950] Trial 9 finished with value: 23.378414666237422 and parameters: {'learning_rate': 0.08490504599719248, 'depth': 7, 'l2_leaf_reg': 1.1309996252073728}. Best is trial 9 with value: 23.378414666237422.\n",
      "[I 2023-09-06 10:24:37,484] Trial 5 finished with value: 25.074627977633313 and parameters: {'learning_rate': 0.08673134861703063, 'depth': 7, 'l2_leaf_reg': 1.5958220993595895}. Best is trial 9 with value: 23.378414666237422.\n",
      "[I 2023-09-06 10:24:41,016] Trial 22 finished with value: 47.10912964283908 and parameters: {'learning_rate': 0.07742838445499256, 'depth': 3, 'l2_leaf_reg': 2.1696587655538013}. Best is trial 9 with value: 23.378414666237422.\n",
      "[I 2023-09-06 10:24:41,862] Trial 17 finished with value: 31.855414641013663 and parameters: {'learning_rate': 0.05396492891499518, 'depth': 7, 'l2_leaf_reg': 1.571912072302109}. Best is trial 9 with value: 23.378414666237422.\n",
      "[I 2023-09-06 10:24:41,926] Trial 16 finished with value: 27.493396463493553 and parameters: {'learning_rate': 0.0715925495549628, 'depth': 7, 'l2_leaf_reg': 1.3469194274301726}. Best is trial 9 with value: 23.378414666237422.\n",
      "[I 2023-09-06 10:24:43,407] Trial 23 finished with value: 42.94217222217084 and parameters: {'learning_rate': 0.08613918374381352, 'depth': 3, 'l2_leaf_reg': 0.5415991573778101}. Best is trial 9 with value: 23.378414666237422.\n",
      "[I 2023-09-06 10:24:46,055] Trial 21 finished with value: 33.783847474493975 and parameters: {'learning_rate': 0.05876910013761306, 'depth': 6, 'l2_leaf_reg': 0.9691831693798678}. Best is trial 9 with value: 23.378414666237422.\n",
      "[I 2023-09-06 10:24:46,477] Trial 8 finished with value: 21.08495707667949 and parameters: {'learning_rate': 0.09524043155470448, 'depth': 7, 'l2_leaf_reg': 1.064510751056794}. Best is trial 8 with value: 21.08495707667949.\n",
      "[I 2023-09-06 10:24:50,127] Trial 11 finished with value: 27.494926876977136 and parameters: {'learning_rate': 0.055528012633554794, 'depth': 8, 'l2_leaf_reg': 1.2986726120407606}. Best is trial 8 with value: 21.08495707667949.\n",
      "[I 2023-09-06 10:25:04,803] Trial 20 finished with value: 24.28886528260034 and parameters: {'learning_rate': 0.08716421235189915, 'depth': 8, 'l2_leaf_reg': 2.384394335474144}. Best is trial 8 with value: 21.08495707667949.\n",
      "[I 2023-09-06 10:25:17,901] Trial 2 finished with value: 18.82123978953845 and parameters: {'learning_rate': 0.07595430900191952, 'depth': 9, 'l2_leaf_reg': 1.4584076196266775}. Best is trial 2 with value: 18.82123978953845.\n",
      "[I 2023-09-06 10:25:52,464] Trial 30 finished with value: 11.708163290110072 and parameters: {'learning_rate': 0.09641877289293979, 'depth': 9, 'l2_leaf_reg': 0.9370276211109084}. Best is trial 30 with value: 11.708163290110072.\n",
      "[I 2023-09-06 10:27:23,725] Trial 24 finished with value: 17.965339349218535 and parameters: {'learning_rate': 0.09372940928375159, 'depth': 10, 'l2_leaf_reg': 2.713921638504979}. Best is trial 30 with value: 11.708163290110072.\n",
      "[I 2023-09-06 10:27:25,361] Trial 27 finished with value: 10.50636139744716 and parameters: {'learning_rate': 0.09983554307824723, 'depth': 10, 'l2_leaf_reg': 1.7719249841719913}. Best is trial 27 with value: 10.50636139744716.\n",
      "[I 2023-09-06 10:27:27,260] Trial 13 finished with value: 17.14830177612084 and parameters: {'learning_rate': 0.07469086661962918, 'depth': 10, 'l2_leaf_reg': 1.9716329038387383}. Best is trial 27 with value: 10.50636139744716.\n",
      "[I 2023-09-06 10:27:27,335] Trial 26 finished with value: 11.502316771143922 and parameters: {'learning_rate': 0.09832827976168873, 'depth': 10, 'l2_leaf_reg': 1.693536408422534}. Best is trial 27 with value: 10.50636139744716.\n",
      "[I 2023-09-06 10:27:29,055] Trial 31 finished with value: 7.158534636881756 and parameters: {'learning_rate': 0.09976812034949822, 'depth': 10, 'l2_leaf_reg': 0.9064760539618868}. Best is trial 31 with value: 7.158534636881756.\n",
      "[I 2023-09-06 10:27:34,151] Trial 15 finished with value: 7.523592872724358 and parameters: {'learning_rate': 0.07150877857273717, 'depth': 10, 'l2_leaf_reg': 0.2631715443022544}. Best is trial 31 with value: 7.158534636881756.\n",
      "[I 2023-09-06 10:27:38,378] Trial 28 finished with value: 10.128342757333083 and parameters: {'learning_rate': 0.09879864863834496, 'depth': 10, 'l2_leaf_reg': 1.576004712779204}. Best is trial 31 with value: 7.158534636881756.\n",
      "[I 2023-09-06 10:27:40,303] Trial 33 finished with value: 11.08537738837447 and parameters: {'learning_rate': 0.09677599895540116, 'depth': 10, 'l2_leaf_reg': 1.6818713887066266}. Best is trial 31 with value: 7.158534636881756.\n",
      "[I 2023-09-06 10:27:40,315] Trial 32 finished with value: 7.25793268400704 and parameters: {'learning_rate': 0.09982619325642289, 'depth': 10, 'l2_leaf_reg': 0.9060243161756825}. Best is trial 31 with value: 7.158534636881756.\n",
      "[I 2023-09-06 10:27:40,906] Trial 25 finished with value: 12.254666463716802 and parameters: {'learning_rate': 0.09408960748917836, 'depth': 10, 'l2_leaf_reg': 1.873103169135705}. Best is trial 31 with value: 7.158534636881756.\n",
      "[I 2023-09-06 10:27:41,173] Trial 29 finished with value: 10.744898550516336 and parameters: {'learning_rate': 0.09832139344386528, 'depth': 10, 'l2_leaf_reg': 1.5230293801133745}. Best is trial 31 with value: 7.158534636881756.\n",
      "[I 2023-09-06 10:27:42,128] Trial 34 finished with value: 7.290919436728067 and parameters: {'learning_rate': 0.099408681635056, 'depth': 10, 'l2_leaf_reg': 0.8705940198635138}. Best is trial 31 with value: 7.158534636881756.\n"
     ]
    }
   ],
   "source": [
    "hi = run_regression_models(X_train_scaled, y_train, featurenames)\n",
    "    # pull the concat best data\n",
    "    # pull out the features that were lassoed \n",
    "    #OUTPUT ===== return concat_best_data, lassoed_features\n",
    "    \n",
    "    ## pull out features that are used.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import math\n",
    "import catboost\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna \n",
    "\n",
    "# Feel free to add all the libraries you need\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/ordinalencoded.csv', index_col=0)\n",
    "df = df1[df1['international_flight']==0]\n",
    "# define features and target\n",
    "# if you want to create a polynomial you do so at the start and recreate the dataframe\n",
    "features = df.columns.tolist()\n",
    "features.remove('target')\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "featurenames = list(X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12, test_size=0.5)\n",
    "#df1 = pd.read_csv('data/ordinalencoded_expandeddataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaler is applied outside of function\n",
    "# standard scaler \n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## pull the column names immediately they are useful\n",
    "### new definition\n",
    "def run_regression_models(x234_array, y234_array, features):\n",
    "## applies lasso optimization to the data set for feature selection\n",
    "## you have two trains --- lassoed_train & X_train \n",
    "## make sure you make a list of the feature in the lassoed train... \n",
    "\n",
    "## how many of each iteration do you want \n",
    "    ridge_iter = 30\n",
    "    ran_forest_iter = 10\n",
    "    SVR_iter = 1\n",
    "    dec_tree_iter = 30\n",
    "    cat_boost_iter = 35\n",
    "    \n",
    "## apply the other optimizations to both the lassoed X-train adn the x-train\n",
    "    ## \n",
    "    lasso_opti = lasso_optimization(x234_array, y234_array, [0.01, 0.08,0.12, 0.15,1,10])\n",
    "    # output of lasso optimization is a dataframe of with all the attempts, pull out the best \n",
    "    ## min score function???\n",
    "    lasso_bestvalue = find_best_value(lasso_opti)\n",
    "    # get the lassoed array and the list of its features\n",
    "    ## lasso features for later\n",
    "    [lassoed_features, lassoed_X_Train] = get_lassoed_array(lasso_bestvalue, features)\n",
    "    \n",
    "    # Ridge Regression\n",
    "    ridge_all_features = ridge_optimization(x234_array, y234_array, ridge_iter)\n",
    "    ridge_lasso_features = ridge_optimization(lassoed_X_Train, y234_array, ridge_iter)\n",
    "    \n",
    "    ridge_all_features['opti_run']= 'ridge_all_features'\n",
    "    ridge_lasso_features['opti_run']= 'ridge_lasso_features'\n",
    "    \n",
    "    # RandomForestRegressor_optimization\n",
    "    ran_forest_all_features = RandomForestRegressor_optimization(x234_array, y234_array, ran_forest_iter)\n",
    "    ran_forest_lasso_features = RandomForestRegressor_optimization(lassoed_X_Train, y234_array, ran_forest_iter)\n",
    "    \n",
    "    ran_forest_all_features['opti_run']= 'ran_forest_all_features'\n",
    "    ran_forest_lasso_features['opti_run']= 'ran_forest_lasso_features'\n",
    "    \n",
    "    # SVRegressor_optimization\n",
    "    #SVR_all_features = SVRegressor_optimization(x234_array, y234_array, SVR_iter)\n",
    "    #SVR_lasso_features = SVRegressor_optimization(lassoed_X_Train, y234_array, SVR_iter)\n",
    "    #\n",
    "    #SVR_all_features['opti_run']= 'SVR_all_features'\n",
    "    #SVR_lasso_features['opti_run']= 'SVR_lasso_features'\n",
    "    \n",
    "    # DecisionTreeRegressor_optimization\n",
    "    Dec_Tree_all_features = DecisionTreeRegressor_optimization(x234_array, y234_array, dec_tree_iter)\n",
    "    Dec_Tree_lasso_features = DecisionTreeRegressor_optimization(lassoed_X_Train, y234_array, dec_tree_iter)\n",
    "    \n",
    "    Dec_Tree_all_features['opti_run']= 'Dec_Tree_all_features'\n",
    "    Dec_Tree_lasso_features['opti_run']= 'Dec_Tree_lasso_features'\n",
    "    \n",
    "    # CatBooster_optimization\n",
    "    catBoost_all_features = catboosterregressor_optimization(X_train_scaled, y_train, number_of_trials=cat_boost_iter, iter_numnber=150)\n",
    "    catBoost_lasso_features = catboosterregressor_optimization(lassoed_X_Train, y_train, number_of_trials=cat_boost_iter, iter_numnber=150)\n",
    "    \n",
    "    catBoost_all_features['opti_run']= 'catBoost_all_features'\n",
    "    catBoost_lasso_features['opti_run']= 'catBoost_lasso_features'\n",
    "    \n",
    "    concat_all_data = pd.concat([\n",
    "        ridge_all_features, \n",
    "        ridge_lasso_features,\n",
    "        ran_forest_all_features,\n",
    "        ran_forest_lasso_features,\n",
    "        #SVR_all_features,\n",
    "        #SVR_lasso_features,\n",
    "        Dec_Tree_all_features,\n",
    "        Dec_Tree_lasso_features,\n",
    "        catBoost_all_features,\n",
    "        catBoost_lasso_features\n",
    "        ])\n",
    "    \n",
    "    concat_best_data = pd.concat([\n",
    "        find_best_value(ridge_all_features), \n",
    "        find_best_value(ridge_lasso_features),\n",
    "        find_best_value(ran_forest_all_features),\n",
    "        find_best_value(ran_forest_lasso_features),\n",
    "        #find_best_value(SVR_all_features),\n",
    "        #find_best_value(SVR_lasso_features),\n",
    "        find_best_value(Dec_Tree_all_features),\n",
    "        find_best_value(Dec_Tree_lasso_features),\n",
    "        find_best_value(catBoost_all_features),\n",
    "        find_best_value(catBoost_lasso_features)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "# Elastic net\n",
    "# XGBoost\n",
    "# CatBoost\n",
    "    \n",
    "    # pull the concat best data\n",
    "    # pull out the features that were lassoed \n",
    "    return concat_best_data, lassoed_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USEFUL FUNCTIONS\n",
    "def find_best_value(output_from_opti):\n",
    "    bestValue = output_from_opti[output_from_opti['combined_score']==min(output_from_opti['combined_score'])]\n",
    "    return bestValue\n",
    "\n",
    "# take list of coefficients - if more than zero \n",
    "# keep the name of the poly feature\n",
    "# keep the non-zero poly features values\n",
    "def get_lassoed_array(a_lasso_opti, column_names):\n",
    "    ## pull the list of coefficients -- some funnny stuff needs to be done because of array stuff, basically reset_index\n",
    "    coefficient_list = list(a_lasso_opti['coefficients'].reset_index()['coefficients'][0]) \n",
    "\n",
    "    ### creature a dictionary of features and coefficients and then turn into dataframe\n",
    "    feature_coefficient_dict = {'feature':[], 'coefficient':[]}\n",
    "    for (feaI, coeI) in zip(featurenames, coefficient_list): \n",
    "        if coeI != 0: \n",
    "            feature_coefficient_dict['feature'].append(feaI)\n",
    "            feature_coefficient_dict['coefficient'].append(coeI)\n",
    "            \n",
    "    ## coefficient ----- important for later to maybe pull out \n",
    "    coefficient_df = pd.DataFrame(feature_coefficient_dict).sort_values('coefficient', ascending=False)\n",
    "    feature_selection = feature_coefficient_dict['feature']\n",
    "    lassoed_dataframe = pd.DataFrame(X_train, columns= column_names)[feature_selection]\n",
    "    #### with the coefficients create a new np.array to \n",
    "    lassoed_x_train = np.array(pd.DataFrame(X_train, columns= column_names)[feature_selection])\n",
    "    ## output is the coefficient list for the lasso and the new lassoed_x_train\n",
    "    return [feature_coefficient_dict['feature'], lassoed_x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature engineering -- create function to pull out coefficients/features\n",
    "## add a gradient descent\n",
    "def lasso_optimization(x123_train, y123_train, alpha_list):\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[], 'alpha':[], 'coeff_used':[], 'coefficients':[], 'intercept':[]}\n",
    "\n",
    "    # initialize and train model with (default value) alpha = 0.5\n",
    "    # for i in [0.1, 1, 10,100,1000, 10000, 33000,66000, 100000, 330000,660000,1000000, 3300000,6600000,10000000,33000000,66000000,100000000]:\n",
    "    for i in alpha_list:\n",
    "\n",
    "        model = Lasso(alpha=i, max_iter=int(2000))\n",
    "        model.fit(x123_train, y123_train)\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        ## use cross validation for \n",
    "        score = cross_validate(model, x123_train, y123_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_root_mean_squared_error'), cv=3)      \n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_root_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_root_mean_squared_error'])\n",
    "        \n",
    "        coeff_used = np.sum(model.coef_!=0)\n",
    "        coefficients = model.coef_\n",
    "        intercept = model.intercept_\n",
    "        dicKeys = ['mean_MAPE','mean_r2','mean_MSE','std_MAPE','std_r2','std_MSE','score','alpha','coeff_used', 'coefficients', 'intercept'] \n",
    "        dicValues = [mean_MAPE, mean_r2, mean_MSE, std_MAPE, std_r2, std_MSE, len(score), i, coeff_used, coefficients, intercept]\n",
    "        for (keyi, vali) in zip(dicKeys, dicValues):\n",
    "            eval_metrics_dictionary[keyi].append(vali)\n",
    "        #eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        #eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        #eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        #eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        #eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        #eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        #eval_metrics_dictionary['score'].append(len(score))\n",
    "        #eval_metrics_dictionary['alpha'].append(i)\n",
    "        #eval_metrics_dictionary['coeff_used'].append(coeff_used)\n",
    "\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    #df_framework['combined_score'] = abs(abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']*df_framework['mean_MAPE'])\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    return df_framework\n",
    "#return df_framework.sort_values('combined_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_optimization(x123_train, y123_train, number_of_trials):\n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[],'alpha': [], \n",
    "                               'intercept' : [], 'tol': [], \n",
    "                               'solver':[], 'no_of_coefficients':[]}\n",
    "\n",
    "    def objective(trial):\n",
    "        ## HYPER PARAMETER TUNING\n",
    "        alpha = trial.suggest_float(\"alpha\", 0, 100)\n",
    "        intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "        tol = trial.suggest_float(\"tol\", 0.001, 1, log=True)\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"auto\", \"svd\",\"cholesky\", \"lsqr\", \"saga\", \"sag\"])\n",
    "\n",
    "        ## MODEL SELECTION\n",
    "        ## Create Model\n",
    "        model = Ridge(alpha=alpha,fit_intercept=intercept,tol=tol,solver=solver)\n",
    "        ## Fit Model\n",
    "        model.fit(x123_train, y123_train)\n",
    "        \n",
    "        # Function parameters for cross_val_score; \n",
    "        #   - estimator - The model object to use to fit the data\n",
    "        #   - X - The data to fit the model on\n",
    "        #   - y - The target of the model\n",
    "        #   - scoring - The error metric to use\n",
    "        #   - cv - The number of splits to use\n",
    "        \n",
    "        # error score to use; \n",
    "        #   - accuracy\n",
    "        #   - balanced_accuracy\n",
    "        #   - roc_auc\n",
    "        #   - f1\n",
    "        #   - neg_mean_absolute_error\n",
    "        #   - neg_root_mean_squared_error\n",
    "        #   - r2\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        score = cross_validate(model, x123_train, y123_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_root_mean_squared_error'), cv=3)      \n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_root_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_root_mean_squared_error'])\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_train= ridge_regressor.predict(X_test)\n",
    "            \n",
    "        Evaluation metrics;\n",
    "        - R squared\n",
    "        - MSE\n",
    "        - RMSE\n",
    "        - MAPE\n",
    "        \n",
    "        infun_MSE = mean_squared_error(y_test,y_pred_train)\n",
    "        infun_RMSE = math.sqrt(infun_MSE)\n",
    "        infun_r2 = r2_score(y_test, y_pred_train) \n",
    "        infun_MAPE = mean_absolute_percentage_error(y_test, y_pred_train) \n",
    "        max_coeff = ridge_regressor.coef_.round(3)\n",
    "        eval_metrics_dictionary['r2'].append(infun_r2)\n",
    "        eval_metrics_dictionary['MSE'].append(infun_MSE)\n",
    "        eval_metrics_dictionary['RMSE'].append(infun_RMSE)\n",
    "        eval_metrics_dictionary['MAPE'].append(infun_MAPE)\n",
    "        \"\"\"\n",
    "        coeff_used = np.sum(model.coef_!=0)\n",
    "\n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        eval_metrics_dictionary['alpha'].append(alpha)\n",
    "        eval_metrics_dictionary['intercept'].append(intercept)\n",
    "        eval_metrics_dictionary['tol'].append(tol)\n",
    "        eval_metrics_dictionary['solver'].append(solver)\n",
    "        eval_metrics_dictionary['no_of_coefficients'].append(coeff_used)\n",
    "\n",
    "        return mean_MSE*(1-mean_r2)\n",
    "        #return mean_MSE\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"RidgeRegression Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials)\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    return df_framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestRegressor_optimization(x123_train, y123_train,number_of_trials):\n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[],\n",
    "        'n_estimators': [], \n",
    "        'max_depth': [],\n",
    "        'min_samples_split': [],\n",
    "        'min_samples_leaf':[]\n",
    "        }\n",
    "    #randoforreg_dictionary = {'n_estimators': [], 'max_depth': [],'min_samples_split': [],'min_samples_leaf':[]}\n",
    "\n",
    "    def objective(trial):\n",
    "        ## HYPER PARAMETER TUNING\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 10, 200, log=True)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "        # Create and fit random forest model\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42,\n",
    "            )\n",
    "        model.fit(x123_train, y123_train)\n",
    "        \n",
    "        # Function parameters for cross_val_score; \n",
    "        #   - estimator - The model object to use to fit the data\n",
    "        #   - X - The data to fit the model on\n",
    "        #   - y - The target of the model\n",
    "        #   - scoring - The error metric to use\n",
    "        #   - cv - The number of splits to use\n",
    "        \n",
    "        # error score to use; \n",
    "        #   - accuracy\n",
    "        #   - balanced_accuracy\n",
    "        #   - roc_auc\n",
    "        #   - f1\n",
    "        #   - neg_mean_absolute_error\n",
    "        #   - neg_root_mean_squared_error\n",
    "        #   - r2\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        score = cross_validate(model, x123_train, y123_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_root_mean_squared_error'), cv=3)      \n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_root_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_root_mean_squared_error'])\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_train= ridge_regressor.predict(X_test)\n",
    "            \n",
    "        Evaluation metrics;\n",
    "        - R squared\n",
    "        - MSE\n",
    "        - RMSE\n",
    "        - MAPE\n",
    "        \n",
    "        infun_MSE = mean_squared_error(y_test,y_pred_train)\n",
    "        infun_RMSE = math.sqrt(infun_MSE)\n",
    "        infun_r2 = r2_score(y_test, y_pred_train) \n",
    "        infun_MAPE = mean_absolute_percentage_error(y_test, y_pred_train) \n",
    "        max_coeff = ridge_regressor.coef_.round(3)\n",
    "        eval_metrics_dictionary['r2'].append(infun_r2)\n",
    "        eval_metrics_dictionary['MSE'].append(infun_MSE)\n",
    "        eval_metrics_dictionary['RMSE'].append(infun_RMSE)\n",
    "        eval_metrics_dictionary['MAPE'].append(infun_MAPE)\n",
    "        \"\"\"\n",
    "        #coeff_used = np.sum(model.coef_!=0)\n",
    "\n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        eval_metrics_dictionary['n_estimators'].append(n_estimators)\n",
    "        eval_metrics_dictionary['max_depth'].append(max_depth)\n",
    "        eval_metrics_dictionary['min_samples_split'].append(min_samples_split)\n",
    "        eval_metrics_dictionary['min_samples_leaf'].append(min_samples_leaf)\n",
    "        #randoforreg_dictionary['no_of_coefficients'].append(coeff_used)\n",
    "\n",
    "        return mean_MSE*(1-mean_r2)\n",
    "        #return mean_MSE\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"RandomForestRegression Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials)\n",
    "    trial = study2.best_trial\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    return df_framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SVRegressor_optimization(x123_train, y123_train,number_of_trials):\n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[],'kernel' :[],'C' :[],'degree' :[],'coef0' :[],'gamma' :[]}\n",
    "    def objective(trial):\n",
    "        ## HYPER PARAMETER TUNING\n",
    "        degree = trial.suggest_int(\"degree\", 3, 8)\n",
    "        C = trial.suggest_int(\"C\", 1, 10)\n",
    "        coef0 = trial.suggest_float(\"coef0\", 0.01, 10)\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [\"auto\", 'scale'])\n",
    "        kernel = trial.suggest_categorical(\"kernel\", ['poly'])\n",
    "        # ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "\n",
    "\n",
    "        # Create and fit random forest model\n",
    "        model = SVR(\n",
    "            kernel=kernel,\n",
    "            C=C,\n",
    "            degree=degree,\n",
    "            coef0=coef0,\n",
    "            gamma=gamma,\n",
    "            )\n",
    "        model.fit(x123_train, y123_train)\n",
    "        \n",
    "        # Function parameters for cross_val_score; \n",
    "        #   - estimator - The model object to use to fit the data\n",
    "        #   - X - The data to fit the model on\n",
    "        #   - y - The target of the model\n",
    "        #   - scoring - The error metric to use\n",
    "        #   - cv - The number of splits to use\n",
    "        \n",
    "        # error score to use; \n",
    "        #   - accuracy\n",
    "        #   - balanced_accuracy\n",
    "        #   - roc_auc\n",
    "        #   - f1\n",
    "        #   - neg_mean_absolute_error\n",
    "        #   - neg_root_mean_squared_error\n",
    "        #   - r2\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        score = cross_validate(model, x123_train, y123_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_root_mean_squared_error'), cv=3)      \n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_root_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_root_mean_squared_error'])\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_train= ridge_regressor.predict(X_test)\n",
    "            \n",
    "        Evaluation metrics;\n",
    "        - R squared\n",
    "        - MSE\n",
    "        - RMSE\n",
    "        - MAPE\n",
    "        \n",
    "        infun_MSE = mean_squared_error(y_test,y_pred_train)\n",
    "        infun_RMSE = math.sqrt(infun_MSE)\n",
    "        infun_r2 = r2_score(y_test, y_pred_train) \n",
    "        infun_MAPE = mean_absolute_percentage_error(y_test, y_pred_train) \n",
    "        max_coeff = ridge_regressor.coef_.round(3)\n",
    "        eval_metrics_dictionary['r2'].append(infun_r2)\n",
    "        eval_metrics_dictionary['MSE'].append(infun_MSE)\n",
    "        eval_metrics_dictionary['RMSE'].append(infun_RMSE)\n",
    "        eval_metrics_dictionary['MAPE'].append(infun_MAPE)\n",
    "        \"\"\"\n",
    "        #coeff_used = np.sum(model.coef_!=0)\n",
    "\n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        eval_metrics_dictionary['degree'].append(degree)\n",
    "        eval_metrics_dictionary['C'].append(C)\n",
    "        eval_metrics_dictionary['coef0'].append(coef0)\n",
    "        eval_metrics_dictionary['gamma'].append(gamma)\n",
    "        eval_metrics_dictionary['kernel'].append(kernel)\n",
    "        #randoforreg_dictionary['no_of_coefficients'].append(coeff_used)\n",
    "        #return mean_MSE*mean_MAPE*(1-mean_r2)*(1-mean_r2)\n",
    "        return mean_MSE*(1-mean_r2)\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"SVRRegression Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials)\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    return df_framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreeRegressor_optimization(x123_train, y123_train,number_of_trials):\n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[], 'max_depth': [],'min_samples_split': [],'min_samples_leaf': [],'random_state':[]}\n",
    "\n",
    "    def objective(trial):\n",
    "        ## HYPER PARAMETER TUNING\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "        random_state = trial.suggest_int(\"random_state\", 0, 50)\n",
    "\n",
    "        # Create and fit random forest model\n",
    "        model = DecisionTreeRegressor(\n",
    "                max_depth=max_depth,\n",
    "                min_samples_leaf=min_samples_leaf, \n",
    "                min_samples_split=min_samples_split,\n",
    "                random_state=random_state\n",
    "                )\n",
    "        model.fit(x123_train, y123_train)\n",
    "        \n",
    "        # Function parameters for cross_val_score; \n",
    "        #   - estimator - The model object to use to fit the data\n",
    "        #   - X - The data to fit the model on\n",
    "        #   - y - The target of the model\n",
    "        #   - scoring - The error metric to use\n",
    "        #   - cv - The number of splits to use\n",
    "        \n",
    "        # error score to use; \n",
    "        #   - accuracy\n",
    "        #   - balanced_accuracy\n",
    "        #   - roc_auc\n",
    "        #   - f1\n",
    "        #   - neg_mean_absolute_error\n",
    "        #   - neg_root_mean_squared_error\n",
    "        #   - r2\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        score = cross_validate(model, x123_train, y123_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_root_mean_squared_error'), cv=3)      \n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_root_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_root_mean_squared_error'])\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_train= ridge_regressor.predict(X_test)\n",
    "            \n",
    "        Evaluation metrics;\n",
    "        - R squared\n",
    "        - MSE\n",
    "        - RMSE\n",
    "        - MAPE\n",
    "        \n",
    "        infun_MSE = mean_squared_error(y_test,y_pred_train)\n",
    "        infun_RMSE = math.sqrt(infun_MSE)\n",
    "        infun_r2 = r2_score(y_test, y_pred_train) \n",
    "        infun_MAPE = mean_absolute_percentage_error(y_test, y_pred_train) \n",
    "        max_coeff = ridge_regressor.coef_.round(3)\n",
    "        eval_metrics_dictionary['r2'].append(infun_r2)\n",
    "        eval_metrics_dictionary['MSE'].append(infun_MSE)\n",
    "        eval_metrics_dictionary['RMSE'].append(infun_RMSE)\n",
    "        eval_metrics_dictionary['MAPE'].append(infun_MAPE)\n",
    "        \"\"\"\n",
    "        #coeff_used = np.sum(model.coef_!=0)\n",
    "\n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        eval_metrics_dictionary['max_depth'].append(max_depth)\n",
    "        eval_metrics_dictionary['min_samples_split'].append(min_samples_split)\n",
    "        eval_metrics_dictionary['min_samples_leaf'].append(min_samples_leaf)\n",
    "        eval_metrics_dictionary['random_state'].append(random_state)\n",
    "        #randoforreg_dictionary['no_of_coefficients'].append(coeff_used)\n",
    "\n",
    "        return mean_MSE*(1-mean_r2)\n",
    "        #return mean_MSE\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"DecisionTreeRegressor Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials)\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    return df_framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboosterregressor_optimization(x123_train, y123_train, number_of_trials, iter_numnber):\n",
    "    \n",
    "    \n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[],\n",
    "                               'learning_rate' : [], \n",
    "                               'depth' : [], \n",
    "                               'l2_leaf_reg':[]\n",
    "                               #'l2_leaf_reg': [], \n",
    "                               #'min_child_samples':[]\n",
    "                               }\n",
    "\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.1),\n",
    "            'depth': trial.suggest_int(\"depth\", 2, 10),\n",
    "            'l2_leaf_reg': trial.suggest_float(\"l2_leaf_reg\", 0.2, 3)\n",
    "        }\n",
    "        model = CatBoostRegressor(**param, iterations=iter_numnber)        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set        \n",
    "        ## problem arises here because cross validate will fit the model each time.....\n",
    "        ## MUST BUILD YOUR OWN CROSS VALIDATE SCORE, SO THAT THE MODEL IS \n",
    "        score = eric_cross_validate(model, X_train_scaled, y_train, 5)\n",
    "        \n",
    "        mean_MSE = np.mean(score['RMSE'])\n",
    "        mean_r2 = np.mean(score['r2_score'])\n",
    "        mean_MAPE = np.mean(score['MAPE'])\n",
    "        std_MSE = np.std(score['RMSE'])\n",
    "        std_r2 = np.std(score['r2_score'])\n",
    "        std_MAPE = np.std(score['MAPE'])\n",
    "                \n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        eval_metrics_dictionary['learning_rate'].append(param['learning_rate'])\n",
    "        eval_metrics_dictionary['depth'].append(param['depth'])\n",
    "        eval_metrics_dictionary['l2_leaf_reg'].append(param['l2_leaf_reg'])\n",
    "\n",
    "        return mean_MSE*(1-mean_r2)\n",
    "        #return mean_MSE\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"catboosterregressor Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials,n_jobs=-1, timeout=24000)\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    return df_framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#catboosterregressor_optimization(X_train_scaled, y_train, number_of_trials=50, iter_numnber=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m cross_val_dict\n\u001b[0;32m     35\u001b[0m \u001b[39m## create a model...off the train data\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m## test it against the val data\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m eric_cross_validate(model, X_train_scaled, y_train, \u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "## Eric Cross Validate\n",
    "def eric_cross_validate(infunc_model, x345_train, y345_train, cv):\n",
    "## we want to partition the data....\n",
    "    x_split = np.array_split(x345_train, cv)\n",
    "    y_split = np.array_split(y345_train, cv)\n",
    "    \n",
    "    cross_val_dict = {'RMSE': [], 'r2_score': [], 'MAPE':[]}\n",
    "    for i in list(range(len(list(x_split)))): \n",
    "        \n",
    "        ## create our CV splits\n",
    "        infuncXList = list(x_split)\n",
    "        infuncYList = list(y_split)\n",
    "        temp_X_val_split = infuncXList.pop(i)\n",
    "        temp_X_train_split = np.concatenate(x_split)  \n",
    "        temp_Y_val_split = infuncYList.pop(i)\n",
    "        temp_Y_train_split = np.concatenate(y_split) \n",
    "        \n",
    "        ## fit the model \n",
    "        inmodel = infunc_model\n",
    "        inmodel.fit(temp_X_train_split, temp_Y_train_split, verbose = 0, early_stopping_rounds = 200) \n",
    "        in_ypred = inmodel.predict(temp_X_val_split)\n",
    "        \n",
    "        ## pull evaluation metrics \n",
    "        rmse = mean_squared_error(temp_Y_val_split, in_ypred,squared=False)\n",
    "        r2 = r2_score(temp_Y_val_split, in_ypred)\n",
    "        MAPE = mean_absolute_percentage_error(temp_Y_val_split, in_ypred)\n",
    "        \n",
    "        ## add values to the dictionary\n",
    "        cross_val_dict['RMSE'].append(rmse)\n",
    "        cross_val_dict['r2_score'].append(r2)\n",
    "        cross_val_dict['MAPE'].append(MAPE)\n",
    "\n",
    "    return cross_val_dict\n",
    "\n",
    "## create a model...off the train data\n",
    "## test it against the val data\n",
    "eric_cross_validate(model, X_train_scaled, y_train, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
