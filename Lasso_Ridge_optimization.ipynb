{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna \n",
    "\n",
    "# Feel free to add all the libraries you need\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/ordinalencoded.csv', index_col=0)\n",
    "\n",
    "#df1 = pd.read_csv('data/ordinalencoded_expandeddataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.read_csv('data/ordinalencoded_expandeddataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1[df1['international_flight']==0]\n",
    "#df = df.drop(['dep_lat','dep_lon','arr_lat','arr_lon', 'day', 'year', 'fltid','arr_elevation', 'dep_elevation', 'elevation_dif', 'average_flight_speed_km_h'], axis=1)\n",
    "#df['status']=df['status']*df['status']*df['status']\n",
    "#df['arr_iata']=df['arr_iata']*df['arr_iata']*df['arr_iata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features and target\n",
    "# if you want to create a polynomial you do so at the start and recreate the dataframe\n",
    "features = df.columns.tolist()\n",
    "features.remove('target')\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "featurenames = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaler is applied outside of function\n",
    "# standard scaler \n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pull the column names immediately they are useful\n",
    "### new definition\n",
    "def run_regression_models(x234_array, y234_array, features):\n",
    "## applies lasso optimization to the data set for feature selection\n",
    "## you have two trains --- lassoed_train & X_train \n",
    "## make sure you make a list of the feature in the lassoed train... \n",
    "\n",
    "## how many of each iteration do you want \n",
    "    ridge_iter = 30\n",
    "    ran_forest_iter = 10\n",
    "    SVR_iter = 1\n",
    "    dec_tree_iter = 30\n",
    "## apply the other optimizations to both the lassoed X-train adn the x-train\n",
    "    ## \n",
    "    lasso_opti = lasso_optimization(x234_array, y234_array, [0.01, 0.08,0.12, 0.15,1,10])\n",
    "    # output of lasso optimization is a dataframe of with all the attempts, pull out the best \n",
    "    ## min score function???\n",
    "    lasso_bestvalue = find_best_value(lasso_opti)\n",
    "    # get the lassoed array and the list of its features\n",
    "    ## lasso features for later\n",
    "    [lassoed_features, lassoed_X_Train] = get_lassoed_array(lasso_bestvalue, features)\n",
    "    \n",
    "    # Ridge Regression\n",
    "    ridge_all_features = ridge_optimization(x234_array, y234_array, ridge_iter)\n",
    "    ridge_lasso_features = ridge_optimization(lassoed_X_Train, y234_array, ridge_iter)\n",
    "    \n",
    "    ridge_all_features['opti_run']= 'ridge_all_features'\n",
    "    ridge_lasso_features['opti_run']= 'ridge_lasso_features'\n",
    "    \n",
    "    # RandomForestRegressor_optimization\n",
    "    ran_forest_all_features = RandomForestRegressor_optimization(x234_array, y234_array, ran_forest_iter)\n",
    "    ran_forest_lasso_features = RandomForestRegressor_optimization(lassoed_X_Train, y234_array, ran_forest_iter)\n",
    "    \n",
    "    ran_forest_all_features['opti_run']= 'ran_forest_all_features'\n",
    "    ran_forest_lasso_features['opti_run']= 'ran_forest_lasso_features'\n",
    "    \n",
    "    # SVRegressor_optimization\n",
    "    #SVR_all_features = SVRegressor_optimization(x234_array, y234_array, SVR_iter)\n",
    "    #SVR_lasso_features = SVRegressor_optimization(lassoed_X_Train, y234_array, SVR_iter)\n",
    "    #\n",
    "    #SVR_all_features['opti_run']= 'SVR_all_features'\n",
    "    #SVR_lasso_features['opti_run']= 'SVR_lasso_features'\n",
    "    \n",
    "    # DecisionTreeRegressor_optimization\n",
    "    Dec_Tree_all_features = DecisionTreeRegressor_optimization(x234_array, y234_array, dec_tree_iter)\n",
    "    Dec_Tree_lasso_features = DecisionTreeRegressor_optimization(lassoed_X_Train, y234_array, dec_tree_iter)\n",
    "    \n",
    "    Dec_Tree_all_features['opti_run']= 'Dec_Tree_all_features'\n",
    "    Dec_Tree_lasso_features['opti_run']= 'Dec_Tree_lasso_features'\n",
    "    \n",
    "    concat_all_data = pd.concat([\n",
    "        ridge_all_features, \n",
    "        ridge_lasso_features,\n",
    "        ran_forest_all_features,\n",
    "        ran_forest_lasso_features,\n",
    "        #SVR_all_features,\n",
    "        #SVR_lasso_features,\n",
    "        Dec_Tree_all_features,\n",
    "        Dec_Tree_lasso_features\n",
    "        ])\n",
    "    \n",
    "    concat_best_data = pd.concat([\n",
    "        find_best_value(ridge_all_features), \n",
    "        find_best_value(ridge_lasso_features),\n",
    "        find_best_value(ran_forest_all_features),\n",
    "        find_best_value(ran_forest_lasso_features),\n",
    "        #find_best_value(SVR_all_features),\n",
    "        #find_best_value(SVR_lasso_features),\n",
    "        find_best_value(Dec_Tree_all_features),\n",
    "        find_best_value(Dec_Tree_lasso_features)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "# Elastic net\n",
    "# XGBoost\n",
    "# CatBoost\n",
    "    return concat_best_data,lassoed_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.118e+04, tolerance: 4.157e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2023-09-04 16:39:24,716] A new study created in memory with name: RidgeRegression Optimization\n",
      "[I 2023-09-04 16:39:24,730] Trial 0 finished with value: 2.991484240257061e+20 and parameters: {'alpha': 72.41978679474211, 'fit_intercept': False, 'tol': 0.01597174712998319, 'solver': 'auto'}. Best is trial 0 with value: 2.991484240257061e+20.\n",
      "[I 2023-09-04 16:39:24,875] Trial 1 finished with value: 2.1464097700716672e+20 and parameters: {'alpha': 67.04627013955685, 'fit_intercept': True, 'tol': 0.004315093876456115, 'solver': 'sag'}. Best is trial 1 with value: 2.1464097700716672e+20.\n",
      "[I 2023-09-04 16:39:24,889] Trial 2 finished with value: 2.1597735816825124e+20 and parameters: {'alpha': 76.7788205619329, 'fit_intercept': True, 'tol': 0.018803516495887393, 'solver': 'lsqr'}. Best is trial 1 with value: 2.1464097700716672e+20.\n",
      "[I 2023-09-04 16:39:24,921] Trial 3 finished with value: 2.9907182642000206e+20 and parameters: {'alpha': 74.16194719297607, 'fit_intercept': False, 'tol': 0.04274348947440572, 'solver': 'svd'}. Best is trial 1 with value: 2.1464097700716672e+20.\n",
      "[I 2023-09-04 16:39:25,151] Trial 4 finished with value: 3.0039822651336576e+20 and parameters: {'alpha': 37.33908687398701, 'fit_intercept': False, 'tol': 0.004754241859694759, 'solver': 'saga'}. Best is trial 1 with value: 2.1464097700716672e+20.\n",
      "[I 2023-09-04 16:39:25,165] Trial 5 finished with value: 3.0035578657013183e+20 and parameters: {'alpha': 46.04715308683468, 'fit_intercept': False, 'tol': 0.13761988254279073, 'solver': 'auto'}. Best is trial 1 with value: 2.1464097700716672e+20.\n",
      "[I 2023-09-04 16:39:25,180] Trial 6 finished with value: 2.1411630111067338e+20 and parameters: {'alpha': 91.44463412606477, 'fit_intercept': True, 'tol': 0.08529014617453215, 'solver': 'auto'}. Best is trial 6 with value: 2.1411630111067338e+20.\n",
      "[I 2023-09-04 16:39:25,258] Trial 7 finished with value: 2.9807695648795125e+20 and parameters: {'alpha': 81.98285125652038, 'fit_intercept': False, 'tol': 0.020549646876146656, 'solver': 'sag'}. Best is trial 6 with value: 2.1411630111067338e+20.\n",
      "[I 2023-09-04 16:39:25,271] Trial 8 finished with value: 3.0077079519481646e+20 and parameters: {'alpha': 37.46222264633641, 'fit_intercept': False, 'tol': 0.5858416976893674, 'solver': 'auto'}. Best is trial 6 with value: 2.1411630111067338e+20.\n",
      "[I 2023-09-04 16:39:25,490] Trial 9 finished with value: 2.1608929825146978e+20 and parameters: {'alpha': 21.271660650437283, 'fit_intercept': True, 'tol': 0.0016002031835189966, 'solver': 'sag'}. Best is trial 6 with value: 2.1411630111067338e+20.\n",
      "[I 2023-09-04 16:39:25,509] Trial 10 finished with value: 2.140164952141575e+20 and parameters: {'alpha': 95.58641510054619, 'fit_intercept': True, 'tol': 0.8539791484419947, 'solver': 'cholesky'}. Best is trial 10 with value: 2.140164952141575e+20.\n",
      "[I 2023-09-04 16:39:25,530] Trial 11 finished with value: 2.1393077977045e+20 and parameters: {'alpha': 99.19387518508381, 'fit_intercept': True, 'tol': 0.7265204097918617, 'solver': 'cholesky'}. Best is trial 11 with value: 2.1393077977045e+20.\n",
      "[I 2023-09-04 16:39:25,552] Trial 12 finished with value: 2.140173518803245e+20 and parameters: {'alpha': 95.55065415500972, 'fit_intercept': True, 'tol': 0.9003081144312196, 'solver': 'cholesky'}. Best is trial 11 with value: 2.1393077977045e+20.\n",
      "[I 2023-09-04 16:39:25,579] Trial 13 finished with value: 2.1393228987633638e+20 and parameters: {'alpha': 99.12983012815326, 'fit_intercept': True, 'tol': 0.31435373981584086, 'solver': 'cholesky'}. Best is trial 11 with value: 2.1393077977045e+20.\n",
      "[I 2023-09-04 16:39:25,603] Trial 14 finished with value: 2.13962139677106e+20 and parameters: {'alpha': 97.86874505340225, 'fit_intercept': True, 'tol': 0.27789007409703453, 'solver': 'cholesky'}. Best is trial 11 with value: 2.1393077977045e+20.\n",
      "[I 2023-09-04 16:39:25,633] Trial 15 finished with value: 2.1484193589555957e+20 and parameters: {'alpha': 63.2888605375571, 'fit_intercept': True, 'tol': 0.3629570802898455, 'solver': 'cholesky'}. Best is trial 11 with value: 2.1393077977045e+20.\n",
      "[I 2023-09-04 16:39:25,657] Trial 16 finished with value: 2.1431184439077097e+20 and parameters: {'alpha': 83.51752392735148, 'fit_intercept': True, 'tol': 0.21896237178959935, 'solver': 'cholesky'}. Best is trial 11 with value: 2.1393077977045e+20.\n",
      "[I 2023-09-04 16:39:25,700] Trial 17 finished with value: 2.1502203677971556e+20 and parameters: {'alpha': 56.88596510384034, 'fit_intercept': True, 'tol': 0.45083371558376223, 'solver': 'svd'}. Best is trial 11 with value: 2.1393077977045e+20.\n",
      "[I 2023-09-04 16:39:25,723] Trial 18 finished with value: 2.0621889009373554e+20 and parameters: {'alpha': 88.31678113867925, 'fit_intercept': True, 'tol': 0.9999227087646411, 'solver': 'lsqr'}. Best is trial 18 with value: 2.0621889009373554e+20.\n",
      "[I 2023-09-04 16:39:25,749] Trial 19 finished with value: 2.0621889655811024e+20 and parameters: {'alpha': 86.85786799893056, 'fit_intercept': True, 'tol': 0.9424479231048035, 'solver': 'lsqr'}. Best is trial 18 with value: 2.0621889009373554e+20.\n",
      "[I 2023-09-04 16:39:25,780] Trial 20 finished with value: 2.0621890790500087e+20 and parameters: {'alpha': 84.5548943575619, 'fit_intercept': True, 'tol': 0.13584692778802426, 'solver': 'lsqr'}. Best is trial 18 with value: 2.0621889009373554e+20.\n",
      "[I 2023-09-04 16:39:25,808] Trial 21 finished with value: 2.0621890420702367e+20 and parameters: {'alpha': 85.27492135926612, 'fit_intercept': True, 'tol': 0.944377312316234, 'solver': 'lsqr'}. Best is trial 18 with value: 2.0621889009373554e+20.\n",
      "[I 2023-09-04 16:39:25,831] Trial 22 finished with value: 2.0621889643264547e+20 and parameters: {'alpha': 86.88502652865256, 'fit_intercept': True, 'tol': 0.9136994193927759, 'solver': 'lsqr'}. Best is trial 18 with value: 2.0621889009373554e+20.\n",
      "[I 2023-09-04 16:39:25,853] Trial 23 finished with value: 2.0621888623148633e+20 and parameters: {'alpha': 89.25537252117203, 'fit_intercept': True, 'tol': 0.47132991441084343, 'solver': 'lsqr'}. Best is trial 23 with value: 2.0621888623148633e+20.\n",
      "[I 2023-09-04 16:39:25,875] Trial 24 finished with value: 2.0621901380456477e+20 and parameters: {'alpha': 75.57723686407186, 'fit_intercept': True, 'tol': 0.4955634234741767, 'solver': 'lsqr'}. Best is trial 23 with value: 2.0621888623148633e+20.\n",
      "[I 2023-09-04 16:39:25,897] Trial 25 finished with value: 2.0621888336744402e+20 and parameters: {'alpha': 89.99080392964322, 'fit_intercept': True, 'tol': 0.4295982664163477, 'solver': 'lsqr'}. Best is trial 25 with value: 2.0621888336744402e+20.\n",
      "[I 2023-09-04 16:39:25,920] Trial 26 finished with value: 2.0621913436404354e+20 and parameters: {'alpha': 68.84052461285692, 'fit_intercept': True, 'tol': 0.2153449948472565, 'solver': 'lsqr'}. Best is trial 25 with value: 2.0621888336744402e+20.\n",
      "[I 2023-09-04 16:39:25,955] Trial 27 finished with value: 2.1730221638489974e+20 and parameters: {'alpha': 90.9899426770638, 'fit_intercept': True, 'tol': 0.379203690636142, 'solver': 'saga'}. Best is trial 25 with value: 2.0621888336744402e+20.\n",
      "[I 2023-09-04 16:39:25,978] Trial 28 finished with value: 2.0621930314289424e+20 and parameters: {'alpha': 61.1634343091353, 'fit_intercept': True, 'tol': 0.4821911112844804, 'solver': 'lsqr'}. Best is trial 25 with value: 2.0621888336744402e+20.\n",
      "[I 2023-09-04 16:39:25,999] Trial 29 finished with value: 2.9745949752096234e+20 and parameters: {'alpha': 77.21405892696103, 'fit_intercept': False, 'tol': 0.07134623084316634, 'solver': 'lsqr'}. Best is trial 25 with value: 2.0621888336744402e+20.\n",
      "[I 2023-09-04 16:39:26,001] A new study created in memory with name: RidgeRegression Optimization\n",
      "[I 2023-09-04 16:39:26,014] Trial 0 finished with value: 2.541224501042813e+20 and parameters: {'alpha': 80.89773019682156, 'fit_intercept': False, 'tol': 0.07404321592282213, 'solver': 'lsqr'}. Best is trial 0 with value: 2.541224501042813e+20.\n",
      "[I 2023-09-04 16:39:26,397] Trial 1 finished with value: 2.6364079072837914e+20 and parameters: {'alpha': 31.367353865307322, 'fit_intercept': False, 'tol': 0.011372378384840957, 'solver': 'saga'}. Best is trial 0 with value: 2.541224501042813e+20.\n",
      "[I 2023-09-04 16:39:26,516] Trial 2 finished with value: 2.606302766359123e+20 and parameters: {'alpha': 72.14855898946875, 'fit_intercept': False, 'tol': 0.038600380195621495, 'solver': 'sag'}. Best is trial 0 with value: 2.541224501042813e+20.\n",
      "[I 2023-09-04 16:39:28,602] Trial 3 finished with value: 2.7635936091974602e+20 and parameters: {'alpha': 58.56058907665257, 'fit_intercept': False, 'tol': 0.0011638533069069988, 'solver': 'saga'}. Best is trial 0 with value: 2.541224501042813e+20.\n",
      "[I 2023-09-04 16:39:30,022] Trial 4 finished with value: 2.7459184761967405e+20 and parameters: {'alpha': 61.35336581689314, 'fit_intercept': False, 'tol': 0.002019826186755268, 'solver': 'saga'}. Best is trial 0 with value: 2.541224501042813e+20.\n",
      "[I 2023-09-04 16:39:30,049] Trial 5 finished with value: 2.167224337317807e+20 and parameters: {'alpha': 28.884364947224274, 'fit_intercept': True, 'tol': 0.0013601626177332689, 'solver': 'svd'}. Best is trial 5 with value: 2.167224337317807e+20.\n",
      "[I 2023-09-04 16:39:30,070] Trial 6 finished with value: 3.105771514305586e+20 and parameters: {'alpha': 38.39698268326907, 'fit_intercept': True, 'tol': 0.6233065413153087, 'solver': 'sag'}. Best is trial 5 with value: 2.167224337317807e+20.\n",
      "[I 2023-09-04 16:39:30,083] Trial 7 finished with value: 2.1172764962138685e+20 and parameters: {'alpha': 1.5933983819718112, 'fit_intercept': False, 'tol': 0.03172670161100303, 'solver': 'cholesky'}. Best is trial 7 with value: 2.1172764962138685e+20.\n",
      "[I 2023-09-04 16:39:30,097] Trial 8 finished with value: 2.1637178597790165e+20 and parameters: {'alpha': 71.02646989268425, 'fit_intercept': True, 'tol': 0.007026784845996618, 'solver': 'auto'}. Best is trial 7 with value: 2.1172764962138685e+20.\n",
      "[I 2023-09-04 16:39:30,112] Trial 9 finished with value: 2.1688077658218208e+20 and parameters: {'alpha': 10.561230952690615, 'fit_intercept': True, 'tol': 0.05370015910162556, 'solver': 'auto'}. Best is trial 7 with value: 2.1172764962138685e+20.\n",
      "[I 2023-09-04 16:39:30,132] Trial 10 finished with value: 2.11706363996695e+20 and parameters: {'alpha': 5.12957563512154, 'fit_intercept': False, 'tol': 0.18132181939926317, 'solver': 'cholesky'}. Best is trial 10 with value: 2.11706363996695e+20.\n",
      "[I 2023-09-04 16:39:30,152] Trial 11 finished with value: 2.1173327971012762e+20 and parameters: {'alpha': 0.6616361134872548, 'fit_intercept': False, 'tol': 0.16967386227732714, 'solver': 'cholesky'}. Best is trial 10 with value: 2.11706363996695e+20.\n",
      "[I 2023-09-04 16:39:30,183] Trial 12 finished with value: 2.1164655800376453e+20 and parameters: {'alpha': 15.148000692154357, 'fit_intercept': False, 'tol': 0.21660583092584682, 'solver': 'cholesky'}. Best is trial 12 with value: 2.1164655800376453e+20.\n",
      "[I 2023-09-04 16:39:30,203] Trial 13 finished with value: 2.1162130947315427e+20 and parameters: {'alpha': 19.414753467763234, 'fit_intercept': False, 'tol': 0.300826629699503, 'solver': 'cholesky'}. Best is trial 13 with value: 2.1162130947315427e+20.\n",
      "[I 2023-09-04 16:39:30,227] Trial 14 finished with value: 2.1161961888130023e+20 and parameters: {'alpha': 19.701439436818067, 'fit_intercept': False, 'tol': 0.7071781131329428, 'solver': 'cholesky'}. Best is trial 14 with value: 2.1161961888130023e+20.\n",
      "[I 2023-09-04 16:39:30,247] Trial 15 finished with value: 2.116203542651543e+20 and parameters: {'alpha': 19.576723130254273, 'fit_intercept': False, 'tol': 0.9060198060074446, 'solver': 'cholesky'}. Best is trial 14 with value: 2.1161961888130023e+20.\n",
      "[I 2023-09-04 16:39:30,281] Trial 16 finished with value: 2.1149409708231292e+20 and parameters: {'alpha': 41.285265485248885, 'fit_intercept': False, 'tol': 0.5947677159478533, 'solver': 'svd'}. Best is trial 16 with value: 2.1149409708231292e+20.\n",
      "[I 2023-09-04 16:39:30,313] Trial 17 finished with value: 2.1147512802395642e+20 and parameters: {'alpha': 44.59994913548312, 'fit_intercept': False, 'tol': 0.9630427226207803, 'solver': 'svd'}. Best is trial 17 with value: 2.1147512802395642e+20.\n",
      "[I 2023-09-04 16:39:30,354] Trial 18 finished with value: 2.1659854259661367e+20 and parameters: {'alpha': 43.545702719512484, 'fit_intercept': True, 'tol': 0.9415946019255855, 'solver': 'svd'}. Best is trial 17 with value: 2.1147512802395642e+20.\n",
      "[I 2023-09-04 16:39:30,390] Trial 19 finished with value: 2.1143217838204148e+20 and parameters: {'alpha': 52.156351243578115, 'fit_intercept': False, 'tol': 0.3801979997104634, 'solver': 'svd'}. Best is trial 19 with value: 2.1143217838204148e+20.\n",
      "[I 2023-09-04 16:39:30,421] Trial 20 finished with value: 2.111860739162568e+20 and parameters: {'alpha': 96.80796015638762, 'fit_intercept': False, 'tol': 0.3626179288648827, 'solver': 'svd'}. Best is trial 20 with value: 2.111860739162568e+20.\n",
      "[I 2023-09-04 16:39:30,453] Trial 21 finished with value: 2.1123220273591422e+20 and parameters: {'alpha': 88.25332544041818, 'fit_intercept': False, 'tol': 0.35948991087112075, 'solver': 'svd'}. Best is trial 20 with value: 2.111860739162568e+20.\n",
      "[I 2023-09-04 16:39:30,486] Trial 22 finished with value: 2.1117035754553108e+20 and parameters: {'alpha': 99.74558854344389, 'fit_intercept': False, 'tol': 0.30422923886826275, 'solver': 'svd'}. Best is trial 22 with value: 2.1117035754553108e+20.\n",
      "[I 2023-09-04 16:39:30,516] Trial 23 finished with value: 2.1117204497322892e+20 and parameters: {'alpha': 99.42906955561568, 'fit_intercept': False, 'tol': 0.1071683405775098, 'solver': 'svd'}. Best is trial 22 with value: 2.1117035754553108e+20.\n",
      "[I 2023-09-04 16:39:30,549] Trial 24 finished with value: 2.111865429241297e+20 and parameters: {'alpha': 96.72058925276538, 'fit_intercept': False, 'tol': 0.1356821526341177, 'solver': 'svd'}. Best is trial 22 with value: 2.1117035754553108e+20.\n",
      "[I 2023-09-04 16:39:30,568] Trial 25 finished with value: 2.5412245010426656e+20 and parameters: {'alpha': 97.96619664374367, 'fit_intercept': False, 'tol': 0.10761631252895915, 'solver': 'lsqr'}. Best is trial 22 with value: 2.1117035754553108e+20.\n",
      "[I 2023-09-04 16:39:30,602] Trial 26 finished with value: 2.161422380775836e+20 and parameters: {'alpha': 99.76232769853102, 'fit_intercept': True, 'tol': 0.10073486641941895, 'solver': 'svd'}. Best is trial 22 with value: 2.1117035754553108e+20.\n",
      "[I 2023-09-04 16:39:30,634] Trial 27 finished with value: 2.1122211012616647e+20 and parameters: {'alpha': 90.11834650981426, 'fit_intercept': False, 'tol': 0.2620617127878568, 'solver': 'svd'}. Best is trial 22 with value: 2.1117035754553108e+20.\n",
      "[I 2023-09-04 16:39:30,668] Trial 28 finished with value: 2.1122569161153813e+20 and parameters: {'alpha': 89.45609458432716, 'fit_intercept': False, 'tol': 0.44884109019287377, 'solver': 'svd'}. Best is trial 22 with value: 2.1117035754553108e+20.\n",
      "[I 2023-09-04 16:39:30,686] Trial 29 finished with value: 2.5412245010428386e+20 and parameters: {'alpha': 77.90964342213553, 'fit_intercept': False, 'tol': 0.07844405769665494, 'solver': 'lsqr'}. Best is trial 22 with value: 2.1117035754553108e+20.\n",
      "[I 2023-09-04 16:39:30,688] A new study created in memory with name: RandomForestRegression Optimization\n",
      "[I 2023-09-04 16:39:34,583] Trial 0 finished with value: 1.1484030357442961e+20 and parameters: {'n_estimators': 86, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 7}. Best is trial 0 with value: 1.1484030357442961e+20.\n",
      "[I 2023-09-04 16:39:37,156] Trial 1 finished with value: 1.1397080130517536e+20 and parameters: {'n_estimators': 52, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 7}. Best is trial 1 with value: 1.1397080130517536e+20.\n",
      "[I 2023-09-04 16:39:38,465] Trial 2 finished with value: 1.1898489259422486e+20 and parameters: {'n_estimators': 24, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 1 with value: 1.1397080130517536e+20.\n",
      "[I 2023-09-04 16:39:42,397] Trial 3 finished with value: 1.1249433143787161e+20 and parameters: {'n_estimators': 82, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 8}. Best is trial 3 with value: 1.1249433143787161e+20.\n",
      "[I 2023-09-04 16:39:42,930] Trial 4 finished with value: 1.1856969310121794e+20 and parameters: {'n_estimators': 11, 'max_depth': 24, 'min_samples_split': 7, 'min_samples_leaf': 9}. Best is trial 3 with value: 1.1249433143787161e+20.\n",
      "[I 2023-09-04 16:39:44,249] Trial 5 finished with value: 1.6629781199716336e+20 and parameters: {'n_estimators': 20, 'max_depth': 22, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 3 with value: 1.1249433143787161e+20.\n",
      "[I 2023-09-04 16:39:45,733] Trial 6 finished with value: 1.5623465245865964e+20 and parameters: {'n_estimators': 23, 'max_depth': 18, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 3 with value: 1.1249433143787161e+20.\n",
      "[I 2023-09-04 16:39:46,092] Trial 7 finished with value: 1.6598705133602737e+20 and parameters: {'n_estimators': 22, 'max_depth': 2, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 3 with value: 1.1249433143787161e+20.\n",
      "[I 2023-09-04 16:39:50,412] Trial 8 finished with value: 1.5544984472934374e+20 and parameters: {'n_estimators': 142, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 1.1249433143787161e+20.\n",
      "[I 2023-09-04 16:39:52,815] Trial 9 finished with value: 1.7774326344228143e+20 and parameters: {'n_estimators': 125, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 3 with value: 1.1249433143787161e+20.\n",
      "[I 2023-09-04 16:39:52,817] A new study created in memory with name: RandomForestRegression Optimization\n",
      "[I 2023-09-04 16:39:55,053] Trial 0 finished with value: 1.1728108927205094e+20 and parameters: {'n_estimators': 76, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 0 with value: 1.1728108927205094e+20.\n",
      "[I 2023-09-04 16:39:55,494] Trial 1 finished with value: 2.322296103797975e+20 and parameters: {'n_estimators': 20, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 0 with value: 1.1728108927205094e+20.\n",
      "[I 2023-09-04 16:39:58,104] Trial 2 finished with value: 1.3801365076378034e+20 and parameters: {'n_estimators': 54, 'max_depth': 32, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 0 with value: 1.1728108927205094e+20.\n",
      "[I 2023-09-04 16:40:02,003] Trial 3 finished with value: 1.2739023429108788e+20 and parameters: {'n_estimators': 183, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 10}. Best is trial 0 with value: 1.1728108927205094e+20.\n",
      "[I 2023-09-04 16:40:06,829] Trial 4 finished with value: 1.1544278115376048e+20 and parameters: {'n_estimators': 138, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 10}. Best is trial 4 with value: 1.1544278115376048e+20.\n",
      "[I 2023-09-04 16:40:09,644] Trial 5 finished with value: 1.149213254557309e+20 and parameters: {'n_estimators': 75, 'max_depth': 23, 'min_samples_split': 9, 'min_samples_leaf': 9}. Best is trial 5 with value: 1.149213254557309e+20.\n",
      "[I 2023-09-04 16:40:15,541] Trial 6 finished with value: 1.1845522481442669e+20 and parameters: {'n_estimators': 133, 'max_depth': 21, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 5 with value: 1.149213254557309e+20.\n",
      "[I 2023-09-04 16:40:16,553] Trial 7 finished with value: 1.2235212048410404e+20 and parameters: {'n_estimators': 22, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 5 with value: 1.149213254557309e+20.\n",
      "[I 2023-09-04 16:40:18,364] Trial 8 finished with value: 1.3590831355379619e+20 and parameters: {'n_estimators': 38, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 5 with value: 1.149213254557309e+20.\n",
      "[I 2023-09-04 16:40:23,372] Trial 9 finished with value: 1.150032709810281e+20 and parameters: {'n_estimators': 139, 'max_depth': 23, 'min_samples_split': 8, 'min_samples_leaf': 10}. Best is trial 5 with value: 1.149213254557309e+20.\n",
      "[I 2023-09-04 16:40:23,374] A new study created in memory with name: DecisionTreeRegressor Optimization\n",
      "[I 2023-09-04 16:40:23,427] Trial 0 finished with value: 1.6380535402615996e+20 and parameters: {'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 7, 'random_state': 18}. Best is trial 0 with value: 1.6380535402615996e+20.\n",
      "[I 2023-09-04 16:40:23,514] Trial 1 finished with value: 1.8864294904472696e+20 and parameters: {'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 5, 'random_state': 28}. Best is trial 0 with value: 1.6380535402615996e+20.\n",
      "[I 2023-09-04 16:40:23,590] Trial 2 finished with value: 8.591726733948538e+20 and parameters: {'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 1, 'random_state': 21}. Best is trial 0 with value: 1.6380535402615996e+20.\n",
      "[I 2023-09-04 16:40:23,673] Trial 3 finished with value: 1.9436881285299438e+20 and parameters: {'max_depth': 28, 'min_samples_split': 4, 'min_samples_leaf': 5, 'random_state': 9}. Best is trial 0 with value: 1.6380535402615996e+20.\n",
      "[I 2023-09-04 16:40:23,722] Trial 4 finished with value: 3.2446482222347236e+20 and parameters: {'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 2, 'random_state': 9}. Best is trial 0 with value: 1.6380535402615996e+20.\n",
      "[I 2023-09-04 16:40:23,796] Trial 5 finished with value: 1.6314080528150374e+20 and parameters: {'max_depth': 24, 'min_samples_split': 10, 'min_samples_leaf': 10, 'random_state': 17}. Best is trial 5 with value: 1.6314080528150374e+20.\n",
      "[I 2023-09-04 16:40:23,869] Trial 6 finished with value: 1.9874755650983028e+20 and parameters: {'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 7, 'random_state': 7}. Best is trial 5 with value: 1.6314080528150374e+20.\n",
      "[I 2023-09-04 16:40:23,929] Trial 7 finished with value: 7.021567972008915e+20 and parameters: {'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 1, 'random_state': 15}. Best is trial 5 with value: 1.6314080528150374e+20.\n",
      "[I 2023-09-04 16:40:24,024] Trial 8 finished with value: 5.371768563039117e+20 and parameters: {'max_depth': 27, 'min_samples_split': 5, 'min_samples_leaf': 2, 'random_state': 50}. Best is trial 5 with value: 1.6314080528150374e+20.\n",
      "[I 2023-09-04 16:40:24,098] Trial 9 finished with value: 1.6294936377435642e+20 and parameters: {'max_depth': 31, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 13}. Best is trial 9 with value: 1.6294936377435642e+20.\n",
      "[I 2023-09-04 16:40:24,183] Trial 10 finished with value: 1.6309219579343005e+20 and parameters: {'max_depth': 32, 'min_samples_split': 6, 'min_samples_leaf': 10, 'random_state': 32}. Best is trial 9 with value: 1.6294936377435642e+20.\n",
      "[I 2023-09-04 16:40:24,268] Trial 11 finished with value: 1.631734583949947e+20 and parameters: {'max_depth': 32, 'min_samples_split': 6, 'min_samples_leaf': 10, 'random_state': 33}. Best is trial 9 with value: 1.6294936377435642e+20.\n",
      "[I 2023-09-04 16:40:24,357] Trial 12 finished with value: 1.7358099897573648e+20 and parameters: {'max_depth': 21, 'min_samples_split': 4, 'min_samples_leaf': 8, 'random_state': 38}. Best is trial 9 with value: 1.6294936377435642e+20.\n",
      "[I 2023-09-04 16:40:24,452] Trial 13 finished with value: 1.8762747605657266e+20 and parameters: {'max_depth': 32, 'min_samples_split': 7, 'min_samples_leaf': 9, 'random_state': 1}. Best is trial 9 with value: 1.6294936377435642e+20.\n",
      "[I 2023-09-04 16:40:24,544] Trial 14 finished with value: 1.7270152554593154e+20 and parameters: {'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 8, 'random_state': 41}. Best is trial 9 with value: 1.6294936377435642e+20.\n",
      "[I 2023-09-04 16:40:24,632] Trial 15 finished with value: 1.6288064386508738e+20 and parameters: {'max_depth': 28, 'min_samples_split': 6, 'min_samples_leaf': 10, 'random_state': 27}. Best is trial 15 with value: 1.6288064386508738e+20.\n",
      "[I 2023-09-04 16:40:24,730] Trial 16 finished with value: 2.344738200809118e+20 and parameters: {'max_depth': 27, 'min_samples_split': 4, 'min_samples_leaf': 6, 'random_state': 24}. Best is trial 15 with value: 1.6288064386508738e+20.\n",
      "[I 2023-09-04 16:40:24,819] Trial 17 finished with value: 1.8762747605657266e+20 and parameters: {'max_depth': 24, 'min_samples_split': 5, 'min_samples_leaf': 9, 'random_state': 1}. Best is trial 15 with value: 1.6288064386508738e+20.\n",
      "[I 2023-09-04 16:40:24,920] Trial 18 finished with value: 2.8426858463047444e+20 and parameters: {'max_depth': 29, 'min_samples_split': 7, 'min_samples_leaf': 4, 'random_state': 13}. Best is trial 15 with value: 1.6288064386508738e+20.\n",
      "[I 2023-09-04 16:40:25,011] Trial 19 finished with value: 1.890135976862534e+20 and parameters: {'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 9, 'random_state': 26}. Best is trial 15 with value: 1.6288064386508738e+20.\n",
      "[I 2023-09-04 16:40:25,101] Trial 20 finished with value: 1.7549892994332148e+20 and parameters: {'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 8, 'random_state': 47}. Best is trial 15 with value: 1.6288064386508738e+20.\n",
      "[I 2023-09-04 16:40:25,188] Trial 21 finished with value: 1.6309219579343005e+20 and parameters: {'max_depth': 32, 'min_samples_split': 6, 'min_samples_leaf': 10, 'random_state': 32}. Best is trial 15 with value: 1.6288064386508738e+20.\n",
      "[I 2023-09-04 16:40:25,276] Trial 22 finished with value: 1.631734583949947e+20 and parameters: {'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 10, 'random_state': 33}. Best is trial 15 with value: 1.6288064386508738e+20.\n",
      "[I 2023-09-04 16:40:25,318] Trial 23 finished with value: 1.8208966477877723e+20 and parameters: {'max_depth': 2, 'min_samples_split': 7, 'min_samples_leaf': 9, 'random_state': 39}. Best is trial 15 with value: 1.6288064386508738e+20.\n",
      "[I 2023-09-04 16:40:25,411] Trial 24 finished with value: 1.980116269650535e+20 and parameters: {'max_depth': 26, 'min_samples_split': 6, 'min_samples_leaf': 7, 'random_state': 29}. Best is trial 15 with value: 1.6288064386508738e+20.\n",
      "[I 2023-09-04 16:40:25,510] Trial 25 finished with value: 1.628417723625329e+20 and parameters: {'max_depth': 29, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 21}. Best is trial 25 with value: 1.628417723625329e+20.\n",
      "[I 2023-09-04 16:40:25,598] Trial 26 finished with value: 1.876895332002258e+20 and parameters: {'max_depth': 21, 'min_samples_split': 3, 'min_samples_leaf': 9, 'random_state': 22}. Best is trial 25 with value: 1.628417723625329e+20.\n",
      "[I 2023-09-04 16:40:25,688] Trial 27 finished with value: 1.743981454299196e+20 and parameters: {'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 8, 'random_state': 20}. Best is trial 25 with value: 1.628417723625329e+20.\n",
      "[I 2023-09-04 16:40:25,776] Trial 28 finished with value: 1.6322512827676934e+20 and parameters: {'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 10, 'random_state': 12}. Best is trial 25 with value: 1.628417723625329e+20.\n",
      "[I 2023-09-04 16:40:25,871] Trial 29 finished with value: 1.9703875423447453e+20 and parameters: {'max_depth': 29, 'min_samples_split': 3, 'min_samples_leaf': 7, 'random_state': 19}. Best is trial 25 with value: 1.628417723625329e+20.\n",
      "[I 2023-09-04 16:40:25,873] A new study created in memory with name: DecisionTreeRegressor Optimization\n",
      "[I 2023-09-04 16:40:25,931] Trial 0 finished with value: 1.938943124171435e+20 and parameters: {'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 8, 'random_state': 38}. Best is trial 0 with value: 1.938943124171435e+20.\n",
      "[I 2023-09-04 16:40:25,999] Trial 1 finished with value: 2.5428555681303082e+20 and parameters: {'max_depth': 25, 'min_samples_split': 10, 'min_samples_leaf': 6, 'random_state': 31}. Best is trial 0 with value: 1.938943124171435e+20.\n",
      "[I 2023-09-04 16:40:26,022] Trial 2 finished with value: 1.8988495040699653e+20 and parameters: {'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 5, 'random_state': 38}. Best is trial 2 with value: 1.8988495040699653e+20.\n",
      "[I 2023-09-04 16:40:26,086] Trial 3 finished with value: 2.1991493706224894e+20 and parameters: {'max_depth': 17, 'min_samples_split': 9, 'min_samples_leaf': 7, 'random_state': 50}. Best is trial 2 with value: 1.8988495040699653e+20.\n",
      "[I 2023-09-04 16:40:26,149] Trial 4 finished with value: 2.008660398910309e+20 and parameters: {'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 8, 'random_state': 1}. Best is trial 2 with value: 1.8988495040699653e+20.\n",
      "[I 2023-09-04 16:40:26,212] Trial 5 finished with value: 1.7860355082625004e+20 and parameters: {'max_depth': 22, 'min_samples_split': 4, 'min_samples_leaf': 10, 'random_state': 32}. Best is trial 5 with value: 1.7860355082625004e+20.\n",
      "[I 2023-09-04 16:40:26,279] Trial 6 finished with value: 1.8884032009795433e+20 and parameters: {'max_depth': 28, 'min_samples_split': 8, 'min_samples_leaf': 5, 'random_state': 11}. Best is trial 5 with value: 1.7860355082625004e+20.\n",
      "[I 2023-09-04 16:40:26,358] Trial 7 finished with value: 9.91164552668905e+20 and parameters: {'max_depth': 30, 'min_samples_split': 8, 'min_samples_leaf': 1, 'random_state': 41}. Best is trial 5 with value: 1.7860355082625004e+20.\n",
      "[I 2023-09-04 16:40:26,421] Trial 8 finished with value: 2.2053932296809762e+20 and parameters: {'max_depth': 31, 'min_samples_split': 3, 'min_samples_leaf': 7, 'random_state': 49}. Best is trial 5 with value: 1.7860355082625004e+20.\n",
      "[I 2023-09-04 16:40:26,485] Trial 9 finished with value: 2.5468470832798636e+20 and parameters: {'max_depth': 23, 'min_samples_split': 9, 'min_samples_leaf': 6, 'random_state': 44}. Best is trial 5 with value: 1.7860355082625004e+20.\n",
      "[I 2023-09-04 16:40:26,557] Trial 10 finished with value: 1.7796173985545214e+20 and parameters: {'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 20}. Best is trial 10 with value: 1.7796173985545214e+20.\n",
      "[I 2023-09-04 16:40:26,628] Trial 11 finished with value: 1.7815542983005182e+20 and parameters: {'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 21}. Best is trial 10 with value: 1.7796173985545214e+20.\n",
      "[I 2023-09-04 16:40:26,698] Trial 12 finished with value: 1.7841849399798697e+20 and parameters: {'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 18}. Best is trial 10 with value: 1.7796173985545214e+20.\n",
      "[I 2023-09-04 16:40:26,764] Trial 13 finished with value: 1.7237082155187795e+20 and parameters: {'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 10, 'random_state': 22}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:26,817] Trial 14 finished with value: 3.3505206977404e+20 and parameters: {'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 2, 'random_state': 13}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:26,886] Trial 15 finished with value: 2.1072513254088806e+20 and parameters: {'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 9, 'random_state': 28}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:26,934] Trial 16 finished with value: 2.071776908894864e+20 and parameters: {'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 3, 'random_state': 7}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,006] Trial 17 finished with value: 2.1814816692280518e+20 and parameters: {'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 9, 'random_state': 22}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,071] Trial 18 finished with value: 2.724560539029222e+20 and parameters: {'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 4, 'random_state': 14}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,145] Trial 19 finished with value: 2.186948834150695e+20 and parameters: {'max_depth': 19, 'min_samples_split': 7, 'min_samples_leaf': 9, 'random_state': 27}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,205] Trial 20 finished with value: 2.1509561896678654e+20 and parameters: {'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 7, 'random_state': 6}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,279] Trial 21 finished with value: 1.7815542983005182e+20 and parameters: {'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 10, 'random_state': 21}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,360] Trial 22 finished with value: 1.788733173599765e+20 and parameters: {'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 10, 'random_state': 18}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,439] Trial 23 finished with value: 2.001212283120803e+20 and parameters: {'max_depth': 18, 'min_samples_split': 5, 'min_samples_leaf': 8, 'random_state': 25}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,515] Trial 24 finished with value: 2.176319302554788e+20 and parameters: {'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 9, 'random_state': 18}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,587] Trial 25 finished with value: 1.7623930371475417e+20 and parameters: {'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 10, 'random_state': 23}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,659] Trial 26 finished with value: 1.965576318668583e+20 and parameters: {'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 8, 'random_state': 32}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,723] Trial 27 finished with value: 2.0693614263133576e+20 and parameters: {'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 9, 'random_state': 23}. Best is trial 13 with value: 1.7237082155187795e+20.\n",
      "[I 2023-09-04 16:40:27,769] Trial 28 finished with value: 1.5045583299619655e+20 and parameters: {'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 10, 'random_state': 15}. Best is trial 28 with value: 1.5045583299619655e+20.\n",
      "[I 2023-09-04 16:40:27,811] Trial 29 finished with value: 1.8231807378982388e+20 and parameters: {'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 8, 'random_state': 10}. Best is trial 28 with value: 1.5045583299619655e+20.\n"
     ]
    }
   ],
   "source": [
    "hi = run_regression_models(X_train_scaled, y_train, featurenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = hi[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_runs = hi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_r2</th>\n",
       "      <th>mean_MAPE</th>\n",
       "      <th>mean_MSE</th>\n",
       "      <th>std_r2</th>\n",
       "      <th>std_MAPE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>score</th>\n",
       "      <th>alpha</th>\n",
       "      <th>intercept</th>\n",
       "      <th>tol</th>\n",
       "      <th>solver</th>\n",
       "      <th>no_of_coefficients</th>\n",
       "      <th>combined_score</th>\n",
       "      <th>opti_run</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>random_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.078274</td>\n",
       "      <td>4.941870e+16</td>\n",
       "      <td>5095.399902</td>\n",
       "      <td>0.019678</td>\n",
       "      <td>6.313382e+14</td>\n",
       "      <td>1959.650251</td>\n",
       "      <td>5</td>\n",
       "      <td>99.193875</td>\n",
       "      <td>True</td>\n",
       "      <td>0.726520</td>\n",
       "      <td>cholesky</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4696.562412</td>\n",
       "      <td>ridge_all_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.078210</td>\n",
       "      <td>4.991315e+16</td>\n",
       "      <td>5096.367459</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>7.071884e+14</td>\n",
       "      <td>1959.807871</td>\n",
       "      <td>5</td>\n",
       "      <td>99.762328</td>\n",
       "      <td>True</td>\n",
       "      <td>0.100735</td>\n",
       "      <td>svd</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4697.780423</td>\n",
       "      <td>ridge_lasso_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.104519</td>\n",
       "      <td>2.856980e+16</td>\n",
       "      <td>4910.334907</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>2.659007e+15</td>\n",
       "      <td>1806.034871</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4397.109372</td>\n",
       "      <td>ran_forest_all_features</td>\n",
       "      <td>82.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.101448</td>\n",
       "      <td>2.893838e+16</td>\n",
       "      <td>4922.085131</td>\n",
       "      <td>0.014197</td>\n",
       "      <td>3.024324e+15</td>\n",
       "      <td>1802.012787</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4422.751352</td>\n",
       "      <td>ran_forest_lasso_features</td>\n",
       "      <td>139.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.037637</td>\n",
       "      <td>3.752781e+16</td>\n",
       "      <td>5239.070289</td>\n",
       "      <td>0.022424</td>\n",
       "      <td>2.924311e+15</td>\n",
       "      <td>1817.159568</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5041.887760</td>\n",
       "      <td>Dec_Tree_all_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.052449</td>\n",
       "      <td>3.238770e+16</td>\n",
       "      <td>5173.963371</td>\n",
       "      <td>0.014550</td>\n",
       "      <td>2.616372e+15</td>\n",
       "      <td>1827.452738</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4902.596351</td>\n",
       "      <td>Dec_Tree_lasso_features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_r2     mean_MAPE     mean_MSE    std_r2      std_MAPE      std_MSE   \n",
       "11  0.078274  4.941870e+16  5095.399902  0.019678  6.313382e+14  1959.650251  \\\n",
       "26  0.078210  4.991315e+16  5096.367459  0.020734  7.071884e+14  1959.807871   \n",
       "3   0.104519  2.856980e+16  4910.334907  0.009670  2.659007e+15  1806.034871   \n",
       "9   0.101448  2.893838e+16  4922.085131  0.014197  3.024324e+15  1802.012787   \n",
       "23  0.037637  3.752781e+16  5239.070289  0.022424  2.924311e+15  1817.159568   \n",
       "28  0.052449  3.238770e+16  5173.963371  0.014550  2.616372e+15  1827.452738   \n",
       "\n",
       "    score      alpha intercept       tol    solver  no_of_coefficients   \n",
       "11      5  99.193875      True  0.726520  cholesky                27.0  \\\n",
       "26      5  99.762328      True  0.100735       svd                23.0   \n",
       "3       5        NaN       NaN       NaN       NaN                 NaN   \n",
       "9       5        NaN       NaN       NaN       NaN                 NaN   \n",
       "23      5        NaN       NaN       NaN       NaN                 NaN   \n",
       "28      5        NaN       NaN       NaN       NaN                 NaN   \n",
       "\n",
       "    combined_score                   opti_run  n_estimators  max_depth   \n",
       "11     4696.562412         ridge_all_features           NaN        NaN  \\\n",
       "26     4697.780423       ridge_lasso_features           NaN        NaN   \n",
       "3      4397.109372    ran_forest_all_features          82.0       19.0   \n",
       "9      4422.751352  ran_forest_lasso_features         139.0       23.0   \n",
       "23     5041.887760      Dec_Tree_all_features           NaN        2.0   \n",
       "28     4902.596351    Dec_Tree_lasso_features           NaN        4.0   \n",
       "\n",
       "    min_samples_split  min_samples_leaf  random_state  \n",
       "11                NaN               NaN           NaN  \n",
       "26                NaN               NaN           NaN  \n",
       "3                 3.0               8.0           NaN  \n",
       "9                 8.0              10.0           NaN  \n",
       "23                7.0               9.0          39.0  \n",
       "28                7.0              10.0          15.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USEFUL FUNCTIONS\n",
    "def find_best_value(output_from_opti):\n",
    "    bestValue = output_from_opti[output_from_opti['combined_score']==min(output_from_opti['combined_score'])]\n",
    "    return bestValue\n",
    "\n",
    "# take list of coefficients - if more than zero \n",
    "# keep the name of the poly feature\n",
    "# keep the non-zero poly features values\n",
    "def get_lassoed_array(a_lasso_opti, column_names):\n",
    "    ## pull the list of coefficients -- some funnny stuff needs to be done because of array stuff, basically reset_index\n",
    "    coefficient_list = list(a_lasso_opti['coefficients'].reset_index()['coefficients'][0]) \n",
    "\n",
    "    ### creature a dictionary of features and coefficients and then turn into dataframe\n",
    "    feature_coefficient_dict = {'feature':[], 'coefficient':[]}\n",
    "    for (feaI, coeI) in zip(featurenames, coefficient_list): \n",
    "        if coeI != 0: \n",
    "            feature_coefficient_dict['feature'].append(feaI)\n",
    "            feature_coefficient_dict['coefficient'].append(coeI)\n",
    "            \n",
    "    ## coefficient ----- important for later to maybe pull out \n",
    "    coefficient_df = pd.DataFrame(feature_coefficient_dict).sort_values('coefficient', ascending=False)\n",
    "    feature_selection = feature_coefficient_dict['feature']\n",
    "    lassoed_dataframe = pd.DataFrame(X_train, columns= column_names)[feature_selection]\n",
    "    #### with the coefficients create a new np.array to \n",
    "    lassoed_x_train = np.array(pd.DataFrame(X_train, columns= column_names)[feature_selection])\n",
    "    ## output is the coefficient list for the lasso and the new lassoed_x_train\n",
    "    return [feature_coefficient_dict['feature'], lassoed_x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature engineering -- create function to pull out coefficients/features\n",
    "## add a gradient descent\n",
    "def lasso_optimization(x123_train, y123_train, alpha_list):\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[], 'alpha':[], 'coeff_used':[], 'coefficients':[], 'intercept':[]}\n",
    "\n",
    "    # initialize and train model with (default value) alpha = 0.5\n",
    "    # for i in [0.1, 1, 10,100,1000, 10000, 33000,66000, 100000, 330000,660000,1000000, 3300000,6600000,10000000,33000000,66000000,100000000]:\n",
    "    for i in alpha_list:\n",
    "\n",
    "        model = Lasso(alpha=i, max_iter=int(2000))\n",
    "        model.fit(x123_train, y123_train)\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        ## use cross validation for \n",
    "        score = cross_validate(model, x123_train, y123_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_mean_squared_error'), cv=3)\n",
    "        \n",
    "        ## pull out the scores\n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_mean_squared_error'])\n",
    "        coeff_used = np.sum(model.coef_!=0)\n",
    "        coefficients = model.coef_\n",
    "        intercept = model.intercept_\n",
    "        dicKeys = ['mean_MAPE','mean_r2','mean_MSE','std_MAPE','std_r2','std_MSE','score','alpha','coeff_used', 'coefficients', 'intercept'] \n",
    "        dicValues = [mean_MAPE, mean_r2, mean_MSE, std_MAPE, std_r2, std_MSE, len(score), i, coeff_used, coefficients, intercept]\n",
    "        for (keyi, vali) in zip(dicKeys, dicValues):\n",
    "            eval_metrics_dictionary[keyi].append(vali)\n",
    "        #eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        #eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        #eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        #eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        #eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        #eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        #eval_metrics_dictionary['score'].append(len(score))\n",
    "        #eval_metrics_dictionary['alpha'].append(i)\n",
    "        #eval_metrics_dictionary['coeff_used'].append(coeff_used)\n",
    "\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    #df_framework['combined_score'] = abs(abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']*df_framework['mean_MAPE'])\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    return df_framework\n",
    "#return df_framework.sort_values('combined_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_optimization(x123_train, y123_train, number_of_trials):\n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[],'alpha': [], \n",
    "                               'intercept' : [], 'tol': [], \n",
    "                               'solver':[], 'no_of_coefficients':[]}\n",
    "\n",
    "    def objective(trial):\n",
    "        ## HYPER PARAMETER TUNING\n",
    "        alpha = trial.suggest_float(\"alpha\", 0, 100)\n",
    "        intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "        tol = trial.suggest_float(\"tol\", 0.001, 1, log=True)\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"auto\", \"svd\",\"cholesky\", \"lsqr\", \"saga\", \"sag\"])\n",
    "\n",
    "        ## MODEL SELECTION\n",
    "        ## Create Model\n",
    "        model = Ridge(alpha=alpha,fit_intercept=intercept,tol=tol,solver=solver)\n",
    "        ## Fit Model\n",
    "        model.fit(x123_train, y123_train)\n",
    "        \n",
    "        # Function parameters for cross_val_score; \n",
    "        #   - estimator - The model object to use to fit the data\n",
    "        #   - X - The data to fit the model on\n",
    "        #   - y - The target of the model\n",
    "        #   - scoring - The error metric to use\n",
    "        #   - cv - The number of splits to use\n",
    "        \n",
    "        # error score to use; \n",
    "        #   - accuracy\n",
    "        #   - balanced_accuracy\n",
    "        #   - roc_auc\n",
    "        #   - f1\n",
    "        #   - neg_mean_absolute_error\n",
    "        #   - neg_root_mean_squared_error\n",
    "        #   - r2\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        score = cross_validate(model, x123_train, y123_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_mean_squared_error'), cv=3)\n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_mean_squared_error'])\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_train= ridge_regressor.predict(X_test)\n",
    "            \n",
    "        Evaluation metrics;\n",
    "        - R squared\n",
    "        - MSE\n",
    "        - RMSE\n",
    "        - MAPE\n",
    "        \n",
    "        infun_MSE = mean_squared_error(y_test,y_pred_train)\n",
    "        infun_RMSE = math.sqrt(infun_MSE)\n",
    "        infun_r2 = r2_score(y_test, y_pred_train) \n",
    "        infun_MAPE = mean_absolute_percentage_error(y_test, y_pred_train) \n",
    "        max_coeff = ridge_regressor.coef_.round(3)\n",
    "        eval_metrics_dictionary['r2'].append(infun_r2)\n",
    "        eval_metrics_dictionary['MSE'].append(infun_MSE)\n",
    "        eval_metrics_dictionary['RMSE'].append(infun_RMSE)\n",
    "        eval_metrics_dictionary['MAPE'].append(infun_MAPE)\n",
    "        \"\"\"\n",
    "        coeff_used = np.sum(model.coef_!=0)\n",
    "\n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        eval_metrics_dictionary['alpha'].append(alpha)\n",
    "        eval_metrics_dictionary['intercept'].append(intercept)\n",
    "        eval_metrics_dictionary['tol'].append(tol)\n",
    "        eval_metrics_dictionary['solver'].append(solver)\n",
    "        eval_metrics_dictionary['no_of_coefficients'].append(coeff_used)\n",
    "\n",
    "        return mean_MSE*mean_MAPE*(1-mean_r2)*(1-mean_r2)\n",
    "        #return mean_MSE\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"RidgeRegression Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials)\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    return df_framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestRegressor_optimization(x123_train, y123_train,number_of_trials):\n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[],\n",
    "        'n_estimators': [], \n",
    "        'max_depth': [],\n",
    "        'min_samples_split': [],\n",
    "        'min_samples_leaf':[]\n",
    "        }\n",
    "    #randoforreg_dictionary = {'n_estimators': [], 'max_depth': [],'min_samples_split': [],'min_samples_leaf':[]}\n",
    "\n",
    "    def objective(trial):\n",
    "        ## HYPER PARAMETER TUNING\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 10, 200, log=True)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "        # Create and fit random forest model\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42,\n",
    "            )\n",
    "        model.fit(x123_train, y123_train)\n",
    "        \n",
    "        # Function parameters for cross_val_score; \n",
    "        #   - estimator - The model object to use to fit the data\n",
    "        #   - X - The data to fit the model on\n",
    "        #   - y - The target of the model\n",
    "        #   - scoring - The error metric to use\n",
    "        #   - cv - The number of splits to use\n",
    "        \n",
    "        # error score to use; \n",
    "        #   - accuracy\n",
    "        #   - balanced_accuracy\n",
    "        #   - roc_auc\n",
    "        #   - f1\n",
    "        #   - neg_mean_absolute_error\n",
    "        #   - neg_root_mean_squared_error\n",
    "        #   - r2\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        score = cross_validate(model, x123_train, y123_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_mean_squared_error'), cv=3)\n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_mean_squared_error'])\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_train= ridge_regressor.predict(X_test)\n",
    "            \n",
    "        Evaluation metrics;\n",
    "        - R squared\n",
    "        - MSE\n",
    "        - RMSE\n",
    "        - MAPE\n",
    "        \n",
    "        infun_MSE = mean_squared_error(y_test,y_pred_train)\n",
    "        infun_RMSE = math.sqrt(infun_MSE)\n",
    "        infun_r2 = r2_score(y_test, y_pred_train) \n",
    "        infun_MAPE = mean_absolute_percentage_error(y_test, y_pred_train) \n",
    "        max_coeff = ridge_regressor.coef_.round(3)\n",
    "        eval_metrics_dictionary['r2'].append(infun_r2)\n",
    "        eval_metrics_dictionary['MSE'].append(infun_MSE)\n",
    "        eval_metrics_dictionary['RMSE'].append(infun_RMSE)\n",
    "        eval_metrics_dictionary['MAPE'].append(infun_MAPE)\n",
    "        \"\"\"\n",
    "        #coeff_used = np.sum(model.coef_!=0)\n",
    "\n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        eval_metrics_dictionary['n_estimators'].append(n_estimators)\n",
    "        eval_metrics_dictionary['max_depth'].append(max_depth)\n",
    "        eval_metrics_dictionary['min_samples_split'].append(min_samples_split)\n",
    "        eval_metrics_dictionary['min_samples_leaf'].append(min_samples_leaf)\n",
    "        #randoforreg_dictionary['no_of_coefficients'].append(coeff_used)\n",
    "\n",
    "        return mean_MSE*mean_MAPE*(1-mean_r2)*(1-mean_r2)\n",
    "        #return mean_MSE\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"RandomForestRegression Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials)\n",
    "    trial = study2.best_trial\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    return df_framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SVRegressor_optimization(x123_train, y123_train,number_of_trials):\n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[],'kernel' :[],'C' :[],'degree' :[],'coef0' :[],'gamma' :[]}\n",
    "    def objective(trial):\n",
    "        ## HYPER PARAMETER TUNING\n",
    "        degree = trial.suggest_int(\"degree\", 3, 8)\n",
    "        C = trial.suggest_int(\"C\", 1, 10)\n",
    "        coef0 = trial.suggest_float(\"coef0\", 0.01, 10)\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [\"auto\", 'scale'])\n",
    "        kernel = trial.suggest_categorical(\"kernel\", ['poly'])\n",
    "        # ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "\n",
    "\n",
    "        # Create and fit random forest model\n",
    "        model = SVR(\n",
    "            kernel=kernel,\n",
    "            C=C,\n",
    "            degree=degree,\n",
    "            coef0=coef0,\n",
    "            gamma=gamma,\n",
    "            )\n",
    "        model.fit(x123_train, y123_train)\n",
    "        \n",
    "        # Function parameters for cross_val_score; \n",
    "        #   - estimator - The model object to use to fit the data\n",
    "        #   - X - The data to fit the model on\n",
    "        #   - y - The target of the model\n",
    "        #   - scoring - The error metric to use\n",
    "        #   - cv - The number of splits to use\n",
    "        \n",
    "        # error score to use; \n",
    "        #   - accuracy\n",
    "        #   - balanced_accuracy\n",
    "        #   - roc_auc\n",
    "        #   - f1\n",
    "        #   - neg_mean_absolute_error\n",
    "        #   - neg_root_mean_squared_error\n",
    "        #   - r2\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        score = cross_validate(model, x123_train, y123_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_mean_squared_error'), cv=3)\n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_mean_squared_error'])\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_train= ridge_regressor.predict(X_test)\n",
    "            \n",
    "        Evaluation metrics;\n",
    "        - R squared\n",
    "        - MSE\n",
    "        - RMSE\n",
    "        - MAPE\n",
    "        \n",
    "        infun_MSE = mean_squared_error(y_test,y_pred_train)\n",
    "        infun_RMSE = math.sqrt(infun_MSE)\n",
    "        infun_r2 = r2_score(y_test, y_pred_train) \n",
    "        infun_MAPE = mean_absolute_percentage_error(y_test, y_pred_train) \n",
    "        max_coeff = ridge_regressor.coef_.round(3)\n",
    "        eval_metrics_dictionary['r2'].append(infun_r2)\n",
    "        eval_metrics_dictionary['MSE'].append(infun_MSE)\n",
    "        eval_metrics_dictionary['RMSE'].append(infun_RMSE)\n",
    "        eval_metrics_dictionary['MAPE'].append(infun_MAPE)\n",
    "        \"\"\"\n",
    "        #coeff_used = np.sum(model.coef_!=0)\n",
    "\n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        eval_metrics_dictionary['degree'].append(degree)\n",
    "        eval_metrics_dictionary['C'].append(C)\n",
    "        eval_metrics_dictionary['coef0'].append(coef0)\n",
    "        eval_metrics_dictionary['gamma'].append(gamma)\n",
    "        eval_metrics_dictionary['kernel'].append(kernel)\n",
    "        #randoforreg_dictionary['no_of_coefficients'].append(coeff_used)\n",
    "        #return mean_MSE*mean_MAPE*(1-mean_r2)*(1-mean_r2)\n",
    "        return mean_MSE\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"SVRRegression Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials)\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    return df_framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreeRegressor_optimization(x123_train, y123_train,number_of_trials):\n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[], 'max_depth': [],'min_samples_split': [],'min_samples_leaf': [],'random_state':[]}\n",
    "\n",
    "    def objective(trial):\n",
    "        ## HYPER PARAMETER TUNING\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "        random_state = trial.suggest_int(\"random_state\", 0, 50)\n",
    "\n",
    "        # Create and fit random forest model\n",
    "        model = DecisionTreeRegressor(\n",
    "                max_depth=max_depth,\n",
    "                min_samples_leaf=min_samples_leaf, \n",
    "                min_samples_split=min_samples_split,\n",
    "                random_state=random_state\n",
    "                )\n",
    "        model.fit(x123_train, y123_train)\n",
    "        \n",
    "        # Function parameters for cross_val_score; \n",
    "        #   - estimator - The model object to use to fit the data\n",
    "        #   - X - The data to fit the model on\n",
    "        #   - y - The target of the model\n",
    "        #   - scoring - The error metric to use\n",
    "        #   - cv - The number of splits to use\n",
    "        \n",
    "        # error score to use; \n",
    "        #   - accuracy\n",
    "        #   - balanced_accuracy\n",
    "        #   - roc_auc\n",
    "        #   - f1\n",
    "        #   - neg_mean_absolute_error\n",
    "        #   - neg_root_mean_squared_error\n",
    "        #   - r2\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        score = cross_validate(model, x123_train, y123_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_mean_squared_error'), cv=3)\n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_mean_squared_error'])\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_train= ridge_regressor.predict(X_test)\n",
    "            \n",
    "        Evaluation metrics;\n",
    "        - R squared\n",
    "        - MSE\n",
    "        - RMSE\n",
    "        - MAPE\n",
    "        \n",
    "        infun_MSE = mean_squared_error(y_test,y_pred_train)\n",
    "        infun_RMSE = math.sqrt(infun_MSE)\n",
    "        infun_r2 = r2_score(y_test, y_pred_train) \n",
    "        infun_MAPE = mean_absolute_percentage_error(y_test, y_pred_train) \n",
    "        max_coeff = ridge_regressor.coef_.round(3)\n",
    "        eval_metrics_dictionary['r2'].append(infun_r2)\n",
    "        eval_metrics_dictionary['MSE'].append(infun_MSE)\n",
    "        eval_metrics_dictionary['RMSE'].append(infun_RMSE)\n",
    "        eval_metrics_dictionary['MAPE'].append(infun_MAPE)\n",
    "        \"\"\"\n",
    "        #coeff_used = np.sum(model.coef_!=0)\n",
    "\n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        eval_metrics_dictionary['max_depth'].append(max_depth)\n",
    "        eval_metrics_dictionary['min_samples_split'].append(min_samples_split)\n",
    "        eval_metrics_dictionary['min_samples_leaf'].append(min_samples_leaf)\n",
    "        eval_metrics_dictionary['random_state'].append(random_state)\n",
    "        #randoforreg_dictionary['no_of_coefficients'].append(coeff_used)\n",
    "\n",
    "        return mean_MSE*mean_MAPE*(1-mean_r2)*(1-mean_r2)\n",
    "        #return mean_MSE\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"DecisionTreeRegressor Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials)\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    df_framework['combined_score'] = abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']\n",
    "    return df_framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DecisionTreeRegressor_optimization(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(ridge_X_test_scaled)\n\u001b[0;32m      2\u001b[0m y_train_pred\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(ridge_Xtrain)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred= model.predict(ridge_X_test_scaled)\n",
    "y_train_pred= model.predict(ridge_Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE:  6975.29374069285\n",
      "train MSE:  5492.89472864518\n"
     ]
    }
   ],
   "source": [
    "print('test MSE: ', mean_squared_error(y_test, y_pred))\n",
    "print('train MSE: ', mean_squared_error(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add XGBoost and CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lasso featured vs non-lassoed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finally test against"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
