{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature engineering -- create function to pull out coefficients/features\n",
    "## add a gradient descent\n",
    "def lasso_optimization():\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[], 'alpha':[], 'coeff_used':[], 'coefficients':[], 'intercept':[]}\n",
    "\n",
    "    # initialize and train model with (default value) alpha = 0.5\n",
    "    for i in [0.1, 1, 10,100,1000, 10000, 33000,66000, 100000, 330000,660000,1000000, 3300000,6600000,10000000,33000000,66000000,100000000]:\n",
    "\n",
    "        model = Lasso(alpha=i, max_iter=int(2000))\n",
    "        model.fit(X_train,y_train)\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        ## use cross validation for \n",
    "        score = cross_validate(model, X_train, y_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_mean_squared_error'), cv=3)\n",
    "        \n",
    "        ## pull out the scores\n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_mean_squared_error'])\n",
    "        coeff_used = np.sum(model.coef_!=0)\n",
    "        coefficients = model.coef_\n",
    "        intercept = model.intercept_\n",
    "\n",
    "        dicKeys = ['mean_MAPE','mean_r2','mean_MSE','std_MAPE','std_r2','std_MSE','score','alpha','coeff_used', 'coefficients', 'intercept'] \n",
    "        dicValues = [mean_MAPE, mean_r2, mean_MSE, std_MAPE, std_r2, std_MSE, len(score), i, coeff_used, coefficients, intercept]\n",
    "        for (keyi, vali) in zip(dicKeys, dicValues):\n",
    "            eval_metrics_dictionary[keyi].append(vali)\n",
    "\n",
    "        #eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        #eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        #eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        #eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        #eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        #eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        #eval_metrics_dictionary['score'].append(len(score))\n",
    "        #eval_metrics_dictionary['alpha'].append(i)\n",
    "        #eval_metrics_dictionary['coeff_used'].append(coeff_used)\n",
    "\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    df_framework['combined_score'] = abs(abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']*df_framework['mean_MAPE'])\n",
    "    df_bestvalue = df_framework[df_framework['combined_score']==min(df_framework['combined_score'])]\n",
    "    return df_bestvalue\n",
    "#return df_framework.sort_values('combined_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = lasso_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take list of coefficients - if more than zero \n",
    "# keep the name of the poly feature\n",
    "# keep the non-zero poly features values\n",
    "featurenames = list(p.get_feature_names_out(X.columns))\n",
    "coefficient_list = list(hi['coefficients']) \n",
    "\n",
    "### creature a dictionary of features and coefficients and then turn into dataframe\n",
    "feature_coefficient_dict = {'feature':[], 'coefficient':[]}\n",
    "for (feaI, coeI) in zip(featurenames,coefficient_list): \n",
    "    if coeI != 0: \n",
    "        feature_coefficient_dict['feature'].append(feaI)\n",
    "        feature_coefficient_dict['coefficient'].append(coeI)\n",
    "        \n",
    "pd.DataFrame(feature_coefficient_dict)\n",
    "feature_selection = feature_coefficient_dict['feature']\n",
    "#### with the coefficients create a new np.array to \n",
    "ridge_Xtrain = np.array(pd.DataFrame(X_train, columns= p.get_feature_names_out(X.columns))[feature_selection])\n",
    "\n",
    "np.array(pd.DataFrame(X_train, columns= p.get_feature_names_out(X.columns))[feature_selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_optimization(number_of_trials):\n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[]}\n",
    "    ridge_opt_dictionary = { 'alpha': [], 'intercept' : [], 'tol': [], 'solver':[], 'no_of_coefficients':[]}\n",
    "\n",
    "    def objective(trial):\n",
    "        ## HYPER PARAMETER TUNING\n",
    "        alpha = trial.suggest_float(\"alpha\", 0, 1000)\n",
    "        intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "        tol = trial.suggest_float(\"tol\", 0.001, 10, log=True)\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"auto\", \"svd\",\"cholesky\", \"lsqr\", \"saga\", \"sag\"])\n",
    "\n",
    "        ## MODEL SELECTION\n",
    "        ## Create Model\n",
    "        model = Ridge(alpha=alpha,fit_intercept=intercept,tol=tol,solver=solver)\n",
    "        ## Fit Model\n",
    "        model.fit(ridge_Xtrain, y_train)\n",
    "        \n",
    "        # Function parameters for cross_val_score; \n",
    "        #   - estimator - The model object to use to fit the data\n",
    "        #   - X - The data to fit the model on\n",
    "        #   - y - The target of the model\n",
    "        #   - scoring - The error metric to use\n",
    "        #   - cv - The number of splits to use\n",
    "        \n",
    "        # error score to use; \n",
    "        #   - accuracy\n",
    "        #   - balanced_accuracy\n",
    "        #   - roc_auc\n",
    "        #   - f1\n",
    "        #   - neg_mean_absolute_error\n",
    "        #   - neg_root_mean_squared_error\n",
    "        #   - r2\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        score = cross_validate(model, X_train, y_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_mean_squared_error'), cv=3)\n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_mean_squared_error'])\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_train= ridge_regressor.predict(X_test)\n",
    "            \n",
    "        Evaluation metrics;\n",
    "        - R squared\n",
    "        - MSE\n",
    "        - RMSE\n",
    "        - MAPE\n",
    "        \n",
    "        infun_MSE = mean_squared_error(y_test,y_pred_train)\n",
    "        infun_RMSE = math.sqrt(infun_MSE)\n",
    "        infun_r2 = r2_score(y_test, y_pred_train) \n",
    "        infun_MAPE = mean_absolute_percentage_error(y_test, y_pred_train) \n",
    "        max_coeff = ridge_regressor.coef_.round(3)\n",
    "        eval_metrics_dictionary['r2'].append(infun_r2)\n",
    "        eval_metrics_dictionary['MSE'].append(infun_MSE)\n",
    "        eval_metrics_dictionary['RMSE'].append(infun_RMSE)\n",
    "        eval_metrics_dictionary['MAPE'].append(infun_MAPE)\n",
    "        \"\"\"\n",
    "        coeff_used = np.sum(model.coef_!=0)\n",
    "\n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        ridge_opt_dictionary['alpha'].append(alpha)\n",
    "        ridge_opt_dictionary['intercept'].append(intercept)\n",
    "        ridge_opt_dictionary['tol'].append(tol)\n",
    "        ridge_opt_dictionary['solver'].append(solver)\n",
    "        ridge_opt_dictionary['no_of_coefficients'].append(coeff_used)\n",
    "\n",
    "        return mean_MAPE\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"RidgeRegression Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials)\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    df_framework['combined_score'] = abs(abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']*df_framework['mean_MAPE'])\n",
    "    df_bestvalue = df_framework[df_framework['combined_score']==min(df_framework['combined_score'])]\n",
    "    return df_bestvalue"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
