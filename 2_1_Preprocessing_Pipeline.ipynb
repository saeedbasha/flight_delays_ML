{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a preprocessing pipeline\n",
    "\n",
    "# Data loading\n",
    "df_flightdata = pd.read_csv('data/Train.csv')\n",
    "df_airportdata = pd.read_csv('data/airportdata.csv', index_col=0)\n",
    "\n",
    "# Defining categorical data and features to drop. Here we use label encoding. Hot encoding is not used.\n",
    "categorical_columns = ['depstn', 'arrstn', 'status', 'arr_country', 'dep_country', 'season', 'airline_code', 'international_flight','ac','dep_iata','arr_iata','fltid']\n",
    "columns_to_drop = ['id', 'std', 'sta', 'fltid', 'arr_iata', 'dep_iata', 'ac']\n",
    "\n",
    "class ColumnNameFixer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X.columns = X.columns.str.replace(' ', '_').str.lower().str.replace('-', '_')\n",
    "        return X\n",
    "\n",
    "class CalculateDistance(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        def calculate_distance(row):\n",
    "            dep_coords = (row['dep_lat'], row['dep_lon'])\n",
    "            arr_coords = (row['arr_lat'], row['arr_lon'])\n",
    "            distance = geodesic(dep_coords, arr_coords).kilometers\n",
    "            return int(round(distance, 0))\n",
    "\n",
    "        X['flight_distance_in_km'] = X.apply(calculate_distance, axis=1)\n",
    "        return X\n",
    "\n",
    "class CustomFeaturesAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['sta'] = pd.to_datetime(X['sta'], format='%Y-%m-%d %H.%M.%S')\n",
    "        X['std'] = pd.to_datetime(X['std'], format='%Y-%m-%d %H:%M:%S')\n",
    "        X['datop'] = pd.to_datetime(X['datop'], format='%Y-%m-%d')\n",
    "        X['std_time'] = X['std'].dt.time\n",
    "        X['sta_time'] = X['sta'].dt.time\n",
    "        X['std_time'] = X['std_time'].astype(str).str.replace(':', '').astype(int)\n",
    "        X['sta_time'] = X['sta_time'].astype(str).str.replace(':', '').astype(int)\n",
    "        \n",
    "        X['elevation_dif'] = (X['arr_elevation'] - X['dep_elevation'])\n",
    "        X['flight_time_in_min'] = (X['sta'] - X['std']).dt.total_seconds() / 60\n",
    "        X['average_flight_speed_km_h'] = (X['flight_distance_in_km'] * 60 / X['flight_time_in_min']).round().astype(int)\n",
    "        X['international_flight'] = np.where(X['arr_country'] != X['dep_country'], 1, 0)\n",
    "        X['airline_code'] = X['fltid'].str[:2]\n",
    "        # Extract year, month, and day components\n",
    "        X['year'] = X['datop'].dt.year\n",
    "        X['month'] = X['datop'].dt.month\n",
    "        X['day'] = X['datop'].dt.day\n",
    "        X['datop'] = X['datop'].astype(str).str.replace('-', '').astype(int)\n",
    "        \n",
    "        # Create the seasons column\n",
    "        X.loc[(X['month'] < 3) | (X['month'] == 12), 'season'] = 'winter'\n",
    "        X.loc[(X['month'] >= 3) & (X['month'] < 6), 'season'] = 'spring' \n",
    "        X.loc[(X['month'] >= 6) & (X['month'] < 9), 'season'] = 'summer' \n",
    "        X.loc[(X['month'] >= 9) & (X['month'] < 12), 'season'] = 'autumn'\n",
    "        \n",
    "        return X\n",
    "\n",
    "class LabelEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.label_encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            label_encoder = LabelEncoder()\n",
    "            label_encoder.fit(X[col])\n",
    "            self.label_encoders[col] = label_encoder\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_encoded = X.copy()\n",
    "        for col, label_encoder in self.label_encoders.items():\n",
    "            X_encoded[col] = label_encoder.transform(X_encoded[col])\n",
    "        return X_encoded\n",
    "\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_drop):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X.drop(self.columns_to_drop, axis=1, inplace=True)\n",
    "        return X\n",
    "\n",
    "# Define the preprocessing steps in the pipeline\n",
    "preprocessing_steps = [\n",
    "    ('column_name_fixer', ColumnNameFixer()),\n",
    "    ('calculate_distance', CalculateDistance()),\n",
    "    ('custom_features_adder', CustomFeaturesAdder()),\n",
    "    ('label_encoding', LabelEncoderTransformer(categorical_columns)),\n",
    "    ('drop_columns', DropColumns(columns_to_drop))\n",
    "\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=preprocessing_steps)\n",
    "\n",
    "# Join the DataFrames first and then apply the preprocessing pipeline\n",
    "merged_data = df_flightdata.join(df_airportdata[['iata', 'country', 'elevation', 'lat', 'lon']].add_prefix('dep_'), how='left', on='DEPSTN') \\\n",
    "    .join(df_airportdata[['iata', 'country', 'elevation', 'lat', 'lon']].add_prefix('arr_'), how='left', on='ARRSTN')\n",
    "\n",
    "df_processed = preprocessing_pipeline.fit_transform(merged_data)\n",
    "\n",
    "# Now df_processed contains the preprocessed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preprocessing_pipeline.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the preprocessing pipeline to a file\n",
    "joblib.dump(preprocessing_pipeline, 'data/preprocessing_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table Representation of Combined DataFrame:\n",
      "+---+------------------+---------------------+-----------------+-----------+-----------+---------------------------+---------------------------+---------+-----------------+---------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+-----------------------+-----------------+-----------------+-----------------+--------------------+---------------------------+----------------------+--------------+-------------+-------+---------+------------+\n",
      "|   |   id (dropped)   |        datop        | fltid (dropped) |  depstn   |  arrstn   |       std (dropped)       |       sta (dropped)       | status  |  ac (dropped)   |    target     | dep_iata (dropped) | dep_country | dep_elevation |            dep_lat            |            dep_lon            | arr_iata (dropped) | arr_country | arr_elevation |            arr_lat            |            arr_lon            | flight_distance_in_km |    std_time     |    sta_time     |  elevation_dif  | flight_time_in_min | average_flight_speed_km_h | international_flight | airline_code |    year     | month |   day   |   season   |\n",
      "+---+------------------+---------------------+-----------------+-----------+-----------+---------------------------+---------------------------+---------+-----------------+---------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+-----------------------+-----------------+-----------------+-----------------+--------------------+---------------------------+----------------------+--------------+-------------+-------+---------+------------+\n",
      "| 0 | nan | train_id_0 | 20160103 | 20160103 |  nan | TU 0712  | 31 | CMN  | 119 | TUN | nan | 2016-01-03 10:30:00 | nan | 2016-01-03 12:55:00 | 0 | ATA | nan | TU 32AIMN | 260.0 | 260.0 |     nan | CMN      |   31 | MA   | 656.0 | 656.0 | 33.3675003052 | 33.3675003052 | -7.5899701118 | -7.5899701118 |     nan | TUN      |   48 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |      1667 | 1667      | 103000 | 103000 | 125500 | 125500 | -634.0 | -634.0 |   145.0 | 145.0    |         690 | 690         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 |  3 | 3  | 3 | winter |\n",
      "| 1 | nan | train_id_1 | 20160113 | 20160113 |  nan | TU 0757  | 86 | MXP  | 119 | TUN | nan | 2016-01-13 15:05:00 | nan | 2016-01-13 16:55:00 | 0 | ATA | nan | TU 31BIMO |  20.0 | 20.0  |     nan | MXP      |   26 | IT   | 768.0 | 768.0 | 45.6305999756 | 45.6305999756 |  8.7281103134 | 8.7281103134  |     nan | TUN      |   48 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |       983 | 983       | 150500 | 150500 | 165500 | 165500 | -746.0 | -746.0 |   110.0 | 110.0    |         536 | 536         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 13 | 13 | 3 | winter |\n",
      "| 2 | nan | train_id_2 | 20160116 | 20160116 |  nan | TU 0214  | 123 | TUN | 58 | IST  | nan | 2016-01-16 04:10:00 | nan | 2016-01-16 06:45:00 | 0 | ATA | nan | TU 32AIMN |   0.0 | 0.0   |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | IST      |   49 | TR   | 325.0 | 325.0 |     41.275333 | 41.275333     |        28.752 | 28.752        |      1673 | 1673      |  41000 | 41000  |  64500 | 64500  |  303.0 | 303.0  |   155.0 | 155.0    |         648 | 648         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 16 | 16 | 3 | winter |\n",
      "| 3 | nan | train_id_3 | 20160117 | 20160117 |  nan | TU 0480  | 37 | DJE  | 91 | NTE  | nan | 2016-01-17 14:10:00 | nan | 2016-01-17 17:00:00 | 0 | ATA | nan | TU 736IOK |   0.0 | 0.0   |     nan | DJE      |   51 | TN   |  19.0 | 19.0  |        33.875 | 33.875        | 10.7755002975 | 10.7755002975 |     nan | NTE      |   17 | FR   |  90.0 | 90.0  | 47.1531982422 | 47.1531982422 |  -1.610730052 | -1.610730052  |      1805 | 1805      | 141000 | 141000 | 170000 | 170000 |   71.0 | 71.0   |   170.0 | 170.0    |         637 | 637         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 17 | 17 | 3 | winter |\n",
      "| 4 | nan | train_id_4 | 20160117 | 20160117 |  nan | TU 0338  | 123 | TUN |  4 | ALG  | nan | 2016-01-17 14:30:00 | nan | 2016-01-17 15:50:00 | 0 | ATA | nan | TU 320IMU |  22.0 | 22.0  |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | ALG      |   14 | DZ   |  82.0 | 82.0  | 36.6910018921 | 36.6910018921 |  3.2154099941 | 3.2154099941  |       626 | 626       | 143000 | 143000 | 155000 | 155000 |   60.0 | 60.0   |    80.0 | 80.0     |         470 | 470         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 17 | 17 | 3 | winter |\n",
      "| 5 | nan | train_id_5 | 20160117 | 20160117 |  nan | TU 0283  | 117 | TLS | 119 | TUN | nan | 2016-01-17 16:20:00 | nan | 2016-01-17 18:15:00 | 0 | ATA | nan | TU 736IOP |  53.0 | 53.0  |     nan | TLS      |   18 | FR   | 499.0 | 499.0 | 43.6291007996 | 43.6291007996 |  1.3638199568 | 1.3638199568  |     nan | TUN      |   48 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |      1064 | 1064      | 162000 | 162000 | 181500 | 181500 | -477.0 | -477.0 |   115.0 | 115.0    |         555 | 555         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 17 | 17 | 3 | winter |\n",
      "| 6 | nan | train_id_6 | 20160118 | 20160118 |  nan | TU 0514  | 123 | TUN | 10 | BCN  | nan | 2016-01-18 07:15:00 | nan | 2016-01-18 09:00:00 | 0 | ATA | nan | TU 32AIMH |  10.0 | 10.0  |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | BCN      |   16 | ES   |  12.0 | 12.0  | 41.2971000671 | 41.2971000671 |  2.0784599781 | 2.0784599781  |       860 | 860       |  71500 | 71500  |  90000 | 90000  |  -10.0 | -10.0  |   105.0 | 105.0    |         491 | 491         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 18 | 18 | 3 | winter |\n",
      "| 7 | nan | train_id_7 | 20160118 | 20160118 |  nan | TU 0716  | 123 | TUN | 94 | ORY  | nan | 2016-01-18 07:35:00 | nan | 2016-01-18 09:55:00 | 0 | ATA | nan | TU 32AIMI |  15.0 | 15.0  |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | ORY      |   17 | FR   | 291.0 | 291.0 | 48.7252998352 | 48.7252998352 |  2.3594400883 | 2.3594400883  |      1466 | 1466      |  73500 | 73500  |  95500 | 95500  |  269.0 | 269.0  |   140.0 | 140.0    |         628 | 628         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 18 | 18 | 3 | winter |\n",
      "| 8 | nan | train_id_8 | 20160118 | 20160118 |  nan | TU 0752  | 123 | TUN | 46 | FCO  | nan | 2016-01-18 07:40:00 | nan | 2016-01-18 09:00:00 | 0 | ATA | nan | TU 32AIMC |  16.0 | 16.0  |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | FCO      |   24 | IT   |  15.0 | 15.0  | 41.8045005798 | 41.8045005798 | 12.2508001328 | 12.2508001328 |       577 | 577       |  74000 | 74000  |  90000 | 90000  |   -7.0 | -7.0   |    80.0 | 80.0     |         433 | 433         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 18 | 18 | 3 | winter |\n",
      "| 9 | nan | train_id_9 | 20160118 | 20160118 |  nan | TU 0996  | 123 | TUN | 87 | NCE  | nan | 2016-01-18 07:45:00 | nan | 2016-01-18 09:15:00 | 0 | ATA | nan | TU 31AIMK |  21.0 | 21.0  |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | NCE      |   17 | FR   |  12.0 | 12.0  | 43.6584014893 | 43.6584014893 |  7.2158699036 | 7.2158699036  |       798 | 798       |  74500 | 74500  |  91500 | 91500  |  -10.0 | -10.0  |    90.0 | 90.0     |         532 | 532         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 18 | 18 | 3 | winter |\n",
      "+---+------------------+---------------------+-----------------+-----------+-----------+---------------------------+---------------------------+---------+-----------------+---------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+-----------------------+-----------------+-----------------+-----------------+--------------------+---------------------------+----------------------+--------------+-------------+-------+---------+------------+\n"
     ]
    }
   ],
   "source": [
    "# tabulate library to compare the df's\n",
    "\n",
    "# Extract the headers of merged_data\n",
    "headers_merged = merged_data.columns.tolist()\n",
    "\n",
    "# Initialize an empty list to store rows\n",
    "headers_list = []\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the first ten rows in both DataFrames and add them to the table_data\n",
    "for index, header_merged in enumerate(headers_merged):\n",
    "    if header_merged in df_processed.columns:\n",
    "        # Headers match, compare data\n",
    "        header_processed = header_merged\n",
    "        row_processed = df_processed.head(10).iloc[:, df_processed.columns.get_loc(header_merged)]\n",
    "        row_merged = merged_data.head(10).iloc[:, index]\n",
    "    else:\n",
    "        # Headers don't match, create an empty column\n",
    "        header_processed = f'{header_merged} (dropped)'\n",
    "        row_processed = pd.Series([np.nan] * 10)\n",
    "        row_merged = merged_data.head(10).iloc[:, index]\n",
    "\n",
    "    # Combine values from both DataFrames in a single cell\n",
    "    combined_row = []\n",
    "    for val_processed, val_merged in zip(row_processed, row_merged):\n",
    "        combined_row.append(f'{val_processed} | {val_merged}')\n",
    "    headers_list.append(header_processed)\n",
    "    combined_df[header_processed] = combined_row\n",
    "\n",
    "# Create the table\n",
    "# Convert the DataFrame to a table and print it\n",
    "combined_table = tabulate(combined_df, headers='keys', tablefmt='pretty')\n",
    "print(\"\\nTable Representation of Combined DataFrame:\")\n",
    "print(combined_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
