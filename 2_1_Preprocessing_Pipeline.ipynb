{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import joblib\n",
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>107823</th>\n",
       "      <th>107824</th>\n",
       "      <th>107825</th>\n",
       "      <th>107826</th>\n",
       "      <th>107827</th>\n",
       "      <th>107828</th>\n",
       "      <th>107829</th>\n",
       "      <th>107830</th>\n",
       "      <th>107831</th>\n",
       "      <th>107832</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>train_id_0</td>\n",
       "      <td>train_id_1</td>\n",
       "      <td>train_id_2</td>\n",
       "      <td>train_id_3</td>\n",
       "      <td>train_id_4</td>\n",
       "      <td>train_id_5</td>\n",
       "      <td>train_id_6</td>\n",
       "      <td>train_id_7</td>\n",
       "      <td>train_id_8</td>\n",
       "      <td>train_id_9</td>\n",
       "      <td>...</td>\n",
       "      <td>train_id_107823</td>\n",
       "      <td>train_id_107824</td>\n",
       "      <td>train_id_107825</td>\n",
       "      <td>train_id_107826</td>\n",
       "      <td>train_id_107827</td>\n",
       "      <td>train_id_107828</td>\n",
       "      <td>train_id_107829</td>\n",
       "      <td>train_id_107830</td>\n",
       "      <td>train_id_107831</td>\n",
       "      <td>train_id_107832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATOP</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-04-12</td>\n",
       "      <td>2018-02-09</td>\n",
       "      <td>2018-08-08</td>\n",
       "      <td>2018-10-15</td>\n",
       "      <td>2018-12-19</td>\n",
       "      <td>2018-07-05</td>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>2018-11-07</td>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>2018-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLTID</th>\n",
       "      <td>TU 0712</td>\n",
       "      <td>TU 0757</td>\n",
       "      <td>TU 0214</td>\n",
       "      <td>TU 0480</td>\n",
       "      <td>TU 0338</td>\n",
       "      <td>TU 0283</td>\n",
       "      <td>TU 0514</td>\n",
       "      <td>TU 0716</td>\n",
       "      <td>TU 0752</td>\n",
       "      <td>TU 0996</td>\n",
       "      <td>...</td>\n",
       "      <td>TU 9001</td>\n",
       "      <td>UG 1730</td>\n",
       "      <td>WKL 0000</td>\n",
       "      <td>UG 0011</td>\n",
       "      <td>SGT 0000</td>\n",
       "      <td>WKL 0000</td>\n",
       "      <td>UG 0003</td>\n",
       "      <td>SGT 0000</td>\n",
       "      <td>UG 0010</td>\n",
       "      <td>UG 0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPSTN</th>\n",
       "      <td>CMN</td>\n",
       "      <td>MXP</td>\n",
       "      <td>TUN</td>\n",
       "      <td>DJE</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TLS</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>...</td>\n",
       "      <td>MIR</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>DJE</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>DJE</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARRSTN</th>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>IST</td>\n",
       "      <td>NTE</td>\n",
       "      <td>ALG</td>\n",
       "      <td>TUN</td>\n",
       "      <td>BCN</td>\n",
       "      <td>ORY</td>\n",
       "      <td>FCO</td>\n",
       "      <td>NCE</td>\n",
       "      <td>...</td>\n",
       "      <td>TUN</td>\n",
       "      <td>NAP</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>DJE</td>\n",
       "      <td>DJE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD</th>\n",
       "      <td>2016-01-03 10:30:00</td>\n",
       "      <td>2016-01-13 15:05:00</td>\n",
       "      <td>2016-01-16 04:10:00</td>\n",
       "      <td>2016-01-17 14:10:00</td>\n",
       "      <td>2016-01-17 14:30:00</td>\n",
       "      <td>2016-01-17 16:20:00</td>\n",
       "      <td>2016-01-18 07:15:00</td>\n",
       "      <td>2016-01-18 07:35:00</td>\n",
       "      <td>2016-01-18 07:40:00</td>\n",
       "      <td>2016-01-18 07:45:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-04-12 17:05:00</td>\n",
       "      <td>2018-02-09 10:00:00</td>\n",
       "      <td>2018-08-08 22:00:00</td>\n",
       "      <td>2018-10-15 19:45:00</td>\n",
       "      <td>2018-12-19 23:45:00</td>\n",
       "      <td>2018-07-05 23:00:00</td>\n",
       "      <td>2018-01-13 08:00:00</td>\n",
       "      <td>2018-11-07 05:00:00</td>\n",
       "      <td>2018-01-23 18:00:00</td>\n",
       "      <td>2018-11-13 06:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STA</th>\n",
       "      <td>2016-01-03 12.55.00</td>\n",
       "      <td>2016-01-13 16.55.00</td>\n",
       "      <td>2016-01-16 06.45.00</td>\n",
       "      <td>2016-01-17 17.00.00</td>\n",
       "      <td>2016-01-17 15.50.00</td>\n",
       "      <td>2016-01-17 18.15.00</td>\n",
       "      <td>2016-01-18 09.00.00</td>\n",
       "      <td>2016-01-18 09.55.00</td>\n",
       "      <td>2016-01-18 09.00.00</td>\n",
       "      <td>2016-01-18 09.15.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-04-12 17.40.00</td>\n",
       "      <td>2018-02-09 11.15.00</td>\n",
       "      <td>2018-08-09 01.00.00</td>\n",
       "      <td>2018-10-15 20.45.00</td>\n",
       "      <td>2018-12-20 01.45.00</td>\n",
       "      <td>2018-07-06 02.00.00</td>\n",
       "      <td>2018-01-13 09.00.00</td>\n",
       "      <td>2018-11-07 12.50.00</td>\n",
       "      <td>2018-01-23 18.45.00</td>\n",
       "      <td>2018-11-13 07.05.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATUS</th>\n",
       "      <td>ATA</td>\n",
       "      <td>ATA</td>\n",
       "      <td>ATA</td>\n",
       "      <td>ATA</td>\n",
       "      <td>ATA</td>\n",
       "      <td>ATA</td>\n",
       "      <td>ATA</td>\n",
       "      <td>ATA</td>\n",
       "      <td>ATA</td>\n",
       "      <td>ATA</td>\n",
       "      <td>...</td>\n",
       "      <td>ATA</td>\n",
       "      <td>SCH</td>\n",
       "      <td>SCH</td>\n",
       "      <td>SCH</td>\n",
       "      <td>SCH</td>\n",
       "      <td>SCH</td>\n",
       "      <td>SCH</td>\n",
       "      <td>SCH</td>\n",
       "      <td>ATA</td>\n",
       "      <td>SCH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC</th>\n",
       "      <td>TU 32AIMN</td>\n",
       "      <td>TU 31BIMO</td>\n",
       "      <td>TU 32AIMN</td>\n",
       "      <td>TU 736IOK</td>\n",
       "      <td>TU 320IMU</td>\n",
       "      <td>TU 736IOP</td>\n",
       "      <td>TU 32AIMH</td>\n",
       "      <td>TU 32AIMI</td>\n",
       "      <td>TU 32AIMC</td>\n",
       "      <td>TU 31AIMK</td>\n",
       "      <td>...</td>\n",
       "      <td>TU 736IOL</td>\n",
       "      <td>TU CR9ISA</td>\n",
       "      <td>TU 320IMV</td>\n",
       "      <td>UG AT7LBE</td>\n",
       "      <td>TU 736IOP</td>\n",
       "      <td>TU 32AIML</td>\n",
       "      <td>UG AT7AT7</td>\n",
       "      <td>TU 736IOK</td>\n",
       "      <td>TU CR9ISA</td>\n",
       "      <td>TU CR9ISA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>260.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep_iata</th>\n",
       "      <td>CMN</td>\n",
       "      <td>MXP</td>\n",
       "      <td>TUN</td>\n",
       "      <td>DJE</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TLS</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>...</td>\n",
       "      <td>MIR</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>DJE</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>DJE</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep_country</th>\n",
       "      <td>MA</td>\n",
       "      <td>IT</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>FR</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>...</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep_elevation</th>\n",
       "      <td>656.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep_lat</th>\n",
       "      <td>33.3675</td>\n",
       "      <td>45.6306</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>33.875</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>43.629101</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>...</td>\n",
       "      <td>35.758099</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>33.875</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>33.875</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep_lon</th>\n",
       "      <td>-7.58997</td>\n",
       "      <td>8.72811</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.7755</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>1.36382</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>...</td>\n",
       "      <td>10.7547</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.7755</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.7755</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arr_iata</th>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>IST</td>\n",
       "      <td>NTE</td>\n",
       "      <td>ALG</td>\n",
       "      <td>TUN</td>\n",
       "      <td>BCN</td>\n",
       "      <td>ORY</td>\n",
       "      <td>FCO</td>\n",
       "      <td>NCE</td>\n",
       "      <td>...</td>\n",
       "      <td>TUN</td>\n",
       "      <td>NAP</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>DJE</td>\n",
       "      <td>DJE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arr_country</th>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TR</td>\n",
       "      <td>FR</td>\n",
       "      <td>DZ</td>\n",
       "      <td>TN</td>\n",
       "      <td>ES</td>\n",
       "      <td>FR</td>\n",
       "      <td>IT</td>\n",
       "      <td>FR</td>\n",
       "      <td>...</td>\n",
       "      <td>TN</td>\n",
       "      <td>IT</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arr_elevation</th>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arr_lat</th>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>41.275333</td>\n",
       "      <td>47.153198</td>\n",
       "      <td>36.691002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>41.2971</td>\n",
       "      <td>48.7253</td>\n",
       "      <td>41.804501</td>\n",
       "      <td>43.658401</td>\n",
       "      <td>...</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>40.886002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>33.875</td>\n",
       "      <td>33.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arr_lon</th>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>28.752</td>\n",
       "      <td>-1.61073</td>\n",
       "      <td>3.21541</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>2.07846</td>\n",
       "      <td>2.35944</td>\n",
       "      <td>12.2508</td>\n",
       "      <td>7.21587</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>14.2908</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.2272</td>\n",
       "      <td>10.7755</td>\n",
       "      <td>10.7755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 107833 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0                    1                    2        \n",
       "ID                      train_id_0           train_id_1           train_id_2  \\\n",
       "DATOP                   2016-01-03           2016-01-13           2016-01-16   \n",
       "FLTID                     TU 0712              TU 0757              TU 0214    \n",
       "DEPSTN                         CMN                  MXP                  TUN   \n",
       "ARRSTN                         TUN                  TUN                  IST   \n",
       "STD            2016-01-03 10:30:00  2016-01-13 15:05:00  2016-01-16 04:10:00   \n",
       "STA            2016-01-03 12.55.00  2016-01-13 16.55.00  2016-01-16 06.45.00   \n",
       "STATUS                         ATA                  ATA                  ATA   \n",
       "AC                       TU 32AIMN            TU 31BIMO            TU 32AIMN   \n",
       "target                       260.0                 20.0                  0.0   \n",
       "dep_iata                       CMN                  MXP                  TUN   \n",
       "dep_country                     MA                   IT                   TN   \n",
       "dep_elevation                656.0                768.0                 22.0   \n",
       "dep_lat                    33.3675              45.6306            36.851002   \n",
       "dep_lon                   -7.58997              8.72811              10.2272   \n",
       "arr_iata                       TUN                  TUN                  IST   \n",
       "arr_country                     TN                   TN                   TR   \n",
       "arr_elevation                 22.0                 22.0                325.0   \n",
       "arr_lat                  36.851002            36.851002            41.275333   \n",
       "arr_lon                    10.2272              10.2272               28.752   \n",
       "\n",
       "                            3                    4                    5        \n",
       "ID                      train_id_3           train_id_4           train_id_5  \\\n",
       "DATOP                   2016-01-17           2016-01-17           2016-01-17   \n",
       "FLTID                     TU 0480              TU 0338              TU 0283    \n",
       "DEPSTN                         DJE                  TUN                  TLS   \n",
       "ARRSTN                         NTE                  ALG                  TUN   \n",
       "STD            2016-01-17 14:10:00  2016-01-17 14:30:00  2016-01-17 16:20:00   \n",
       "STA            2016-01-17 17.00.00  2016-01-17 15.50.00  2016-01-17 18.15.00   \n",
       "STATUS                         ATA                  ATA                  ATA   \n",
       "AC                       TU 736IOK            TU 320IMU            TU 736IOP   \n",
       "target                         0.0                 22.0                 53.0   \n",
       "dep_iata                       DJE                  TUN                  TLS   \n",
       "dep_country                     TN                   TN                   FR   \n",
       "dep_elevation                 19.0                 22.0                499.0   \n",
       "dep_lat                     33.875            36.851002            43.629101   \n",
       "dep_lon                    10.7755              10.2272              1.36382   \n",
       "arr_iata                       NTE                  ALG                  TUN   \n",
       "arr_country                     FR                   DZ                   TN   \n",
       "arr_elevation                 90.0                 82.0                 22.0   \n",
       "arr_lat                  47.153198            36.691002            36.851002   \n",
       "arr_lon                   -1.61073              3.21541              10.2272   \n",
       "\n",
       "                            6                    7                    8        \n",
       "ID                      train_id_6           train_id_7           train_id_8  \\\n",
       "DATOP                   2016-01-18           2016-01-18           2016-01-18   \n",
       "FLTID                     TU 0514              TU 0716              TU 0752    \n",
       "DEPSTN                         TUN                  TUN                  TUN   \n",
       "ARRSTN                         BCN                  ORY                  FCO   \n",
       "STD            2016-01-18 07:15:00  2016-01-18 07:35:00  2016-01-18 07:40:00   \n",
       "STA            2016-01-18 09.00.00  2016-01-18 09.55.00  2016-01-18 09.00.00   \n",
       "STATUS                         ATA                  ATA                  ATA   \n",
       "AC                       TU 32AIMH            TU 32AIMI            TU 32AIMC   \n",
       "target                        10.0                 15.0                 16.0   \n",
       "dep_iata                       TUN                  TUN                  TUN   \n",
       "dep_country                     TN                   TN                   TN   \n",
       "dep_elevation                 22.0                 22.0                 22.0   \n",
       "dep_lat                  36.851002            36.851002            36.851002   \n",
       "dep_lon                    10.2272              10.2272              10.2272   \n",
       "arr_iata                       BCN                  ORY                  FCO   \n",
       "arr_country                     ES                   FR                   IT   \n",
       "arr_elevation                 12.0                291.0                 15.0   \n",
       "arr_lat                    41.2971              48.7253            41.804501   \n",
       "arr_lon                    2.07846              2.35944              12.2508   \n",
       "\n",
       "                            9       ...               107823   \n",
       "ID                      train_id_9  ...      train_id_107823  \\\n",
       "DATOP                   2016-01-18  ...           2018-04-12   \n",
       "FLTID                     TU 0996   ...             TU 9001    \n",
       "DEPSTN                         TUN  ...                  MIR   \n",
       "ARRSTN                         NCE  ...                  TUN   \n",
       "STD            2016-01-18 07:45:00  ...  2018-04-12 17:05:00   \n",
       "STA            2016-01-18 09.15.00  ...  2018-04-12 17.40.00   \n",
       "STATUS                         ATA  ...                  ATA   \n",
       "AC                       TU 31AIMK  ...            TU 736IOL   \n",
       "target                        21.0  ...                  0.0   \n",
       "dep_iata                       TUN  ...                  MIR   \n",
       "dep_country                     TN  ...                   TN   \n",
       "dep_elevation                 22.0  ...                  9.0   \n",
       "dep_lat                  36.851002  ...            35.758099   \n",
       "dep_lon                    10.2272  ...              10.7547   \n",
       "arr_iata                       NCE  ...                  TUN   \n",
       "arr_country                     FR  ...                   TN   \n",
       "arr_elevation                 12.0  ...                 22.0   \n",
       "arr_lat                  43.658401  ...            36.851002   \n",
       "arr_lon                    7.21587  ...              10.2272   \n",
       "\n",
       "                            107824               107825               107826   \n",
       "ID                 train_id_107824      train_id_107825      train_id_107826  \\\n",
       "DATOP                   2018-02-09           2018-08-08           2018-10-15   \n",
       "FLTID                     UG 1730             WKL 0000              UG 0011    \n",
       "DEPSTN                         TUN                  TUN                  DJE   \n",
       "ARRSTN                         NAP                  TUN                  TUN   \n",
       "STD            2018-02-09 10:00:00  2018-08-08 22:00:00  2018-10-15 19:45:00   \n",
       "STA            2018-02-09 11.15.00  2018-08-09 01.00.00  2018-10-15 20.45.00   \n",
       "STATUS                         SCH                  SCH                  SCH   \n",
       "AC                       TU CR9ISA            TU 320IMV            UG AT7LBE   \n",
       "target                         0.0                  0.0                  0.0   \n",
       "dep_iata                       TUN                  TUN                  DJE   \n",
       "dep_country                     TN                   TN                   TN   \n",
       "dep_elevation                 22.0                 22.0                 19.0   \n",
       "dep_lat                  36.851002            36.851002               33.875   \n",
       "dep_lon                    10.2272              10.2272              10.7755   \n",
       "arr_iata                       NAP                  TUN                  TUN   \n",
       "arr_country                     IT                   TN                   TN   \n",
       "arr_elevation                294.0                 22.0                 22.0   \n",
       "arr_lat                  40.886002            36.851002            36.851002   \n",
       "arr_lon                    14.2908              10.2272              10.2272   \n",
       "\n",
       "                            107827               107828               107829   \n",
       "ID                 train_id_107827      train_id_107828      train_id_107829  \\\n",
       "DATOP                   2018-12-19           2018-07-05           2018-01-13   \n",
       "FLTID                    SGT 0000             WKL 0000              UG 0003    \n",
       "DEPSTN                         TUN                  TUN                  DJE   \n",
       "ARRSTN                         TUN                  TUN                  TUN   \n",
       "STD            2018-12-19 23:45:00  2018-07-05 23:00:00  2018-01-13 08:00:00   \n",
       "STA            2018-12-20 01.45.00  2018-07-06 02.00.00  2018-01-13 09.00.00   \n",
       "STATUS                         SCH                  SCH                  SCH   \n",
       "AC                       TU 736IOP            TU 32AIML            UG AT7AT7   \n",
       "target                         0.0                  0.0                  0.0   \n",
       "dep_iata                       TUN                  TUN                  DJE   \n",
       "dep_country                     TN                   TN                   TN   \n",
       "dep_elevation                 22.0                 22.0                 19.0   \n",
       "dep_lat                  36.851002            36.851002               33.875   \n",
       "dep_lon                    10.2272              10.2272              10.7755   \n",
       "arr_iata                       TUN                  TUN                  TUN   \n",
       "arr_country                     TN                   TN                   TN   \n",
       "arr_elevation                 22.0                 22.0                 22.0   \n",
       "arr_lat                  36.851002            36.851002            36.851002   \n",
       "arr_lon                    10.2272              10.2272              10.2272   \n",
       "\n",
       "                            107830               107831               107832  \n",
       "ID                 train_id_107830      train_id_107831      train_id_107832  \n",
       "DATOP                   2018-11-07           2018-01-23           2018-11-13  \n",
       "FLTID                    SGT 0000              UG 0010              UG 0002   \n",
       "DEPSTN                         TUN                  TUN                  TUN  \n",
       "ARRSTN                         TUN                  DJE                  DJE  \n",
       "STD            2018-11-07 05:00:00  2018-01-23 18:00:00  2018-11-13 06:15:00  \n",
       "STA            2018-11-07 12.50.00  2018-01-23 18.45.00  2018-11-13 07.05.00  \n",
       "STATUS                         SCH                  ATA                  SCH  \n",
       "AC                       TU 736IOK            TU CR9ISA            TU CR9ISA  \n",
       "target                         0.0                  0.0                  0.0  \n",
       "dep_iata                       TUN                  TUN                  TUN  \n",
       "dep_country                     TN                   TN                   TN  \n",
       "dep_elevation                 22.0                 22.0                 22.0  \n",
       "dep_lat                  36.851002            36.851002            36.851002  \n",
       "dep_lon                    10.2272              10.2272              10.2272  \n",
       "arr_iata                       TUN                  DJE                  DJE  \n",
       "arr_country                     TN                   TN                   TN  \n",
       "arr_elevation                 22.0                 19.0                 19.0  \n",
       "arr_lat                  36.851002               33.875               33.875  \n",
       "arr_lon                    10.2272              10.7755              10.7755  \n",
       "\n",
       "[20 rows x 107833 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Making a preprocessing pipeline\n",
    "\n",
    "# Data loading\n",
    "df_flightdata = pd.read_csv('data/Train.csv')\n",
    "df_airportdata = pd.read_csv('data/airportdata.csv', index_col=0)\n",
    "\n",
    "# Defining categorical data and features to drop. Here we use label encoding. Hot encoding is not used.\n",
    "columns_to_drop = ['id', 'std', 'sta', 'fltid', 'arr_iata', 'dep_iata', 'ac','status']\n",
    "categorical_columns = ['depstn', 'arrstn',  'arr_country', 'dep_country', 'season', 'airline_code', 'international_flight']\n",
    "\n",
    "# Join the DataFrames first and then apply the preprocessing pipeline\n",
    "merged_data = df_flightdata.join(df_airportdata[['iata', 'country', 'elevation', 'lat', 'lon']].add_prefix('dep_'), how='left', on='DEPSTN') \\\n",
    "    .join(df_airportdata[['iata', 'country', 'elevation', 'lat', 'lon']].add_prefix('arr_'), how='left', on='ARRSTN')\n",
    "    \n",
    "display(merged_data.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Frontend should only use known data(airports, airlines, ...) because label encoding on the api needs known data  \n",
    "# Unique airports and airlines of the original dataset is extracted\n",
    "\n",
    "# Get unique values for 'DEPSTN' and 'ARRSTN'\n",
    "unique_depstn = df_flightdata['DEPSTN'].unique()\n",
    "unique_arrstn = df_flightdata['ARRSTN'].unique()\n",
    "\n",
    "# Combine unique values into a single DataFrame and make it unique\n",
    "unique_airports = pd.concat([pd.Series(unique_depstn), pd.Series(unique_arrstn)], ignore_index=True).unique()\n",
    "\n",
    "# Create a dictionary for airports\n",
    "org_airports = {\"airports\": unique_airports.tolist()}\n",
    "\n",
    "# Save airports data to a JSON file\n",
    "with open('./data/airports_in_org_dataset.json', 'w') as airports_json_file:\n",
    "    json.dump(org_airports, airports_json_file)\n",
    "\n",
    "\n",
    "# Get values for the airline code out of flight id\n",
    "unique_airline_code = (df_flightdata['FLTID'].str[:2]).unique()\n",
    "\n",
    "# Create a dictionary for airline codes\n",
    "org_airline_codes = {\"airline_code\": unique_airline_code.tolist()}\n",
    "\n",
    "# Save airports data to a JSON file\n",
    "with open('./data/airline_codes_in_org_dataset.json', 'w') as airline_codes_json_file:\n",
    "    json.dump(org_airline_codes, airline_codes_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATOP</th>\n",
       "      <th>FLTID</th>\n",
       "      <th>DEPSTN</th>\n",
       "      <th>ARRSTN</th>\n",
       "      <th>STD</th>\n",
       "      <th>STA</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>AC</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_id_0</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>TU 0712</td>\n",
       "      <td>CMN</td>\n",
       "      <td>TUN</td>\n",
       "      <td>2016-01-03 10:30:00</td>\n",
       "      <td>2016-01-03 12.55.00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 32AIMN</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_id_1</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>TU 0757</td>\n",
       "      <td>MXP</td>\n",
       "      <td>TUN</td>\n",
       "      <td>2016-01-13 15:05:00</td>\n",
       "      <td>2016-01-13 16.55.00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 31BIMO</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_id_2</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>TU 0214</td>\n",
       "      <td>TUN</td>\n",
       "      <td>IST</td>\n",
       "      <td>2016-01-16 04:10:00</td>\n",
       "      <td>2016-01-16 06.45.00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 32AIMN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_id_3</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>TU 0480</td>\n",
       "      <td>DJE</td>\n",
       "      <td>NTE</td>\n",
       "      <td>2016-01-17 14:10:00</td>\n",
       "      <td>2016-01-17 17.00.00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 736IOK</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_id_4</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>TU 0338</td>\n",
       "      <td>TUN</td>\n",
       "      <td>ALG</td>\n",
       "      <td>2016-01-17 14:30:00</td>\n",
       "      <td>2016-01-17 15.50.00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 320IMU</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID       DATOP     FLTID DEPSTN ARRSTN                  STD   \n",
       "0  train_id_0  2016-01-03  TU 0712     CMN    TUN  2016-01-03 10:30:00  \\\n",
       "1  train_id_1  2016-01-13  TU 0757     MXP    TUN  2016-01-13 15:05:00   \n",
       "2  train_id_2  2016-01-16  TU 0214     TUN    IST  2016-01-16 04:10:00   \n",
       "3  train_id_3  2016-01-17  TU 0480     DJE    NTE  2016-01-17 14:10:00   \n",
       "4  train_id_4  2016-01-17  TU 0338     TUN    ALG  2016-01-17 14:30:00   \n",
       "\n",
       "                   STA STATUS         AC  target  \n",
       "0  2016-01-03 12.55.00    ATA  TU 32AIMN   260.0  \n",
       "1  2016-01-13 16.55.00    ATA  TU 31BIMO    20.0  \n",
       "2  2016-01-16 06.45.00    ATA  TU 32AIMN     0.0  \n",
       "3  2016-01-17 17.00.00    ATA  TU 736IOK     0.0  \n",
       "4  2016-01-17 15.50.00    ATA  TU 320IMU    22.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flightdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FixColumnNames(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X.columns = X.columns.str.replace(' ', '_').str.lower().str.replace('-', '_')\n",
    "        return X\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Return a dictionary with any essential attributes\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_state(cls, state):\n",
    "        # Create an instance of the class using the state dictionary\n",
    "        return cls()\n",
    "\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_drop):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X.drop(self.columns_to_drop, axis=1, inplace=True)\n",
    "        return X\n",
    "\n",
    "    def get_state(self):\n",
    "        # Return a dictionary with any essential attributes\n",
    "        return {'columns_to_drop': self.columns_to_drop}\n",
    "\n",
    "    @classmethod\n",
    "    def from_state(cls, state):\n",
    "        # Create an instance of the class using the state dictionary\n",
    "        return cls(columns_to_drop=state['columns_to_drop'])\n",
    "    \n",
    "class LabelEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.label_encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            label_encoder = LabelEncoder()\n",
    "            label_encoder.fit(X[col])\n",
    "            self.label_encoders[col] = label_encoder\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_encoded = X.copy()\n",
    "        for col, label_encoder in self.label_encoders.items():\n",
    "            X_encoded[col] = label_encoder.transform(X_encoded[col])\n",
    "        return X_encoded\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = {\n",
    "            'columns': self.columns,\n",
    "            'label_encoders': {col: label_encoder.classes_.tolist() for col, label_encoder in self.label_encoders.items()}\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    @classmethod\n",
    "    def from_state(cls, state):\n",
    "        columns = state['columns']\n",
    "        label_encoders = {col: LabelEncoder() for col in columns}\n",
    "\n",
    "        for col, classes in state['label_encoders'].items():\n",
    "            label_encoder = label_encoders[col]\n",
    "            label_encoder.classes_ = classes\n",
    "\n",
    "        instance = cls(columns=columns)\n",
    "        instance.label_encoders = label_encoders\n",
    "\n",
    "        return instance\n",
    "    \n",
    "class CalculateFlightDistance(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        def calculate_distance(row):\n",
    "            dep_coords = (row['dep_lat'], row['dep_lon'])\n",
    "            arr_coords = (row['arr_lat'], row['arr_lon'])\n",
    "            distance = geodesic(dep_coords, arr_coords).kilometers\n",
    "            return int(round(distance, 0))\n",
    "\n",
    "        X['flight_distance_in_km'] = X.apply(calculate_distance, axis=1)\n",
    "        return X\n",
    "\n",
    "    def get_state(self):\n",
    "        # Return a dictionary with any essential attributes\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_state(cls, state):\n",
    "        # Create an instance of the class using the state dictionary\n",
    "        return cls()\n",
    "\n",
    "class AddAdditionalFlightDataFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['sta'] = pd.to_datetime(X['sta'], format='%Y-%m-%d %H.%M.%S')\n",
    "        X['std'] = pd.to_datetime(X['std'], format='%Y-%m-%d %H:%M:%S')\n",
    "        X['datop'] = pd.to_datetime(X['datop'], format='%Y-%m-%d')\n",
    "        X['std_time'] = X['std'].dt.time\n",
    "        X['sta_time'] = X['sta'].dt.time\n",
    "        X['std_time'] = X['std_time'].astype(str).str.replace(':', '').astype(int)\n",
    "        X['sta_time'] = X['sta_time'].astype(str).str.replace(':', '').astype(int)\n",
    "        \n",
    "        X['elevation_dif'] = (X['arr_elevation'] - X['dep_elevation'])\n",
    "        X['flight_time_in_min'] = (X['sta'] - X['std']).dt.total_seconds() / 60\n",
    "        X['average_flight_speed_km_h'] = (X['flight_distance_in_km'] * 60 / X['flight_time_in_min']).round().astype(int)\n",
    "        X['international_flight'] = np.where(X['arr_country'] != X['dep_country'], 'international', 'domestic')\n",
    "        X['airline_code'] = X['fltid'].str[:2]\n",
    "        # Extract year, month, and day components\n",
    "        X['year'] = X['datop'].dt.year\n",
    "        X['month'] = X['datop'].dt.month\n",
    "        X['day'] = X['datop'].dt.day\n",
    "        X['datop'] = X['datop'].astype(str).str.replace('-', '').astype(int)\n",
    "        \n",
    "        # Create the seasons column\n",
    "        X.loc[(X['month'] < 3) | (X['month'] == 12), 'season'] = 'winter'\n",
    "        X.loc[(X['month'] >= 3) & (X['month'] < 6), 'season'] = 'spring' \n",
    "        X.loc[(X['month'] >= 6) & (X['month'] < 9), 'season'] = 'summer' \n",
    "        X.loc[(X['month'] >= 9) & (X['month'] < 12), 'season'] = 'autumn'\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Return a dictionary with any essential attributes\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_state(cls, state):\n",
    "        # Create an instance of the class using the state dictionary\n",
    "        return cls()\n",
    "\n",
    "\n",
    "# Define the preprocessing steps in the pipeline\n",
    "preprocessing_steps = [\n",
    "    ('column_name_fixer', FixColumnNames()),\n",
    "    ('calculate_flight_distance', CalculateFlightDistance()),\n",
    "    ('add_additional_flight_data_features', AddAdditionalFlightDataFeatures()),\n",
    "    ('drop_columns', DropColumns(columns_to_drop)),\n",
    "    ('encode_labels', LabelEncoderTransformer(categorical_columns))\n",
    "\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=preprocessing_steps)\n",
    "# df_processed contains the preprocessed data\n",
    "df_processed = preprocessing_pipeline.fit_transform(merged_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialization for ColumnNameFixer\n",
    "def serialize_fix_column_names(transformer, filename):\n",
    "    state = transformer.get_state()\n",
    "    joblib.dump(state, filename)\n",
    "\n",
    "# Serialization for CalculateDistance\n",
    "def serialize_calculate_flight_distance(transformer, filename):\n",
    "    state = transformer.get_state()\n",
    "    joblib.dump(state, filename)\n",
    "\n",
    "# Serialization for CustomFeaturesAdder\n",
    "def serialize_add_additional_flight_data_features(transformer, filename):\n",
    "    state = transformer.get_state()\n",
    "    joblib.dump(state, filename)\n",
    "\n",
    "# Serialization for DropColumns\n",
    "def serialize_drop_columns(transformer, filename):\n",
    "    state = transformer.get_state()\n",
    "    joblib.dump(state, filename)\n",
    "\n",
    "# Serialization for LabelEncoderTransformer\n",
    "def serialize_label_encoder_transformer(transformer, filename):\n",
    "    state = transformer.get_state()\n",
    "    joblib.dump(state, filename)\n",
    "\n",
    "# Serialize all custom classes\n",
    "serialize_fix_column_names(preprocessing_pipeline.named_steps['column_name_fixer'], 'data/pipelines/classes/column_name_fixer.pkl')\n",
    "serialize_calculate_flight_distance(preprocessing_pipeline.named_steps['calculate_flight_distance'], 'data/pipelines/classes/calculate_flight_distance.pkl')\n",
    "serialize_add_additional_flight_data_features(preprocessing_pipeline.named_steps['add_additional_flight_data_features'], 'data/pipelines/classes/add_additional_flight_data_features.pkl')\n",
    "serialize_drop_columns(preprocessing_pipeline.named_steps['drop_columns'], 'data/pipelines/classes/drop_columns.pkl')\n",
    "serialize_label_encoder_transformer(preprocessing_pipeline.named_steps['encode_labels'], 'data/pipelines/classes/label_encoder_transformer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/pipelines/preprocessing_pipeline.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and save the preprocessing pipeline\n",
    "joblib.dump(preprocessing_pipeline, 'data/pipelines/preprocessing_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually serialize each custom class\n",
    "custom_classes = {\n",
    "    'column_name_fixer': preprocessing_pipeline.named_steps['column_name_fixer'],\n",
    "    'calculate_distance': preprocessing_pipeline.named_steps['calculate_distance'],\n",
    "    'custom_features_adder': preprocessing_pipeline.named_steps['custom_features_adder'],\n",
    "    'label_encoding': preprocessing_pipeline.named_steps['label_encoding'],\n",
    "    'drop_columns': preprocessing_pipeline.named_steps['drop_columns']\n",
    "}\n",
    "\n",
    "for name, custom_class in custom_classes.items():\n",
    "    custom_class_state = custom_class.get_state()  # Define a method to get the class state\n",
    "    with open(f'{name}_state.pkl', 'wb') as file:\n",
    "        joblib.dump(custom_class_state, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Serialize the pipeline using cloudpickle\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdata/preprocessing_pipeline_cp.joblib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     cloudpickle\u001b[39m.\u001b[39;49mdump(preprocessing_pipeline, f)\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/.venv/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:605\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m--> 605\u001b[0m     CloudPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/.venv/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:107\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minject_addons()\n\u001b[1;32m    106\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[1;32m    108\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mrecursion\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[0;32m--> 487\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(obj)\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[1;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/.venv/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:518\u001b[0m, in \u001b[0;36mCloudPickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_setitems(dictitems)\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m     save(state)\n\u001b[1;32m    519\u001b[0m     write(pickle\u001b[39m.\u001b[39mBUILD)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 972\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    997\u001b[0m         save(k)\n\u001b[0;32m--> 998\u001b[0m         save(v)\n\u001b[1;32m    999\u001b[0m     write(SETITEMS)\n\u001b[1;32m   1000\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:932\u001b[0m, in \u001b[0;36m_Pickler.save_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m LIST)\n\u001b[1;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 932\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_appends(obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:956\u001b[0m, in \u001b[0;36m_Pickler._batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    954\u001b[0m     write(MARK)\n\u001b[1;32m    955\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tmp:\n\u001b[0;32m--> 956\u001b[0m         save(x)\n\u001b[1;32m    957\u001b[0m     write(APPENDS)\n\u001b[1;32m    958\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    886\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 887\u001b[0m         save(element)\n\u001b[1;32m    888\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/.venv/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:485\u001b[0m, in \u001b[0;36mCloudPickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mPicklingError(\n\u001b[1;32m    483\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39margs[0] from __newobj__ args has the wrong class\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    484\u001b[0m args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 485\u001b[0m save(\u001b[39mcls\u001b[39;49m)\n\u001b[1;32m    487\u001b[0m \u001b[39m#Don't pickle transient entries\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obj, \u001b[39m'\u001b[39m\u001b[39m__transient__\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/.venv/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:353\u001b[0m, in \u001b[0;36mCloudPickler.save_global\u001b[0;34m(self, obj, name, pack)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[39mif\u001b[39;00m new_override:\n\u001b[1;32m    351\u001b[0m         d[\u001b[39m'\u001b[39m\u001b[39m__new__\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(typ, (obj\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m, obj\u001b[39m.\u001b[39;49m\u001b[39m__bases__\u001b[39;49m, d), obj\u001b[39m=\u001b[39;49mobj)\n\u001b[1;32m    354\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mPicklingError(\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m obj)\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/.venv/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:500\u001b[0m, in \u001b[0;36mCloudPickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     save(func)\n\u001b[0;32m--> 500\u001b[0m     save(args)\n\u001b[1;32m    501\u001b[0m     write(pickle\u001b[39m.\u001b[39mREDUCE)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    886\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 887\u001b[0m         save(element)\n\u001b[1;32m    888\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 972\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    997\u001b[0m         save(k)\n\u001b[0;32m--> 998\u001b[0m         save(v)\n\u001b[1;32m    999\u001b[0m     write(SETITEMS)\n\u001b[1;32m   1000\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/.venv/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:193\u001b[0m, in \u001b[0;36mCloudPickler.save_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m# if func is lambda, def'ed at prompt, is in main, or is nested, then\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m# we'll pickle the actual function object rather than simply saving a\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39m# reference (as is done in default pickler), via save_function_tuple.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m islambda(obj) \u001b[39mor\u001b[39;00m obj\u001b[39m.\u001b[39m\u001b[39m__code__\u001b[39m\u001b[39m.\u001b[39mco_filename \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m<stdin>\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m themodule \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[39m#print(\"save global\", islambda(obj), obj.__code__.co_filename, modname, themodule)\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_function_tuple(obj)\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[39m# func is nested\u001b[39;00m\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/.venv/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:229\u001b[0m, in \u001b[0;36mCloudPickler.save_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    226\u001b[0m save \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave\n\u001b[1;32m    227\u001b[0m write \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite\n\u001b[0;32m--> 229\u001b[0m code, f_globals, defaults, closure, dct, base_globals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_func_data(func)\n\u001b[1;32m    231\u001b[0m save(_fill_function)  \u001b[39m# skeleton function updater\u001b[39;00m\n\u001b[1;32m    232\u001b[0m write(pickle\u001b[39m.\u001b[39mMARK)    \u001b[39m# beginning of tuple that _fill_function expects\u001b[39;00m\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/.venv/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:290\u001b[0m, in \u001b[0;36mCloudPickler.extract_func_data\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    287\u001b[0m code \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__code__\u001b[39m\n\u001b[1;32m    289\u001b[0m \u001b[39m# extract all global ref's\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m func_global_refs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_code_globals(code)\n\u001b[1;32m    292\u001b[0m \u001b[39m# process all variables referenced by global environment\u001b[39;00m\n\u001b[1;32m    293\u001b[0m f_globals \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/.venv/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:272\u001b[0m, in \u001b[0;36mCloudPickler.extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    270\u001b[0m             extended_arg \u001b[39m=\u001b[39m oparg\u001b[39m*\u001b[39m\u001b[39m65536\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[39mif\u001b[39;00m op \u001b[39min\u001b[39;00m GLOBAL_OPS:\n\u001b[0;32m--> 272\u001b[0m             out_names\u001b[39m.\u001b[39madd(names[oparg])\n\u001b[1;32m    274\u001b[0m \u001b[39m# see if nested function have any global refs\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m co\u001b[39m.\u001b[39mco_consts:\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Serialize the pipeline using cloudpickle: this doesnt work\n",
    "# with open('data/preprocessing_pipeline_cp.joblib', 'wb') as f:\n",
    "#     cloudpickle.dump(preprocessing_pipeline, f)\n",
    "# Saving using cloudpickle - allows lambda function, classes  etc. , Joblib causing errors on the API \n",
    "# def dump_pickle(data,path):\n",
    "#     path = str(path)\n",
    "#     with open(path,\"wb\") as f:\n",
    "#         cloudpickle.dump(data,f)\n",
    "        \n",
    "# dump_pickle(preprocessing_pipeline,'data/preprocessing_pipeline_cp.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m columns_to_drop \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msta\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfltid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39marr_iata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdep_iata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mac\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Importing the custom classes from a file so it also works on the api\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# https://stackoverflow.com/questions/57888291/how-to-properly-pickle-sklearn-pipeline-when-using-custom-transformer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcustom_transformer_classes\u001b[39;00m \u001b[39mimport\u001b[39;00m ColumnNameFixer,CalculateDistance,CustomFeaturesAdder,DropColumns,LabelEncoderTransformer\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Define the preprocessing steps in the pipeline\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m preprocessing_steps \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mcolumn_name_fixer\u001b[39m\u001b[39m'\u001b[39m, ColumnNameFixer()),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mcalculate_distance\u001b[39m\u001b[39m'\u001b[39m, CalculateDistance()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/flight_delays_ML/2_1_Preprocessing_Pipeline.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/spiced/data_science/flight_delays_ML/custom_transformer_classes.py:2\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mColumnNameFixer\u001b[39;00m(BaseEstimator, TransformerMixin):\n\u001b[1;32m      3\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Defining categorical data and features to drop. Here we use label encoding. Hot encoding is not used.\n",
    "categorical_columns = ['depstn', 'arrstn', 'status', 'arr_country', 'dep_country', 'season', 'airline_code', 'international_flight','ac','dep_iata','arr_iata','fltid']\n",
    "columns_to_drop = ['id', 'std', 'sta', 'fltid', 'arr_iata', 'dep_iata', 'ac']\n",
    "# Importing the custom classes from a file so it also works on the api\n",
    "# https://stackoverflow.com/questions/57888291/how-to-properly-pickle-sklearn-pipeline-when-using-custom-transformer\n",
    "from custom_transformer_classes import ColumnNameFixer,CalculateDistance,CustomFeaturesAdder,DropColumns,LabelEncoderTransformer\n",
    "# Define the preprocessing steps in the pipeline\n",
    "preprocessing_steps = [\n",
    "    ('column_name_fixer', ColumnNameFixer()),\n",
    "    ('calculate_distance', CalculateDistance()),\n",
    "    ('custom_features_adder', CustomFeaturesAdder()),\n",
    "    ('label_encoding', LabelEncoderTransformer(categorical_columns)),\n",
    "    ('drop_columns', DropColumns(columns_to_drop))\n",
    "\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=preprocessing_steps)\n",
    "preprocessing_pipeline.fit_transform(merged_data)\n",
    "# Save the preprocessing pipeline to a file\n",
    "joblib.dump(preprocessing_pipeline, 'data/preprocessing_pipeline_imported_classes.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column_name_fixer': ColumnNameFixer(), 'calculate_distance': CalculateDistance(), 'custom_features_adder': CustomFeaturesAdder(), 'label_encoding': LabelEncoderTransformer(columns=['depstn', 'arrstn', 'status', 'arr_country',\n",
      "                                 'dep_country', 'season', 'airline_code',\n",
      "                                 'international_flight', 'ac', 'dep_iata',\n",
      "                                 'arr_iata', 'fltid']), 'drop_columns': DropColumns(columns_to_drop=['id', 'std', 'sta', 'fltid', 'arr_iata',\n",
      "                             'dep_iata', 'ac'])}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the joblib file\n",
    "loaded_pipeline = joblib.load('data/preprocessing_pipeline.joblib')\n",
    "\n",
    "# Check if the custom classes are included in the loaded pipeline\n",
    "print(loaded_pipeline.named_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table Representation of Combined DataFrame:\n",
      "+---+------------------+---------------------+-----------------+-----------+-----------+---------------------------+---------------------------+---------+-----------------+---------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+-----------------------+-----------------+-----------------+-----------------+--------------------+---------------------------+----------------------+--------------+-------------+-------+---------+------------+\n",
      "|   |   id (dropped)   |        datop        | fltid (dropped) |  depstn   |  arrstn   |       std (dropped)       |       sta (dropped)       | status  |  ac (dropped)   |    target     | dep_iata (dropped) | dep_country | dep_elevation |            dep_lat            |            dep_lon            | arr_iata (dropped) | arr_country | arr_elevation |            arr_lat            |            arr_lon            | flight_distance_in_km |    std_time     |    sta_time     |  elevation_dif  | flight_time_in_min | average_flight_speed_km_h | international_flight | airline_code |    year     | month |   day   |   season   |\n",
      "+---+------------------+---------------------+-----------------+-----------+-----------+---------------------------+---------------------------+---------+-----------------+---------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+-----------------------+-----------------+-----------------+-----------------+--------------------+---------------------------+----------------------+--------------+-------------+-------+---------+------------+\n",
      "| 0 | nan | train_id_0 | 20160103 | 20160103 |  nan | TU 0712  | 31 | CMN  | 119 | TUN | nan | 2016-01-03 10:30:00 | nan | 2016-01-03 12:55:00 | 0 | ATA | nan | TU 32AIMN | 260.0 | 260.0 |     nan | CMN      |   31 | MA   | 656.0 | 656.0 | 33.3675003052 | 33.3675003052 | -7.5899701118 | -7.5899701118 |     nan | TUN      |   48 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |      1667 | 1667      | 103000 | 103000 | 125500 | 125500 | -634.0 | -634.0 |   145.0 | 145.0    |         690 | 690         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 |  3 | 3  | 3 | winter |\n",
      "| 1 | nan | train_id_1 | 20160113 | 20160113 |  nan | TU 0757  | 86 | MXP  | 119 | TUN | nan | 2016-01-13 15:05:00 | nan | 2016-01-13 16:55:00 | 0 | ATA | nan | TU 31BIMO |  20.0 | 20.0  |     nan | MXP      |   26 | IT   | 768.0 | 768.0 | 45.6305999756 | 45.6305999756 |  8.7281103134 | 8.7281103134  |     nan | TUN      |   48 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |       983 | 983       | 150500 | 150500 | 165500 | 165500 | -746.0 | -746.0 |   110.0 | 110.0    |         536 | 536         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 13 | 13 | 3 | winter |\n",
      "| 2 | nan | train_id_2 | 20160116 | 20160116 |  nan | TU 0214  | 123 | TUN | 58 | IST  | nan | 2016-01-16 04:10:00 | nan | 2016-01-16 06:45:00 | 0 | ATA | nan | TU 32AIMN |   0.0 | 0.0   |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | IST      |   49 | TR   | 325.0 | 325.0 |     41.275333 | 41.275333     |        28.752 | 28.752        |      1673 | 1673      |  41000 | 41000  |  64500 | 64500  |  303.0 | 303.0  |   155.0 | 155.0    |         648 | 648         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 16 | 16 | 3 | winter |\n",
      "| 3 | nan | train_id_3 | 20160117 | 20160117 |  nan | TU 0480  | 37 | DJE  | 91 | NTE  | nan | 2016-01-17 14:10:00 | nan | 2016-01-17 17:00:00 | 0 | ATA | nan | TU 736IOK |   0.0 | 0.0   |     nan | DJE      |   51 | TN   |  19.0 | 19.0  |        33.875 | 33.875        | 10.7755002975 | 10.7755002975 |     nan | NTE      |   17 | FR   |  90.0 | 90.0  | 47.1531982422 | 47.1531982422 |  -1.610730052 | -1.610730052  |      1805 | 1805      | 141000 | 141000 | 170000 | 170000 |   71.0 | 71.0   |   170.0 | 170.0    |         637 | 637         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 17 | 17 | 3 | winter |\n",
      "| 4 | nan | train_id_4 | 20160117 | 20160117 |  nan | TU 0338  | 123 | TUN |  4 | ALG  | nan | 2016-01-17 14:30:00 | nan | 2016-01-17 15:50:00 | 0 | ATA | nan | TU 320IMU |  22.0 | 22.0  |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | ALG      |   14 | DZ   |  82.0 | 82.0  | 36.6910018921 | 36.6910018921 |  3.2154099941 | 3.2154099941  |       626 | 626       | 143000 | 143000 | 155000 | 155000 |   60.0 | 60.0   |    80.0 | 80.0     |         470 | 470         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 17 | 17 | 3 | winter |\n",
      "| 5 | nan | train_id_5 | 20160117 | 20160117 |  nan | TU 0283  | 117 | TLS | 119 | TUN | nan | 2016-01-17 16:20:00 | nan | 2016-01-17 18:15:00 | 0 | ATA | nan | TU 736IOP |  53.0 | 53.0  |     nan | TLS      |   18 | FR   | 499.0 | 499.0 | 43.6291007996 | 43.6291007996 |  1.3638199568 | 1.3638199568  |     nan | TUN      |   48 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |      1064 | 1064      | 162000 | 162000 | 181500 | 181500 | -477.0 | -477.0 |   115.0 | 115.0    |         555 | 555         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 17 | 17 | 3 | winter |\n",
      "| 6 | nan | train_id_6 | 20160118 | 20160118 |  nan | TU 0514  | 123 | TUN | 10 | BCN  | nan | 2016-01-18 07:15:00 | nan | 2016-01-18 09:00:00 | 0 | ATA | nan | TU 32AIMH |  10.0 | 10.0  |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | BCN      |   16 | ES   |  12.0 | 12.0  | 41.2971000671 | 41.2971000671 |  2.0784599781 | 2.0784599781  |       860 | 860       |  71500 | 71500  |  90000 | 90000  |  -10.0 | -10.0  |   105.0 | 105.0    |         491 | 491         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 18 | 18 | 3 | winter |\n",
      "| 7 | nan | train_id_7 | 20160118 | 20160118 |  nan | TU 0716  | 123 | TUN | 94 | ORY  | nan | 2016-01-18 07:35:00 | nan | 2016-01-18 09:55:00 | 0 | ATA | nan | TU 32AIMI |  15.0 | 15.0  |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | ORY      |   17 | FR   | 291.0 | 291.0 | 48.7252998352 | 48.7252998352 |  2.3594400883 | 2.3594400883  |      1466 | 1466      |  73500 | 73500  |  95500 | 95500  |  269.0 | 269.0  |   140.0 | 140.0    |         628 | 628         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 18 | 18 | 3 | winter |\n",
      "| 8 | nan | train_id_8 | 20160118 | 20160118 |  nan | TU 0752  | 123 | TUN | 46 | FCO  | nan | 2016-01-18 07:40:00 | nan | 2016-01-18 09:00:00 | 0 | ATA | nan | TU 32AIMC |  16.0 | 16.0  |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | FCO      |   24 | IT   |  15.0 | 15.0  | 41.8045005798 | 41.8045005798 | 12.2508001328 | 12.2508001328 |       577 | 577       |  74000 | 74000  |  90000 | 90000  |   -7.0 | -7.0   |    80.0 | 80.0     |         433 | 433         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 18 | 18 | 3 | winter |\n",
      "| 9 | nan | train_id_9 | 20160118 | 20160118 |  nan | TU 0996  | 123 | TUN | 87 | NCE  | nan | 2016-01-18 07:45:00 | nan | 2016-01-18 09:15:00 | 0 | ATA | nan | TU 31AIMK |  21.0 | 21.0  |     nan | TUN      |   51 | TN   |  22.0 | 22.0  | 36.8510017395 | 36.8510017395 | 10.2271995544 | 10.2271995544 |     nan | NCE      |   17 | FR   |  12.0 | 12.0  | 43.6584014893 | 43.6584014893 |  7.2158699036 | 7.2158699036  |       798 | 798       |  74500 | 74500  |  91500 | 91500  |  -10.0 | -10.0  |    90.0 | 90.0     |         532 | 532         |        1 | 1         |   14 | TU    | 2016 | 2016 | 1 | 1 | 18 | 18 | 3 | winter |\n",
      "+---+------------------+---------------------+-----------------+-----------+-----------+---------------------------+---------------------------+---------+-----------------+---------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+--------------------+-------------+---------------+-------------------------------+-------------------------------+-----------------------+-----------------+-----------------+-----------------+--------------------+---------------------------+----------------------+--------------+-------------+-------+---------+------------+\n"
     ]
    }
   ],
   "source": [
    "# tabulate library to compare the df's\n",
    "\n",
    "# Extract the headers of merged_data\n",
    "headers_merged = merged_data.columns.tolist()\n",
    "\n",
    "# Initialize an empty list to store rows\n",
    "headers_list = []\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the first ten rows in both DataFrames and add them to the table_data\n",
    "for index, header_merged in enumerate(headers_merged):\n",
    "    if header_merged in df_processed.columns:\n",
    "        # Headers match, compare data\n",
    "        header_processed = header_merged\n",
    "        row_processed = df_processed.head(10).iloc[:, df_processed.columns.get_loc(header_merged)]\n",
    "        row_merged = merged_data.head(10).iloc[:, index]\n",
    "    else:\n",
    "        # Headers don't match, create an empty column\n",
    "        header_processed = f'{header_merged} (dropped)'\n",
    "        row_processed = pd.Series([np.nan] * 10)\n",
    "        row_merged = merged_data.head(10).iloc[:, index]\n",
    "\n",
    "    # Combine values from both DataFrames in a single cell\n",
    "    combined_row = []\n",
    "    for val_processed, val_merged in zip(row_processed, row_merged):\n",
    "        combined_row.append(f'{val_processed} | {val_merged}')\n",
    "    headers_list.append(header_processed)\n",
    "    combined_df[header_processed] = combined_row\n",
    "\n",
    "# Create the table\n",
    "# Convert the DataFrame to a table and print it\n",
    "combined_table = tabulate(combined_df, headers='keys', tablefmt='pretty')\n",
    "print(\"\\nTable Representation of Combined DataFrame:\")\n",
    "print(combined_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
