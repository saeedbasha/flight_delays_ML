{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['figure.figsize'] = (11, 7)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna \n",
    "\n",
    "# Feel free to add all the libraries you need\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/labelencoded.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1[df1['target']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define features and target\n",
    "features = df.columns.tolist()\n",
    "features.remove('target')\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "# LETS DO FEATURE ENGINEERING.....gonna square some shit \n",
    "# generate interaction features to the 3rd degree\n",
    "p = PolynomialFeatures(interaction_only=False, include_bias=False, degree=1)\n",
    "X_poly = p.fit_transform(X)\n",
    "p.n_output_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, random_state=12, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature engineering -- create function to pull out coefficients/features\n",
    "## add a gradient descent\n",
    "def lasso_optimization(x123_train):\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[], 'alpha':[], 'coeff_used':[], 'coefficients':[], 'intercept':[]}\n",
    "\n",
    "    # initialize and train model with (default value) alpha = 0.5\n",
    "    # for i in [0.1, 1, 10,100,1000, 10000, 33000,66000, 100000, 330000,660000,1000000, 3300000,6600000,10000000,33000000,66000000,100000000]:\n",
    "    for i in [1, 10, 100, 1000]:\n",
    "\n",
    "        model = Lasso(alpha=i, max_iter=int(1000))\n",
    "        model.fit(x123_train,y_train)\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        ## use cross validation for \n",
    "        score = cross_validate(model, x123_train, y_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_mean_squared_error'), cv=3)\n",
    "        \n",
    "        ## pull out the scores\n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_mean_squared_error'])\n",
    "        coeff_used = np.sum(model.coef_!=0)\n",
    "        coefficients = model.coef_\n",
    "        intercept = model.intercept_\n",
    "\n",
    "        dicKeys = ['mean_MAPE','mean_r2','mean_MSE','std_MAPE','std_r2','std_MSE','score','alpha','coeff_used', 'coefficients', 'intercept'] \n",
    "        dicValues = [mean_MAPE, mean_r2, mean_MSE, std_MAPE, std_r2, std_MSE, len(score), i, coeff_used, coefficients, intercept]\n",
    "        for (keyi, vali) in zip(dicKeys, dicValues):\n",
    "            eval_metrics_dictionary[keyi].append(vali)\n",
    "        #eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        #eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        #eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        #eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        #eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        #eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        #eval_metrics_dictionary['score'].append(len(score))\n",
    "        #eval_metrics_dictionary['alpha'].append(i)\n",
    "        #eval_metrics_dictionary['coeff_used'].append(coeff_used)\n",
    "\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    df_framework['combined_score'] = abs(abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']*df_framework['mean_MAPE'])\n",
    "    df_bestvalue = df_framework[df_framework['combined_score']==min(df_framework['combined_score'])]\n",
    "    return df_bestvalue\n",
    "#return df_framework.sort_values('combined_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.965e+07, tolerance: 4.831e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "hi = lasso_optimization(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_r2</th>\n",
       "      <th>mean_MAPE</th>\n",
       "      <th>mean_MSE</th>\n",
       "      <th>std_r2</th>\n",
       "      <th>std_MAPE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>score</th>\n",
       "      <th>alpha</th>\n",
       "      <th>coeff_used</th>\n",
       "      <th>coefficients</th>\n",
       "      <th>intercept</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023187</td>\n",
       "      <td>2.976948</td>\n",
       "      <td>19231.288366</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.068948</td>\n",
       "      <td>1799.078446</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.0014450860490376252, -0.056552885094557755,...</td>\n",
       "      <td>-29103.564121</td>\n",
       "      <td>55923.07879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_r2  mean_MAPE      mean_MSE    std_r2  std_MAPE      std_MSE  score   \n",
       "0  0.023187   2.976948  19231.288366  0.001005  0.068948  1799.078446      5  \\\n",
       "\n",
       "   alpha  coeff_used                                       coefficients   \n",
       "0      1          21  [0.0014450860490376252, -0.056552885094557755,...  \\\n",
       "\n",
       "      intercept  combined_score  \n",
       "0 -29103.564121     55923.07879  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take list of coefficients - if more than zero \n",
    "# keep the name of the poly feature\n",
    "# keep the non-zero poly features values\n",
    "featurenames = list(p.get_feature_names_out(X.columns))\n",
    "##fix the zero\n",
    "coefficient_list = list(hi['coefficients'][0]) \n",
    "\n",
    "### creature a dictionary of features and coefficients and then turn into dataframe\n",
    "feature_coefficient_dict = {'feature':[], 'coefficient':[]}\n",
    "for (feaI, coeI) in zip(featurenames, coefficient_list): \n",
    "    if coeI != 0: \n",
    "        feature_coefficient_dict['feature'].append(feaI)\n",
    "        feature_coefficient_dict['coefficient'].append(coeI)\n",
    "        \n",
    "pd.DataFrame(feature_coefficient_dict)\n",
    "feature_selection = feature_coefficient_dict['feature']\n",
    "#### with the coefficients create a new np.array to \n",
    "Xtrain1 = np.array(pd.DataFrame(X_train, columns= p.get_feature_names_out(X.columns))[feature_selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = lasso_optimization(Xtrain1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_r2</th>\n",
       "      <th>mean_MAPE</th>\n",
       "      <th>mean_MSE</th>\n",
       "      <th>std_r2</th>\n",
       "      <th>std_MAPE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>score</th>\n",
       "      <th>alpha</th>\n",
       "      <th>coeff_used</th>\n",
       "      <th>coefficients</th>\n",
       "      <th>intercept</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023217</td>\n",
       "      <td>2.97702</td>\n",
       "      <td>19230.768267</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.069052</td>\n",
       "      <td>1799.697369</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.0014450859119771776, -0.057313962698797224,...</td>\n",
       "      <td>-29103.570323</td>\n",
       "      <td>55921.229329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_r2  mean_MAPE      mean_MSE    std_r2  std_MAPE      std_MSE  score   \n",
       "0  0.023217    2.97702  19230.768267  0.000977  0.069052  1799.697369      5  \\\n",
       "\n",
       "   alpha  coeff_used                                       coefficients   \n",
       "0      1          21  [0.0014450859119771776, -0.057313962698797224,...  \\\n",
       "\n",
       "      intercept  combined_score  \n",
       "0 -29103.570323    55921.229329  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take list of coefficients - if more than zero \n",
    "# keep the name of the poly feature\n",
    "# keep the non-zero poly features values\n",
    "featurenames = list(p.get_feature_names_out(X.columns))\n",
    "##fix the zero\n",
    "coefficient_list = list(hi['coefficients'][0]) \n",
    "\n",
    "### creature a dictionary of features and coefficients and then turn into dataframe\n",
    "feature_coefficient_dict = {'feature':[], 'coefficient':[]}\n",
    "for (feaI, coeI) in zip(featurenames, coefficient_list): \n",
    "    if coeI != 0: \n",
    "        feature_coefficient_dict['feature'].append(feaI)\n",
    "        feature_coefficient_dict['coefficient'].append(coeI)\n",
    "        \n",
    "pd.DataFrame(feature_coefficient_dict)\n",
    "feature_selection = feature_coefficient_dict['feature']\n",
    "#### with the coefficients create a new np.array to \n",
    "ridge_Xtrain = np.array(pd.DataFrame(X_train, columns= p.get_feature_names_out(X.columns))[feature_selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>datop</td>\n",
       "      <td>1.445086e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>depstn</td>\n",
       "      <td>-5.731396e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arrstn</td>\n",
       "      <td>-5.951240e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>status</td>\n",
       "      <td>-2.210036e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ac</td>\n",
       "      <td>1.118436e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>year</td>\n",
       "      <td>9.840891e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>month</td>\n",
       "      <td>-2.077935e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>day</td>\n",
       "      <td>2.994251e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dep_iata</td>\n",
       "      <td>8.585264e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dep_country</td>\n",
       "      <td>-8.176810e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dep_elevation</td>\n",
       "      <td>4.040404e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dep_lat</td>\n",
       "      <td>-2.169694e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dep_lon</td>\n",
       "      <td>7.529423e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>arr_iata</td>\n",
       "      <td>1.772917e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>arr_country</td>\n",
       "      <td>5.655566e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>arr_elevation</td>\n",
       "      <td>3.216661e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>arr_lat</td>\n",
       "      <td>-8.096894e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>arr_lon</td>\n",
       "      <td>1.371684e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>season</td>\n",
       "      <td>-7.039338e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>airline_code</td>\n",
       "      <td>2.567962e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>elevation_dif</td>\n",
       "      <td>3.600057e-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature   coefficient\n",
       "0           datop  1.445086e-03\n",
       "1          depstn -5.731396e-02\n",
       "2          arrstn -5.951240e-02\n",
       "3          status -2.210036e-02\n",
       "4              ac  1.118436e+00\n",
       "5            year  9.840891e-02\n",
       "6           month -2.077935e-02\n",
       "7             day  2.994251e-01\n",
       "8        dep_iata  8.585264e-03\n",
       "9     dep_country -8.176810e-01\n",
       "10  dep_elevation  4.040404e-01\n",
       "11        dep_lat -2.169694e-03\n",
       "12        dep_lon  7.529423e-01\n",
       "13       arr_iata  1.772917e-03\n",
       "14    arr_country  5.655566e-01\n",
       "15  arr_elevation  3.216661e-01\n",
       "16        arr_lat -8.096894e-02\n",
       "17        arr_lon  1.371684e-02\n",
       "18         season -7.039338e-02\n",
       "19   airline_code  2.567962e-04\n",
       "20  elevation_dif  3.600057e-19"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(feature_coefficient_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0170829e+07,  1.2300000e+02,  4.8000000e+01, ...,\n",
       "         2.0000000e+00,  1.4000000e+01,  3.4200000e+02],\n",
       "       [ 2.0160812e+07,  4.8000000e+01,  1.1900000e+02, ...,\n",
       "         2.0000000e+00,  1.4000000e+01, -3.4200000e+02],\n",
       "       [ 2.0160622e+07,  5.9000000e+01,  1.1900000e+02, ...,\n",
       "         2.0000000e+00,  1.4000000e+01, -2.6000000e+01],\n",
       "       ...,\n",
       "       [ 2.0170716e+07,  9.6000000e+01,  1.1900000e+02, ...,\n",
       "         2.0000000e+00,  1.4000000e+01, -2.6900000e+02],\n",
       "       [ 2.0171016e+07,  5.8000000e+01,  1.1900000e+02, ...,\n",
       "         0.0000000e+00,  1.4000000e+01, -3.0300000e+02],\n",
       "       [ 2.0161010e+07,  1.1700000e+02,  1.1900000e+02, ...,\n",
       "         0.0000000e+00,  1.4000000e+01, -4.7700000e+02]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_optimization(number_of_trials):\n",
    "    # input to study should be dataset and number of iterations\n",
    "    eval_metrics_dictionary = {'mean_r2':[],'mean_MAPE':[],'mean_MSE':[],'std_r2':[],'std_MAPE':[],'std_MSE':[],'score':[]}\n",
    "    ridge_opt_dictionary = { 'alpha': [], 'intercept' : [], 'tol': [], 'solver':[], 'no_of_coefficients':[]}\n",
    "\n",
    "    def objective(trial):\n",
    "        ## HYPER PARAMETER TUNING\n",
    "        alpha = trial.suggest_float(\"alpha\", 0, 100)\n",
    "        intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "        tol = trial.suggest_float(\"tol\", 0.001, 1, log=True)\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"auto\", \"svd\",\"cholesky\", \"lsqr\", \"saga\", \"sag\"])\n",
    "\n",
    "        ## MODEL SELECTION\n",
    "        ## Create Model\n",
    "        model = Ridge(alpha=alpha,fit_intercept=intercept,tol=tol,solver=solver)\n",
    "        ## Fit Model\n",
    "        model.fit(ridge_Xtrain, y_train)\n",
    "        \n",
    "        # Function parameters for cross_val_score; \n",
    "        #   - estimator - The model object to use to fit the data\n",
    "        #   - X - The data to fit the model on\n",
    "        #   - y - The target of the model\n",
    "        #   - scoring - The error metric to use\n",
    "        #   - cv - The number of splits to use\n",
    "        \n",
    "        # error score to use; \n",
    "        #   - accuracy\n",
    "        #   - balanced_accuracy\n",
    "        #   - roc_auc\n",
    "        #   - f1\n",
    "        #   - neg_mean_absolute_error\n",
    "        #   - neg_root_mean_squared_error\n",
    "        #   - r2\n",
    "        \n",
    "        ## EVALUATION METRICS\n",
    "        # you should really use cross_val_score rather than testing against the test set\n",
    "        score = cross_validate(model, X_train, y_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_mean_squared_error'), cv=3)\n",
    "        mean_MAPE = -1*np.mean(score['test_neg_mean_absolute_percentage_error'])\n",
    "        mean_r2 = np.mean(score['test_r2'])\n",
    "        mean_MSE = -1*np.mean(score['test_neg_mean_squared_error'])\n",
    "        std_MAPE = np.std(score['test_neg_mean_absolute_percentage_error'])\n",
    "        std_r2 = np.std(score['test_r2'])\n",
    "        std_MSE = np.std(score['test_neg_mean_squared_error'])\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_train= ridge_regressor.predict(X_test)\n",
    "            \n",
    "        Evaluation metrics;\n",
    "        - R squared\n",
    "        - MSE\n",
    "        - RMSE\n",
    "        - MAPE\n",
    "        \n",
    "        infun_MSE = mean_squared_error(y_test,y_pred_train)\n",
    "        infun_RMSE = math.sqrt(infun_MSE)\n",
    "        infun_r2 = r2_score(y_test, y_pred_train) \n",
    "        infun_MAPE = mean_absolute_percentage_error(y_test, y_pred_train) \n",
    "        max_coeff = ridge_regressor.coef_.round(3)\n",
    "        eval_metrics_dictionary['r2'].append(infun_r2)\n",
    "        eval_metrics_dictionary['MSE'].append(infun_MSE)\n",
    "        eval_metrics_dictionary['RMSE'].append(infun_RMSE)\n",
    "        eval_metrics_dictionary['MAPE'].append(infun_MAPE)\n",
    "        \"\"\"\n",
    "        coeff_used = np.sum(model.coef_!=0)\n",
    "\n",
    "        eval_metrics_dictionary['mean_MAPE'].append(mean_MAPE)\n",
    "        eval_metrics_dictionary['mean_r2'].append(mean_r2)\n",
    "        eval_metrics_dictionary['mean_MSE'].append(mean_MSE)\n",
    "        eval_metrics_dictionary['std_MAPE'].append(std_MAPE)\n",
    "        eval_metrics_dictionary['std_r2'].append(std_r2)\n",
    "        eval_metrics_dictionary['std_MSE'].append(std_MSE)\n",
    "        eval_metrics_dictionary['score'].append(len(score))\n",
    "        \n",
    "        ridge_opt_dictionary['alpha'].append(alpha)\n",
    "        ridge_opt_dictionary['intercept'].append(intercept)\n",
    "        ridge_opt_dictionary['tol'].append(tol)\n",
    "        ridge_opt_dictionary['solver'].append(solver)\n",
    "        ridge_opt_dictionary['no_of_coefficients'].append(coeff_used)\n",
    "\n",
    "        return mean_r2\n",
    "\n",
    "    study2 = optuna.create_study(study_name=\"RidgeRegression Optimization\")\n",
    "    study2.optimize(objective, n_trials=number_of_trials)\n",
    "    df_framework = pd.DataFrame(eval_metrics_dictionary)\n",
    "    # * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\n",
    "    df_framework['combined_score'] = abs(abs(1 - df_framework['mean_r2'])*df_framework['mean_MSE']*df_framework['mean_MAPE'])\n",
    "    df_bestvalue = df_framework[df_framework['combined_score']==min(df_framework['combined_score'])]\n",
    "    return df_bestvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-31 22:50:32,350] A new study created in memory with name: RidgeRegression Optimization\n",
      "[W 2023-08-31 22:50:55,481] Trial 0 failed with parameters: {'alpha': 75.61895757061254, 'fit_intercept': False, 'tol': 0.00102216412755496, 'solver': 'sag'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_24896\\951272127.py\", line 37, in objective\n",
      "    score = cross_validate(model, X_train, y_train, scoring=('neg_mean_absolute_percentage_error', 'r2', 'neg_mean_squared_error'), cv=3)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 266, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1134, in fit\n",
      "    return super().fit(X, y, sample_weight=sample_weight)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 900, in fit\n",
      "    self.coef_, self.n_iter_ = _ridge_regression(\n",
      "                               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 724, in _ridge_regression\n",
      "    coef_, n_iter_, _ = sag_solver(\n",
      "                        ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 325, in sag_solver\n",
      "    num_seen, n_iter_ = sag(\n",
      "                        ^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2023-08-31 22:50:55,485] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ridge_optimization(\u001b[39m200\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[122], line 83\u001b[0m, in \u001b[0;36mridge_optimization\u001b[1;34m(number_of_trials)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m mean_r2\n\u001b[0;32m     82\u001b[0m study2 \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(study_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRidgeRegression Optimization\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m study2\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mnumber_of_trials)\n\u001b[0;32m     84\u001b[0m df_framework \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(eval_metrics_dictionary)\n\u001b[0;32m     85\u001b[0m \u001b[39m# * by the r2_score that is closest to 1 --- you want to minimise the values of the rest, dont need to include RMSE, you overweight that value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     _optimize(\n\u001b[0;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    452\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[122], line 37\u001b[0m, in \u001b[0;36mridge_optimization.<locals>.objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     17\u001b[0m model\u001b[39m.\u001b[39mfit(ridge_Xtrain, y_train)\n\u001b[0;32m     19\u001b[0m \u001b[39m# Function parameters for cross_val_score; \u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m#   - estimator - The model object to use to fit the data\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m#   - X - The data to fit the model on\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39m## EVALUATION METRICS\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m# you should really use cross_val_score rather than testing against the test set\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m score \u001b[39m=\u001b[39m cross_validate(model, X_train, y_train, scoring\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mneg_mean_absolute_percentage_error\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mneg_mean_squared_error\u001b[39;49m\u001b[39m'\u001b[39;49m), cv\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m     38\u001b[0m mean_MAPE \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mmean(score[\u001b[39m'\u001b[39m\u001b[39mtest_neg_mean_absolute_percentage_error\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     39\u001b[0m mean_r2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(score[\u001b[39m'\u001b[39m\u001b[39mtest_r2\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:1134\u001b[0m, in \u001b[0;36mRidge.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1125\u001b[0m _accept_sparse \u001b[39m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[39m.\u001b[39missparse(X), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver)\n\u001b[0;32m   1126\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m   1127\u001b[0m     X,\n\u001b[0;32m   1128\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     y_numeric\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1133\u001b[0m )\n\u001b[1;32m-> 1134\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:900\u001b[0m, in \u001b[0;36m_BaseRidge.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    896\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    897\u001b[0m         \u001b[39m# for dense matrices or when intercept is set to 0\u001b[39;00m\n\u001b[0;32m    898\u001b[0m         params \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 900\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m _ridge_regression(\n\u001b[0;32m    901\u001b[0m         X,\n\u001b[0;32m    902\u001b[0m         y,\n\u001b[0;32m    903\u001b[0m         alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[0;32m    904\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    905\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    906\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m    907\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[0;32m    908\u001b[0m         positive\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpositive,\n\u001b[0;32m    909\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[0;32m    910\u001b[0m         return_n_iter\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    911\u001b[0m         return_intercept\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    912\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    913\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[0;32m    914\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[0;32m    915\u001b[0m     )\n\u001b[0;32m    916\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_intercept(X_offset, y_offset, X_scale)\n\u001b[0;32m    918\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:724\u001b[0m, in \u001b[0;36m_ridge_regression\u001b[1;34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, positive, random_state, return_n_iter, return_intercept, X_scale, X_offset, check_input, fit_intercept)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[39mfor\u001b[39;00m i, (alpha_i, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(alpha, y\u001b[39m.\u001b[39mT)):\n\u001b[0;32m    721\u001b[0m     init \u001b[39m=\u001b[39m {\n\u001b[0;32m    722\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcoef\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mzeros((n_features \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(return_intercept), \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    723\u001b[0m     }\n\u001b[1;32m--> 724\u001b[0m     coef_, n_iter_, _ \u001b[39m=\u001b[39m sag_solver(\n\u001b[0;32m    725\u001b[0m         X,\n\u001b[0;32m    726\u001b[0m         target\u001b[39m.\u001b[39;49mravel(),\n\u001b[0;32m    727\u001b[0m         sample_weight,\n\u001b[0;32m    728\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39msquared\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    729\u001b[0m         alpha_i,\n\u001b[0;32m    730\u001b[0m         \u001b[39m0\u001b[39;49m,\n\u001b[0;32m    731\u001b[0m         max_iter,\n\u001b[0;32m    732\u001b[0m         tol,\n\u001b[0;32m    733\u001b[0m         verbose,\n\u001b[0;32m    734\u001b[0m         random_state,\n\u001b[0;32m    735\u001b[0m         \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    736\u001b[0m         max_squared_sum,\n\u001b[0;32m    737\u001b[0m         init,\n\u001b[0;32m    738\u001b[0m         is_saga\u001b[39m=\u001b[39;49msolver \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    739\u001b[0m     )\n\u001b[0;32m    740\u001b[0m     \u001b[39mif\u001b[39;00m return_intercept:\n\u001b[0;32m    741\u001b[0m         coef[i] \u001b[39m=\u001b[39m coef_[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\flight_delays_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:325\u001b[0m, in \u001b[0;36msag_solver\u001b[1;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mZeroDivisionError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCurrent sag implementation does not handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    324\u001b[0m sag \u001b[39m=\u001b[39m sag64 \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mfloat64 \u001b[39melse\u001b[39;00m sag32\n\u001b[1;32m--> 325\u001b[0m num_seen, n_iter_ \u001b[39m=\u001b[39m sag(\n\u001b[0;32m    326\u001b[0m     dataset,\n\u001b[0;32m    327\u001b[0m     coef_init,\n\u001b[0;32m    328\u001b[0m     intercept_init,\n\u001b[0;32m    329\u001b[0m     n_samples,\n\u001b[0;32m    330\u001b[0m     n_features,\n\u001b[0;32m    331\u001b[0m     n_classes,\n\u001b[0;32m    332\u001b[0m     tol,\n\u001b[0;32m    333\u001b[0m     max_iter,\n\u001b[0;32m    334\u001b[0m     loss,\n\u001b[0;32m    335\u001b[0m     step_size,\n\u001b[0;32m    336\u001b[0m     alpha_scaled,\n\u001b[0;32m    337\u001b[0m     beta_scaled,\n\u001b[0;32m    338\u001b[0m     sum_gradient_init,\n\u001b[0;32m    339\u001b[0m     gradient_memory_init,\n\u001b[0;32m    340\u001b[0m     seen_init,\n\u001b[0;32m    341\u001b[0m     num_seen_init,\n\u001b[0;32m    342\u001b[0m     fit_intercept,\n\u001b[0;32m    343\u001b[0m     intercept_sum_gradient,\n\u001b[0;32m    344\u001b[0m     intercept_decay,\n\u001b[0;32m    345\u001b[0m     is_saga,\n\u001b[0;32m    346\u001b[0m     verbose,\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    349\u001b[0m \u001b[39mif\u001b[39;00m n_iter_ \u001b[39m==\u001b[39m max_iter:\n\u001b[0;32m    350\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    351\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    352\u001b[0m         ConvergenceWarning,\n\u001b[0;32m    353\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ridge_optimization(200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
